{"title": "로그 스트리밍으로 서비스 구성하기", "content": "## 배경\n\n### 로그도 남기고, 이벤트도 남겨?\n\n서비스를 쭉 구성하다보니 이런 생각을 했습니다.\n\n1. 어차피 로그는 남긴다.\n2. 필요한 유저 행동에 대해 이벤트르 발행한다.\n3. 그러면 각 서비스가 로그에서 필요한 걸 가져가서 상태 변화를 기록하면 되는 것 아닐까?\n4. 뭣하러 굳이 이벤트를 별도로 발행하지? 어차피 모두 kafka같은 메시지 브로커를 탈텐데?\n\n왜 굳이 2개를 중복으로 메시지 브로커에게 남겨야 하는가가 매우 큰 궁금증이었습니다.  \n기본적으로 어떤 메시지 브로커를 선택하든 간에, 아니 가장 대표적인 Kafka를 상정하고 얘기해도, 각 컨슈머 그룹에 따라 메시지를 복제해서 전달하고, 컨슈머 그룹 내에 여러 컨슈머가 있을 경우에 적절히 분배해주는 기능이 존재합니다.  \nMSA를 구성하든, MMA를 구성하든 간에 사실 필요한 로그를 누락 없이 받아서 처리할 수 있을 것입니다.  \n~~처리량이 많다고요? 노드를 늘리고, 스펙을 올리죠!~~\n\n### 그럼 현실적으로 왜 그러지 않지?\n\n체리픽을 어떻게 할지가 가장 큰 문제가 아닐까 생각합니다.  \n이 부분 자체가 단순히 받는 쪽에서 실시간으로 흐르고 있는 로그 스트림을 봐가면서 하나하나 가져오기에는 너무 큰 부담이라 생각합니다.  \n그렇다면 로그를 생성하는 쪽에서도 어떠한 로그인지에 대해 최소 2~3개의 분류를 해주고, 받는 쪽에서 어떤 로그 분류를 받아서 처리할지 골라서 받으면, 부담이 많이 줄어들 수 있을 것입니다.\n\n근데 이러면 이벤트 발행하는 것과 다르지 않냐구요?  \n사실 이 포스트 자체가 로그와 이벤트를 둘 다 남기는 것이 불만이었고, 둘을 하나로 줄이는 과정에 있다고 보시면 해당 의문은 그렇게 중요한 것이 아니라고 느끼실 겁니다.  \n단지 이벤트가 사라지고 로그만이 남았다고 생각하시면 편해요.\n\n## 가정\n\n### 뭘 만들까?\n\n제가 해당 도메인에 있으니 라이브 스트리밍 서비스를 만든다고 가정해보겠습니다.  \n기본적으로 스트리밍을 할 수 있는 환경을 만들어야 할 것입니다.\n\n1. RTMP(S) 서버: RTMP를 받는 서버입니다.\n2. Transcoder: 송출 처리된 비디오를 여러 화질로 변환합니다.\n3. Packetizer & Uploader: ts나 mp4 형식으로 비디오나 오디오를 박싱합니다.\n4. HLS 서버: 박싱된 미디어 데이터나 메타데이터를 사용자에게 서빙합니다.\n5. LiveInfo: 라이브 스트리밍에 대한 정보를 관리합니다.\n6. Live API 서버: 라이브 스트리밍에 대한 정보를 서빙합니다.\n\n구현하는 게 아니니 좀 넉넉히 가정했습니다.\n\n여기에 메시지 브로커로는 apache kafka를 가정하겠습니다.  \n제가 카프카에 대한 지식이 깊지 않아서 실수할 수 있으니, 만약 잘못된 내용이 있다면 제 이메일로 알려주시면 첨삭하도록 하겠습니다.\n\n## 구상\n\n### 토픽을 구성해볼까?\n\n#### 카프카로 흐르는 모든 것은 로그다.\n\n개인적으로는 카프카 클러스터를 구성한다면, 해당 카프카 클러스터는 오로지 `로그만을 전달하기 위한 클러스터`로 구성할 것입니다.  \n하지만 많은 조직에서 비싼 카프카 클러스터를 오로지 로그 시핑을 위한 용도로만 쓰도록 설계할 것같진 않습니다.  \n예를 들어, 로컬에서 생성형 AI를 서비스에 활용할 경우에 카프카를 통해 요청과 응답을 주고 받을 수 있을 것입니다.\n\n하지만, 이 글에서는 제가 편한 방식과 이상을 반영해서 로그만을 전달한다고 가정하겠습니다.  \n그러므로 일단 `*` 토픽에 ElasticSearch Sink Connector를 붙이겠습니다.  \n필요하다면 Transforms를 이용하여 인덱스 이름을 직접 수정하도록 합니다.  \n~~저도 일단 지르고 보는 거에요~~\n\n#### 그래서 각 서비스는?\n\n필요한 이벤트 로그(서비스 운영 상 정상적인 상황의 정보 발생에 따른 로그)는 대표적으로 이런 것들이 있을 것같습니다.\n\n1. RTMP 서버: 방송 개설 및 종료 여부, 인증 성공 및 실패 여부\n2. Transcoder: 변환되는 해상도 및 화질 정보\n3. Live API 서버: Publish Key 변경 여부, 방송 제목 변경 여부, 방송 옵션 변경 여부\n\n이제 어떻게 해야 커플링을 최소화하며 로그를 남길 수 있을까요?  \n미리 작성했던 대로 2~3개 정도의 분류를 적용하여 토픽을 설정해야할 것입니다.  \n현재 나와 있는 스펙을 보면 대분류는 `media` 하나로 가능할 것같습니다.\n\n그 다음 중분류에서 2가지 분류로, 소분류에선 더 잘게 나눌 수 있어 보입니다.\n\n1. `live_stream`: 실제 라이브 과정의 것임을 의미합니다.\n    1. `status`: 방송 시작, 중단과 같은 상태 변경에 쓰입니다.\n    2. `metadata`: 해당 방송의 메타데이터가 변경됨을 남깁니다.\n2. `broad_info`: 방송에 이미 설정되어 있는 것임을 의미합니다.\n    1. `publish_key`: 퍼블리시키에 대한 변경이 발생됨을 남깁니다.\n    2. `title`: 방송 제목에 대한 변경이 발생됨을 남깁니다.\n    3. `tags`: 방송 태그가 변경됨을 남깁니다.\n    4. `options`: 기타 방송을 위한 옵션이 변경됨을 남깁니다.\n\n\n그러면 현재 제안 상으로 다음 토픽들이 나오게 됩니다.\n\n1. media.live_stream.status\n2. media.live_stream.metadata\n3. media.broad_info.publish_key\n4. media.broad_info.title\n5. media.broad_info.tags\n7. media.broad_info.options\n\n### 토픽 좀 긁어볼까?\n\n이제 이걸 어떻게 활용할 수 있을까요?  \n대략적으로 생각나는 건 이정도입니다.\n\n1. media.broad_info.publish_key: API를 통해 변경된 퍼블리시키를 RTMP 서버의 인증 부분에서 사용\n2. media.broad_info.title: API를 통해 변경된 방송 제목을 LiveInfo가 받아서 변경 (쓰기 작업)\n3. media.broad_info.tags: API를 통해 변경된 방송 태그를 LiveInfo가 받아서 변경 (쓰기 작업)\n4. media.broad_info.options: API를 통해 변경된 방송 옵션을 LiveInfo가 받아서 변경 (쓰기 작업)\n5. media.live_stream.status: RTMP 서버에서 publish 성공 시 개설, 커넥션 종료 시 중단을 발행하여 LiveInfo에서 방송 상태 변경 (쓰기 작업)\n6. media.live_stream.metadata: Transcoder에서 오리지널 비디오 및 오디오에 대한 정보 및 트랜스코딩 되는 타겟들을 발행하여 LiveInfo와 HLS 서버가 사용할 수 있는 HLS 옵션으로 기록 (쓰기 작업)\n\n그리고 이 데이터들은\n\n1. 송출자가 송출 시 인증 과정에 사용\n2. 시청자가 방송 정보 조회 및 방송 시청 시 데이터로 사용\n\n### 조금 더 나아가서\n\n사실 만약 엘라스틱 서치나 밀리서치를 구성한다면, kafka를 통해 검색 엔진에 추가하고 사용할 수 있을 것입니다.  \n물론 직접적으로 넣기보단 관련 서버가 받아서 검색엔진에 대한 작업을 하는 방향으로 입니다.  \n예를 들어,\n\n1. 방송 제목이 변경되었다면 기존의 것을 삭제하고 새로운 것으로 대체하는 동작\n2. 방송이 개설되었다면, 제목을 조회 및 추가하여 검색 가능하게 설정\n3. 방송이 중단되었다면, 제목을 삭제하여 검색 불가능하게 설정\n\n이에 대해선 `tags`나 `metadata`에도 동일합니다.  \n특정 태그에 대한 방송을 검색한다거나, 어떤 해상도를 지원하는 방송을 검색하는 등이 가능해지는 거죠.\n\n## 끝으로\n\n한번쯤 구성해보고 싶은 구조입니다.  \n당연한 이야기지만 트랜잭션이 필요한 구간은 쓰기가 끝난 후에, CDC를 통해 메시지 브로커에 publish하고 온라인 분석에 활용할 수 있도록 하는 구조가 일반적으로 보입니다.  \n또한 이 구조 자체가 로그를 생산하고 소비하는 주체가 되는 곳에서 확실하게 합의가 되어야 하는 부분이라 시작할 때 체계를 잘 잡아야 할만해 보입니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/041_log_stream/", "created_date": "2024-11-16T09:12:42Z"}
{"title": "enum in go", "content": "> 이 페이지의 고 코드는 제네릭이 포함되어 있습니다.\n\n## enum?\n\n고에서 열거형은 아래의 단순한 형태, 혹은 조금의 변형으로밖에 작성되지 않습니다.\n\n```go\npackage week\n\nconst (\n    Monday = iota\n    Tuesday\n    Wednesday\n    Thursday\n    Friday\n    Saturday\n    Sunday\n)\n```\n\n단순하게 `week` 패키지 내에서 상수를 선언하여 가져다 쓰는 정도입니다. 하지만 그건 어디까지나 열거형을 위한 패키지를 분리하지 않았기에 제한되는 방식이라 생각합니다.\n\n## package as class\n\n단일 역할에 대해 단일 구현체만 패키지에 작성할 경우 패키지를 클래스처럼 이용할 수 있습니다. \n\n### 바이트 슬라이스 열거형\n\n바이트 슬라이스를 열거형의 값으로 가지려면 `const` 키워드를 사용하여 상수로 선언할 수는 없습니다. \n\n```go\npackage buffer\n\nvar A = [32]byte{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32}\nvar B = [32]byte{2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31}\nvar C = [32]byte{3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 1, 4, 7, 10, 13, 16, 19, 22, 25, 28}\n```\n\n`var` 키워드로 변수로 선언할 수밖에 없습니다. 그러면 극단적인 상황으로 외부에서 전역 변수의 값을 바꿔버릴 경우 의도하지 않은 동작을 하게 될 수도 있습니다. 이걸 방지하기 위해 코드를 복잡하게 만들어 보겠습니다.\n\n```go\npackage buffer\n\nvar a = [32]byte{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32}\nvar b = [32]byte{2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31}\nvar c = [32]byte{3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 1, 4, 7, 10, 13, 16, 19, 22, 25, 28}\n\nfunc A() [32]byte {\n\treturn a\n}\nfunc B() [32]byte {\n\treturn b\n}\nfunc C() [32]byte {\n\treturn c\n}\n```\n\n변수는 숨긴 채로 함수를 만들어서 값을 반환하여 변수가 노출되는 것을 방지하였습니다. 여기에 더 나아가서 `buffer` 패키지만의 타입을 만듭니다.\n\n```go\npackage buffer\n\ntype Buffer [32]byte\n\nvar a = [32]byte{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32}\nvar b = [32]byte{2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31}\nvar c = [32]byte{3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 1, 4, 7, 10, 13, 16, 19, 22, 25, 28}\n\nfunc A() Buffer {\n\treturn a\n}\nfunc B() Buffer {\n\treturn b\n}\nfunc C() Buffer {\n\treturn c\n}\n```\n\n`Buffer` 타입을 만들어 `[32]byte`의 별칭으로 지정합니다. 이렇게 `Buffer` 타입의 정적 변수를 만든 셈입니다. 열거형에 맞게 인덱스 관련 함수를 작성합니다. \n\n```go\nfunc Get(i int) Buffer {\n\tswitch i {\n\tcase 0:\n\t\treturn A()\n\tcase 1:\n\t\treturn B()\n\tcase 2:\n\t\treturn C()\n\tdefault:\n\t\treturn Buffer{}\n\t}\n}\nfunc IndexOf(buf *Buffer) int {\n\tif *buf == a {\n\t\treturn 0\n\t}\n\tif *buf == b {\n\t\treturn 1\n\t}\n\tif *buf == c {\n\t\treturn 2\n\t}\n\treturn -1\n}\n```\n\n`Get`과 `IndexOf` 함수를 통해 각각 특정 인덱스의 버퍼를 가져오거나 입력받은 버퍼의 인덱스 값을 받아올 수 있습니다.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"prac/buffer\"\n)\n\nfunc main() {\n\ta := buffer.A()\n\tfmt.Println(a)\n\tfmt.Println(\"index:\", buffer.IndexOf(&a))\n\tfmt.Println(\"first value:\", buffer.Get(0))\n}\n```\n\n```bash\n[1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]\nindex: 0\nfirst value: [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]\n```\n\n간단한 코드인 만큼 제대로 동작하는 걸 확인할 수 있고 `buffer` 패키지를 스태틱 클래스처럼 쓸 수 있는 걸 볼 수 있습니다.\n\n### Result[T any] type\n\n위의 코드는 단순 상수 목록과 사실 다를게 없습니다. 좀 더 나아가서 열거형 항목을 별개의 객체처럼 다루는 것도 가능할 것입니다. \n\n```go\npackage result\n\ntype Result[T any] any\n\ntype resultOK[T any] struct {\n\tValue T\n}\n\ntype resultError struct {\n\terr error\n}\n\nfunc OK[T any](value T) Result[T] {\n\treturn resultOK[T]{Value: value}\n}\n\nfunc Err[T any](err error) Result[T] {\n\treturn resultError{err: err}\n}\n\nfunc IndexOf[T any](result Result[T]) int {\n\tswitch result.(type) {\n\tcase resultOK[T]:\n\t\treturn 0\n\tcase resultError:\n\t\treturn -1\n\t}\n\treturn -1\n}\n\nfunc ValueOf[T any](result Result[T]) T {\n\tswitch result.(type) {\n\tcase resultOK[T]:\n\t\treturn result.(resultOK[T]).Value\n\t}\n\tpanic(\"Unwrap on Error\")\n}\n\nfunc ErrorOf[T any](result Result[T]) error {\n\tswitch result.(type) {\n\tcase resultError:\n\t\treturn result.(resultError).err\n\t}\n\tpanic(\"Unwrap on OK\")\n}\n```\n\n가급적 간단한 형태로 만들었습니다. `result` 패키지에 `Result` 타입을 만들고 내부에 `resultOk`와 `resultError` 타입을 세부 타입으로 만들었습니다. 이전의 접근자나 `Get` 함수를 통해 값을 가져오기보다 각 세부 타입에 맞는 접근자(생성자)를 새로 작성하였습니다.\n\n```go\npackage main\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"prac/result\"\n)\n\nfunc main() {\n\ta := result.OK(10)\n\tswitch result.IndexOf(a) {\n\tcase 0:\n\t\tfmt.Println(\"OK:\", result.ValueOf(a))\n\tcase -1:\n\t\tfmt.Println(\"Err:\", result.ErrorOf(a))\n\t}\n\n\tn := result.Err[int](errors.New(\"error\"))\n\tswitch result.IndexOf(n) {\n\tcase 0:\n\t\tfmt.Println(\"OK:\", result.ValueOf(n))\n\tcase -1:\n\t\tfmt.Println(\"Err:\", result.ErrorOf(n))\n\t}\n}\n```\n\n`result`의 `OK`, `Err`과 `IndexOf`, 스위치 구문을 활용하여 러스트의 result를 모방해봤습니다. \n\n```bash\nOK: 10\nErr: error\n```\n\n출력된 결과 또한 의도한 대로 출력되는 것을 확인할 수 있습니다. 좀 더 사용성에 중점을 두고 `IndexOf` 함수를 `IsOk`와 `IsErr`로 분리하는 등 좀 더 나은 코드로 수정하는 방법도 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/011_enum-in-go/", "created_date": "2022-03-12T18:41:20Z"}
{"title": "강한 일관성과 최종 일관성", "content": "## 잠재적인 데이터 불일치 문제\n\nMSA는 개발 및 배포의 유연성, 기술 스택의 다양성, 확장성, 장애 격리 등 다양한 장점을 제공하지만, 그로 인해 트레이드 오프되는 단점들도 분명히 존재합니다. 그 대표적인 부분이 비즈니스 로직 오류, 사용자 경험 저하, 심각한 경우 금전적 손실까지 야기할 수 있는 중요한 문제인 `잠재적인 데이터 불일치 문제`일 겁니다. 이 문제는 다양한 이유로 발생할 수 있지만, 바로 생각나는 이유로는 다음과 같은 것들이 있을 겁니다.\n\n- 각 서비스 마다 독립적인 데이터베이스를 사용한다거나\n- EDA를 기반으로 요청과 처리를 해서 트랜잭션 처리가 어렵다거나\n- 네트워크 불안정으로 인해 흐름이 순간적으로 끊긴다거나\n- 특정 서비스의 장애로 트랜잭션 처리가 지연되거나 취소된다거나\n\n결국 요지는 기존의 단일 데이터베이스에서 하던 트랜잭션을, 여러 서비스가 서로 다른 목적을 위해 각자의 데이터베이스에서 독립적으로 수행하다보니 연결이 느슨해지고, 그 틈새에서 발생할 수 있는 다양한 이슈로 인해 서비스 간 데이터 상태 동기화에 문제가 생길 수 있다는 것입니다.\n\n## 강한 일관성과 최종 일관성\n\n이에 대해 전통적인 관념을 강한 일관성이라고 합니다. 강한 일관성은 특정 데이터의 변경이 모든 노드와 유관 서비스에 완전히 적용되어 모두가 최신 데이터를 가지며 무결성을 보장하는 것입니다. 이로인해 항상 모든 데이터가 안정적으로 저장되고, 사용되는 장점을 보입니다. 하지만 이는 노드가 많아질수록, 유관 서비스가 많아질수록, 또한 데이터의 흐름이 설계 실수로 인해 복잡해질수록 절망적인 성능 하락폭을 보이는 단점, 또한 존재합니다.\n\n이러한 점을 보완하기 위해 조명받은 관념이 최종 일관성입니다. 최종 일관성은 결과적으로 시간이 지남에 따라 자연적으로 모든 노드와 유관 서비스에 변경된 데이터가 반영이 되는 것입니다. 당장은 유저에게 일관된 데이터를 전송하지 못할 수 있지만, 시간이 지나면 결과적으로 모든 유저에게 동일 데이터를 반환하게 될 겁니다. 이러한 최종 일관성은 MSA의 분산 환경 및 확장성, 성능 요구 사항을 충족하면서 데이터 일관성을 확보할 수 있는 현실적인 대안이라 생각합니다.\n\n최종 일관성을 잘 살린 솔루션에는 S3가 있으며, 사람에따라 평가가 갈리겠지만 저는 cassandra도 이에 포함될 수 있다 생각합니다. 쓰기가 발생한 노드는 자신의 변경 사항을 다른 노드에 전파하고, 다른 모든 노드는 전파를 받은 시점에 최신 데이터를 제공할 수 있게 됩니다.\n\n## 최종 일관성을 확보하기 위한 기반\n\n일반적으로 당연히 롤백을 고려할 수밖에 없겠지만, 저는 데이터의 전파와 오케스트레이터로 충분히 커버할 수 있다고 고려합니다.\n\n### CQRS\n\n쓰기 작업을 할 수 있는 노드 및 서비스는 제한적으로 존재하지만, 읽기는 모두가 가능한 구조입니다. 이러한 구조를 가지게 되면 각 구간에 대한 불확정성이 줄어들고, 읽기를 위한 인프라에 데이터가 전파됨으로 모든 노드와 유관 서비스가 동일한 데이터를 공유하게 됩니다.\n\n#### CDC\n\nCDC는 데이터 변경 이벤트를 감지하여 전파하는 역할을 합니다. 그리고 이 전파를 보통 Message Bus로 하게 되며, 이러한 구조가 보통 CDC를 기반으로 EDA를 구현한 경우에 해당합니다. CQRS를 하게 되면 쓰기를 하는 곳, 예를 들어 결제와 같은 분야에서 정합성을 확보할 수 있을 것입니다. RDB를 씀으로요.\n\n하지만 읽기는 굳이 정합성을 확보해야하는 RDB에서 할 필요가 없습니다. 제가 가상 재화를 구매했다해서 서버는 저에게 즉시 갱신된 지갑을 보여줄 필요는 없기 때문이죠. 그렇다고 너무 느리면 문제가 되겠지만요.\n\n그럼 정합성을 챙길 필요가 없는, 읽기가 더 빠른 경우는 어떤 것이 있을까요? NoSQL이나 단순 스토리지, 분산 캐시 등이 있을 수 있습니다. 정합성을 챙긴 RDB에 기록 및 변경된 데이터를 CDC를 통해 이러한 NoSQL이나 분산 캐시에 기록하면, 정합성을 챙김과 동시에 최신 데이터를 빠르게 제공할 수 있을 겁니다.\n\n또한 이 방식은 EDA와 비슷하게 다른 서비스에서 특정 서비스의 데이터 변경을 쉽고 거의 바로 파악할 수 있게 되며, 상호 의존성과 결합도를 낮추는 역할을 합니다.\n\n### Orchestration-based Saga Pattern\n\n오케스트레이션 기반 사가 패턴은 일반적인 사가와 달리 여러 서비스 위의 레이어에 존재하는 오케스트레이터를 통해 트랜잭션을 관리하는 방법입니다.\n\n1. 먼저 각 서비스는 트랜잭션을 위한 데이터 CRUD와 롤백을 위한 RPC 혹은 API 엔드포인트를 제공합니다.\n2. 오케스트레이터는 요청을 받으면 해당 요청을 수행하기 위해 필요한 트랜잭션을 각 서비스의 API를 통해 구성합니다.\n3. 이를 시나리오 대로 실행하여 오케스트레이터는 분산 트랜잭션을 효과적으로 수행합니다.\n\n이 방식의 장점은 무엇보다 기존 사가 패턴이 가지는 로직적 복잡성과 메시지 큐를 거치면서 발생하는 동시성 논리 에러를 부담할 필요가 없어진다는 것입니다. \n\n### 유연하고 합리적인 복구\n\n만약 CDC 과정이나 전파 과정 등에서 지연이 발생하여 데이터 변경 사항이 순서대로 전달되지 못할 수 있습니다. 이에 대해 이벤트에 대한 timestamp나 kafka의 경우처럼 offset을 활용하거나, 상황과 정책을 통해 충돌을 유연하고 합리적으로 복구할 수 있어야 합니다.\n\n예를 들어, 특정 유저가 자신의 프로필을 숨김처리 했다고 가정하고, 데이터 변경 이벤트가 순서대로 처리되지 못하는 경우를 예시로 들어보겠습니다.\n\n관련 서비스로는 다음과 같은 케이스가 있을 것입니다.\n\n- 프로필 서비스 (Profile Service): 유저 프로필 정보 (이름, 소개, 프로필 사진 등)를 관리하는 서비스\n- 개인정보 설정 서비스 (Privacy Service): 유저의 개인정보 설정 (프로필 공개 여부, 연락처 공개 여부 등)을 관리하는 서비스\n- 알림 서비스 (Notification Service): 유저에게 프로필 변경 사항 등을 알리는 서비스 \n\n1. 유저 A가 개인정보 설정 서비스 앱을 통해 자신의 프로필을 \"숨김\"으로 설정합니다.\n2. 개인정보 설정 서비스는 유저 A의 프로필 공개 처리 이벤트를 발행합니다.\n3. 유저 A가 개인정보 설정 서비스 앱을 통해 자신의 프로필을 \"공개\"로 설정합니다.\n4. 개인정보 설정 서비스는 유저 A의 프로필 숨김 처리 이벤트를 발행합니다.\n5. 유저 A는 프로필 숨김 처리 직후, 프로필 서비스 앱을 통해 다시 자신의 프로필을 \"숨김\"으로 설정합니다.\n6. 개인정보 설정 서비스는 유저 A의 프로필 공개 처리 이벤트를 발행합니다.\n\n그러면 발생되는 이벤트는 총 프로필 공개, 프로필 숨김, 프로필 공개 순으로 3가지가 됩니다. 만약 발행에 문제가 되어 메시지가 프로필 공개 -> 프로필 공개 -> 프로필 숨김 순으로 오게 되고, 별도의 방어 로직이 없다면 알림 서비스는 유저 A의 프로필이 공개되어 있음에도 불구하고, 결과적으로 숨김 처리되었다고 다른 유저들에게 알림을 주게 될 것입니다.\n\n하지만 예를 들어, 메시지에 timestamp가 포함되거나 offset이 들어가 있다면 이야기가 달라집니다. 쉬운 방법으론 아래 2가지 방법이 있을 겁니다.\n\n- 해당 값을 보고 최근에 다룬 것에 비해 이전 값이면 메시지를 무시합니다.\n- 단기간만 저장하는 프로필 공개 상태 변경 카운트를 작성하여 공개 2회면 +2, 숨김 1회면 -1로 결과적으로 공개로 처리합니다.\n\n예시의 경우는 보기 힘든 케이스지만, 실제로 특정 상태 변경이 자주 발생하는 경우에는 이러한 유연하고 합리적인 복구 정책이 필요할 것입니다.\n\n## 결론\n\nMSA는 분명 훌륭한 아키텍처지만, `잠재적인 데이터 불일치`라는 중대한 문제가 남아 있습니다. 모놀리스와 단일 DB에선 당연했던 강한 일관성은 분산 시스템에서 유지하기 어려운 선택지가 되었습니다.\n\n이에 대해 최종 일관성이 대안이 되었으며, 제가 괜찮다고 고려한 안을 몇가지 나열했습니다. CQRS, 오케스트레이션 기반 사가 패턴, 유연한 복구 등이 이러한 문제에 직면한 분들에게 도움이 되길 바랍니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/043_data_consistency_in_msa/", "created_date": "2025-03-08T21:33:08Z"}
{"title": "소..솔직히 유지보수는 아키텍트의 책임이라고 생각해요...", "content": "![goto](/img/035/goto_hitori.webp)\n\n## 메락아, 그게 무슨 소리니?\n\n### 아니 일단 들어봐\n\n서비스가 되었든, 소프트웨어가 되었든, 어떤 프로젝트에 대해 보통 기업은 신기술에 밝거나, 손이 빠르거나, 임원의 말을 잘 듣는 사람에게 첫 스타트를 시키는 경우가 많습니다. 하지만 단순히 그렇게 되어서는 서비스나 소프트웨어의 수명이 줄어들어서, 유지보수 비용의 증가 뿐만 아니라 해당 프로젝트를 대체할 새로운 프로젝트를 해야할 수 있어, 큰 지출로 다시 돌아올 수 있습니다.\n\n### 그래서?\n\n만약 아키텍트가 시작할 때, 어떠한 철학이나 체계 없이 주먹구구로 프로젝트를 진행하면 어떤 일이 벌어질까요?\n\n1. 프로젝트에 참여해 있는 사람들이 가이드라인 없이 인프라를 구성해서 전체 구조가 복잡해질 수 있습니다.\n2. 잘못된 설계로 인해 새로운 컴포넌트를 추가할 때, 어느 영역에 추가해야할 지 혼란이 올 수 있습니다.\n3. 무분별하거나 무책임한 gray zone이 생겨나서, 작업자들이 상호 간 업무 파악을 할 수 없을 수 있습니다.\n4. 잘못된 도메인/관심사 분리로 인해 내부 로직이 필요 이상으로 복잡해질 수 있습니다.\n5. 프론트엔드 - 백엔드 간의 API 호출에 대해 중복된 프로세스를 위한 코드가 생성될 수 있습니다.\n6. 잘못된 데이터 모델링으로 인해 무결성과 확장성을 확보하기 어려워 집니다.\n7. 경직된 권한과 보안 정책으로 인해 새로운 컴포넌트를 추가할 때마다 작업이 늘어날 수 있습니다.\n8. 일관되지 않은 로깅과 모니터링 과정으로 인해, 사후 처리에 어려움을 겪을 수 있습니다.\n9. 적절하지 않은 테스트 환경으로 인해 프로젝트의 안정성을 헤칠 수 있습니다.\n10. 필요 이상으로 경직된 운영 환경으로 인해 문제 파악에 필요 이상의 노력과 시간이 투자될 수 있습니다.\n11. 적절하지 않은 기술 도입으로 프로젝트 전체의 한계를 앞당길 수 있습니다.\n\n전부 생각나는 대로 적은 거라 이해가 안되거나, 공감이 안되거나, 할 수 있습니다.\n\n## 그럼 뭘 하길 원하는데?\n\n> 모든 것은 트레이드오프입니다.  \n> 당연히 지금부터 쓰는 게 무조건 정답도 아니고, 경우에 따라서는 완벽한 오답일 수 있습니다.\n\n### 사용자 요청과 도메인 분리는 별개의 레이어에서 해주세요.\n\n사용자의 요청은 각 클라이언트에서 한 페이지나 컴포넌트에서 필요한 데이터를 한번에 요청할 것입니다. 그걸 위해 백엔드는 어떻게 구성되어야 적합할까요? 그건 프로젝트마다 다르겠지만 규모가 있는 프로젝트라면, 네임스페이스가 나뉘어지고 각 네임스페이스마다 담당할 부분이 할당될 겁니다. 그 나뉘는 기준이 도메인과 데이터를 중심으로 이루어졌으면 합니다.\n\n요청할 때 같은 페이지 영역, 혹은 컴포넌트 영역에 있기 때문에 해당 하는 데이터의 관리 주체가 주로 사용하는 네임스페이스가 아닌 다른 네임스페이스로 넘어가는 일은 있어서는 안됩니다. 그러면 해당 데이터를 주로 사용하는 네임스페이스는 매번 자기는 쓰지도 않는 다른 네임스페이스에게 데이터 엑세스를 요청해야합니다. 이는 매우 불합리하고 비효율적인 구조입니다.\n\n그러면 어떻게 해야 클라이언트 입장에서 같은 영역에서 쉽게 다룰 수 있으면서, 두 네임스페이스가 상호 간 데이터 관리 주체를 적절히 가져가게 할 수 있을까요? 일반적인 MSA라면 BFF(Backend-For-Front)나 Aggregator 패턴 등을 활용할 수 있을 것입니다. 그러면 같은 API 라우트 밑에서 서로 다른 네임스페이스의 데이터를 충돌 없이 가져와 제공할 수 있습니다.\n\n그리고 무엇보다 아키텍트는 설계할 때, 이러한 충돌이 발생할 것같으면 미리 두 네임스페이스의 담당자들과 이야기하여 적절히 관리 주체를 정해야할 것입니다.\n\n### 아키텍처의 기본 골격을 우선 만들어 주세요.\n\n도시가 원활히 돌아가려면 상-하수도, 전기, 가스, 도로 같은 기반 시설이 잘 설치되고 동작해야합니다. 버스, 지하철, 기차, 비행기 등의 운송 수단도 마련하고, 도로나 철도 및 철도역, 공항 등을 잘 설치하고 규칙을 제정해서 운영하면 사람과 물류가 막힘 없이 활발히 돌 것이고, 이는 도시의 활력으로 이어질 것입니다. 상-하수도 및 정수처리시설이 잘 발달되고 설치되어 있어야 도시의 기본적인 위생을 지킬 수 있을 것입니다. 전기와 가스가 잘 운반되고 제공되어야 생활 속에서, 산업에서 도시를 움직이게 할 것입니다.\n\n우리는 이 도시의 구조에서 무엇을 느낄 수 있을까요? 저는 아키텍처에 기본적으로 필요한 것들에 대해 우선 확실히 하고 넘어가야, 그 위에서 어떤 컴포넌트가 올라가도 멀쩡히 돌아갈 수 있음을 느껴야 한다고 생각합니다.\n\n당연히 프론트엔드와 백엔드 API 사이나 백엔드 내에서 각 컴포넌트 간에도 주고 받아야할 기본적인 운송 수단, 네트워크 상에서 어떤 통신 프로토콜(TCP, UDP, HTTP/1, HTTP/2, QUIC, HTTP/3, MessageQueue, gRPC 등)로 주고 받을 것인지 정하고, 화물이 되는 데이터를 어떻게 포장(JSON, protobuf, xml, CSV, flatbuf, 등)할 것인지, 그리고 각 데이터는 어떤 값이 필수적으로 포함되어야 하는지 등을 확실히 잡고 넘어가야 차후 각자 추가적인 서비스를 기획 및 개발하면서 중구난방으로 결과물이 튀는 상황을 방지할 수 있습니다.\n\n또한 로그를 처리하는 것은 마치 하수도와 같다고 생각합니다. 모든 컴포넌트에서 로그는 마치 하수처럼 끊임없이 흘러내립니다. 이 하수도 설계 및 구성를 어떻게 하고, 로그 포맷과 수집을 얼마나 잘 잡냐에 따라 각 작업자들이 큰 고민 없이 각 플랫폼에 맞는 로그 라이브러리를 개발하고, 적용할 수 있을 것입니다. 그리고 차후 모니터링 시에 통일된 형식으로 인한 빠른 문제 파악에 기여할 수 있을 것입니다.\n\n만약 아키텍트가 이에 가이드라인을 처리해주지 않는다면, 각 작업자들이 모여서 별도로 시간을 들여서 미팅 후 결정하고, 보통은 각 영역마다 각자에 맞는 데이터/로그 형식을 쓰게 되어, 추후 과도한 파편화로 인한 불편함을 겪게 됩니다.\n\n### 적절한 기술과 구조를 도입해주세요.\n\n각 컴포넌트에는 할당된 명확한 책임과 역할이 있을 것입니다. 그럼 그것에 부합하는 외부 솔루션이나 기존에 있는 best practice가 존재할 것입니다. 그들이 무엇으로, 어떻게 문제를 해결했었고, 운영하고 있는지 알아보고 힌트를 받아 적절한 기술과 구조를 도입할 수 있을 것입니다. 혹은 아예 새로운 것이라도, 필요하다면 PoC를 통해 동작을 확인하고 도입한 후에, 예상되는 운영 시의 문제에 대해 유연하게 대처하도록 가이드를 남겨야할 것입니다.\n\n이는 공통적으로 작성 언어나 동작하는 플랫폼이 있을 것입니다. 그리고 프론트엔드라면 여러 상태관리 솔루션이나 컴포넌트 구성, 아키텍처 디자인 패턴 등이 있을 것입니다. 마지막으로 백엔드라면 메시지 큐나, 분산 캐시 솔루션, 서비스 디스커버리 등이 있을 것입니다.\n\n이에 대해 저로서는 최대한 최소 비용에 최대 효과를 얻을 수 있는 방법을 고르는 편입니다. 물론 이는 현재 상황과 1차원적인 목적 뿐만 아니라, 추가로 기획이 추가되었을 때 어느정도는 유연하게 대처할 수 있는 정도, 프로젝트가 잘되어 많은 트래픽이나 작업을 처리해야할 때 최소한의 대처만으로 부담할 수 있는 정도가 필요하다 생각합니다.\n\n그렇기에 어느정도의 오버 엔지니어링을 용인되어야 합니다. 지금 당장의 요구 사항과 기획에만 맞추어 엔지니어링을 하는 것은 프로젝트의 구조적, 기술적 한계를 앞당겨 경직되게 만들고, 수명을 단축시킬 수 있습니다.\n\n### 각자의 바운더리가 되는 부분에 대해 명확히 해주세요.\n\n기본적으로 도메인이든 페이지든 R&R을 잘 나누었다 하더라도, 결국에는 경계에 아슬아슬하게 겹치는 부분이 생길 것입니다. 이 부분에 대해서 리드가 제대로 역할해주지 않는 다면, 해당 부분은 그 누구도 적극적으로 관리하려 하지 않을 것이고, 이내 곪아 누구도 감히 손대지 못하는 부분이 될 것입니다.\n\n이에 대해 리드라면 솔로몬처럼 반토막을 내서라도 각 담당 도메인에 보내줘야 합니다. 대표적인 gray zone으로 MSA에선 aggregator 패턴이나 BFF가 있습니다. 이 영역은 클라이언트가 필요로 하고, 여러 서비스 앱에서 데이터를 제공해야합니다. 누가 주도적으로 비즈니스 로직을 작성할 것인지는 매우 모호해집니다. 기본적으로 정한 아키텍처의 구조가 있다면, 그걸 지키기 위해 이러한 구조를 적용해야한다면, 과감하게 개입해서 정해주었으면 합니다.\n\n### 원하는 개발 방향성과 방법론을 제시해주세요.\n\n각 작업자는 서로 다른 개발 방식, 테스트 방식, 버저닝 방법, 배포 수단 등을 가지고 있습니다. Git을 다룰 때, 공통적으로 크게 어떤 식으로 브랜치를 나누고 머지를 할 것인지가 있을 수 있습니다. 작업의 시작과 과정, 끝을 어떻게 수행할 것인지를 보여줄 수도 있을 겁니다. 배포에 대해서 스크립트나 별도의 배포를 위한 ant를 사용할 수도 있으며, gitlab runner나 jenkins같은 CI/CD 툴을 이용할 수도 있을 것입니다.\n\n혹은 테스트 방식에 대해서도, 어느정도 고민을 미리 하여 효과적인 테스트 환경을 구성하는 것에 일조할 수 있을 것입니다. 예를 들어, 전체 통합 테스트를 위한 스테이징을 만들더라도, 서로 다른 개발 중인 기능을 테스트 하고 싶다면 어떻게 해야하는 지, 만약 좋은 방법이 있다면 추가적으로 필요한 인프라 요소나 기능은 무엇이 있는 지를 검토 후 적용해주었으면 합니다. 혹은 개발 과정에서의 생산성과 안정성을 확보하기 위한 문화, 코드 리뷰나 버저닝 등에 대해서도 최소한의 가이드라인은 제시하면 좋을 것같습니다.\n\n그렇지 않았을 때, 해당 컴포넌트를 받은 작업자들의 역량에 따라 결과물이 천차만별로 달라지는 결과를 맞이해서, 전체적인 결과물이 조화롭게 구성되지 못하는 상황이 펼쳐질 것입니다. 이상적인 조직 구성에선 문제 없이 돌아갈 것이지만, 일반적으론 그렇게 되지 않을 확률이 높다고 느낍니다. 전반적인 코드 품질에 대해서도 아키텍트는 어느정도 책임을 져야한다 생각합니다.\n\n## 결론은\n\n아키텍트는 매우 어려운 위치라고 생각됩니다. 하지만 많은 부분에 영향을 주고, 그에 대한 책임을 질 수 있기에, 정말 멋지고 매력적인 직책이라 생각합니다. 언젠가 저도 기회가 된다면, 이러한 아키텍트로서 역할을 할 수 있길 바랍니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/035_service_planner/", "created_date": "2024-08-12T00:08:12Z"}
{"title": "더 나은 제안, Kemtls", "content": "`본 문서는 https://blog.cloudflare.com/kemtls-post-quantum-tls-without-signatures/ 의 번역이 일부 포함되어 있습니다.`\n\n***\n\n## 더 나은 제안: KEMTLS\n\n서명이 외에 인증에 사용되는 다른 메커니즘에는 긴 역사가 있습니다. Signal 프로토콜, Noise 프레임워크 또는 WireGuard와 같은 최신 프로토콜은 인증을 위해 키 교환 매커니즘에 의존합니다. 하지만 이런 것들은 길게보면 중요한 키가 이해관계자 사이에 알려지기 때문에 TLS 1.3에 적합하지 않습니다.\n\n(중략)\n\nKrawczyk와 Wee의 OPTLS 제안은 NIKE(Non-Interactive Key Exchange)를 통해 악수 없이 TLS 핸드셰이크를 합니다. 그러나 포스트 퀀텀 NIKE를 위한 다소 효율적인 유일한 선택지는 CSIDH이며, 보안은 현재 진행 중인 논의 내용입니다. 하지만 우리는 이 아이디어를 바탕으로 KEM을 인증에 사용할 수 있습니다. 현재 실험 단계인 KEMTLS는 포스트 퀀텀 KEM 핸드셰이크를 대체합니다. 이 내용은 ‘Post-Quantum TLS Without Handshake Signatures’에서 Peter Schwabe, Douglas Stebila 그리고 Thom Wiggers가 디자인 및 소개했습니다.\n\nKEMTLS는 TLS 1.3과 같은 목표(인증, 기밀성, 무결성)를 양자 컴퓨터에 대응하여 성취하고자 합니다. 하지만 TLS 1.3의 핸드셰이크와는 조금 다른 부분이 있습니다. KEMTLS는 클라이언트의 인증이 필요하지 않을 때, 클라이언트에서 서버로 보내는 두번째 TLS 메시지부터 암호화된 데이터를 보낼 수 있습니다. 그리고 양쪽 모두의 인증이 필요할 때는 클라이언트에서 서버로 보내는 세번째 TLS 메시지부터 암호화된 메시지를 보낼 수 있습니다. TLS 1.3에서는 서버도 첫번째 응답에 암호화된 데이터를 전송할 수 있습니다. ~~KEMTLS에서 클라이언트의 인증이 필요하지 않을 때, TLS 1.3과 같은 핸드셰이크 라운드 트립 이후에서야 첫번째 암호화된 데이터를 전송할 수 있습니다.~~\n\n직관적으로 TLS 1.3의 핸드셰이크 서명은 서버 인증서에 인증된 공개키에 대응하는 개인키의 소유를 증명합니다. 이러한 서명 방식에서 소유를 증명할 수 있는 쉬운 방법이 있습니다. 다른 소유를 증명하는 방법은 키를 교환함으로 소유를 증명합니다. 키 파생 순서를 잘 생각해보면 서버는 인증된 공개키에 상응하는 개인키를 가지고 있을 경우에만 클라이언트의 메시지를 복호화할 수 있습니다. ~~그래서 암묵적 인증이 수행됩니다.~~ KEMTLS는 여전히 장기적인 KEM 키의 인증을 위해 인증 기관에 의존하고 있음을 알 필요가 있다. \n\nKEMTLS를 사용하면 핸드셰이크 과정에서 교환되는 데이터는 명시적이기보다 암시적으로 인증된다. 그리고 조금 약하게 저하된 회복성과 전방향 기밀성(forward secrecy)을 가집니다. 그러나 한번 KEMTLS 핸드셰이크가 완료되면 회복성의 완전한 소멸과 전방향 기밀성이 확보됩니다.\n\n***\n\n## 테스트 코드\n\n위 내용은 Cloudflare의 블로그의 내용을 일부 번역한 것입니다. 제 영어 실력이 좋지 않아 이해가 안되실테니 cloudflare에서 작성한 `sidh` 라이브러리를 사용하여 작성한 간단한 코드를 보여드리겠습니다.\n\n```go\nimport (\n\t\"crypto/rand\"\n\t\"errors\"\n\n\t\"github.com/cloudflare/circl/dh/sidh\"\n\t\"github.com/twisted-lyfes/utility/dh\"\n)\n\ntype KeyPair struct {\n\tpriv *sidh.PrivateKey\n\tpub  *sidh.PublicKey\n\tkem  *sidh.KEM\n}\n```\n\n먼저 `KeyPair` 구조체를 선언합니다. 멤버는 sidh의 개인키, 공개키, 그리고 `KEM`의 포인터를 저장합니다.\n\n```go\nfunc NewKeyPair() (dh.DH, error) {\n\tpriv := sidh.NewPrivateKey(sidh.Fp751, sidh.KeyVariantSike)\n\tif err := priv.Generate(rand.Reader); err != nil {\n\t\treturn nil, err\n\t}\n\tpub := sidh.NewPublicKey(sidh.Fp751, sidh.KeyVariantSike)\n\tpriv.GeneratePublicKey(pub)\n\tkem := sidh.NewSike751(rand.Reader)\n\n\treturn &KeyPair{\n\t\tpriv: priv,\n\t\tpub:  pub,\n\t\tkem:  kem,\n\t}, nil\n}\n```\n\n첫번째 생성자는 `KeyPair`에 개인키와 공개키, KEM을 생성하고 저장합니다.\n\n```go\nfunc NewKem() (dh.DH, error) {\n\treturn &KeyPair{\n\t\tpriv: nil,\n\t\tpub:  nil,\n\t\tkem:  sidh.NewSike751(rand.Reader),\n\t}, nil\n}\n```\n\n두번째 생성자는 `KeyPair`에 개인키와 공개키를 `nil`로 비워두고 KEM만 생성해서 반환합니다.\n\n```go\nfunc (k *KeyPair) Encapsulate(publicKey []byte) (ct []byte, ss []byte, err error) {\n\tct = make([]byte, k.kem.CiphertextSize())\n\tss = make([]byte, k.kem.SharedSecretSize())\n\tother := sidh.NewPublicKey(sidh.Fp751, sidh.KeyVariantSike)\n\tif err := other.Import(publicKey); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif err := k.kem.Encapsulate(ct, ss, other); err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn ct, ss, nil\n}\n```\n\n`Encapsulate` 메서드는 외부 공개키를 받고, KEM에서 암호문 크기와 비밀키 크기를 받아 새로운 `[]byte`를 만들어 각각 생성 받아 반환하는 함수입니다. 이 중 암호문을 서버에 전송하여 지금 생성한 비밀키를 공유하는 데에 사용합니다.\n\n```go\nfunc (k *KeyPair) Decapsulate(cipherText []byte) (ss []byte, err error) {\n\tss = make([]byte, k.kem.SharedSecretSize())\n\tif err := k.kem.Decapsulate(ss, k.priv, k.pub, cipherText); err != nil {\n\t\treturn nil, err\n\t}\n\treturn ss, nil\n}\n```\n\n`Decapsulate` 메서드는 공유받은 암호문을 받아서 자신의 개인키와 공개키를 활용하여 비밀키를 복호화합니다.\n\n```go\nfunc TestKeyExchangeA(t *testing.T) {\n\tclient, err := sidh.NewKem()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tserver, err := sidh.NewKeyPair()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tctClient, ssClient, err := client.Encapsulate(server.ExportPublic())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tssServer, err := b.Decapsulate(ctClient)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !bytes.Equal(ssClient, ssServer) {\n\t\tt.Fatal(\"secret shares do not match\")\n\t}\n\tt.Log(\"secret shares match\")\n}\n```\n\n방금 코드를 기반으로 작성한 테스트 파일입니다. `client`와 `server`는 문자 그대로 클라이언트와 서버를 의미합니다. 보통 서버에서 공개키를 공개하고 있을 테니 `server`가 공개키를 제공하는 것으로 코드를 작성하였습니다. 먼저 클라이언트는 KEM만 생성하고 서버는 키쌍을 전부 생성합니다. 클라이언트에서 서버의 공개키를 받아서 `Encapsulate` 메서드를 통해 암호문(`ctClient`)과 비밀키(`ssClient`)를 만듭니다. 그리고 클라이언트는 생성된 암호문을 서버에 전송하고 서버는 `Decapsulate` 메서드를 사용하여 비밀키(`ssServer`)를 생성합니다. 마지막으로 이 두 비밀키가 동일한지 확인합니다.\n\n```bash\nRunning tool: C:\\Program Files\\Go\\bin\\go.exe test -timeout 30s -run ^TestKeyExchangeA$ github.com/twisted-lyfes/utility/dh/sidh\n\nok  \tgithub.com/twisted-lyfes/utility/dh/sidh\t0.060s\n```\n\n테스트를 실행하면 성공하는 것을 확인할 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/000_kemtls/", "created_date": "2021-12-21T12:07:02Z"}
{"title": "스마트폰 파괴 방법과 매개변수 덕타핑", "content": "## 개요\n\n인터페이스 비교로 에러를 처리하는 방식에 기반하여 인터페이스로 매개변수를 덕타이핑하는 건 어떨까 해서 한번 시도해봤습니다.\n\n### 아이디어\n\n아이디어는 쉽습니다. 베이스가 되는 인터페이스와 구조체를 작성합니다. 이후 필요에 따라 매개변수를 얻을 수 있는 새로운 인터페이스를 제공하고 해당 인터페이스에 대한 구조체를 작성합니다. 그렇다면 동일한 함수 원형에 여러가지 타입의 매개변수를 넘겨줄 수 있을 것입니다.\n\n## 구현\n\n스마트폰을 파괴하는 코드를 작성해보겠습니다.\n\n### PhoneBreaker\n\n```go\npackage phonebreaker\n\ntype PhoneBreakable interface {\n\tBreak(PhoneBreakingTool) int\n}\n\ntype PhoneBreakingTool interface {\n\tHand() int\n}\n```\n\n간단하게 `PhoneBreakable` 인터페이스로 휴대폰을 파괴할 수 있는 행위를 정의해 놓았습니다.  \n그리고 가장 간단한 파괴 도구로 `Hand()`를 가지는 툴 인터페이스도 만들어 놓았습니다.\n\n### IPhoneBreaker\n\n```go\npackage iphonebreaker\n\nimport \"prac/phonebreaker\"\n\ntype IPhoneBreakingTool interface {\n\tHammer() int\n}\n\ntype IPhoneBreaker struct {\n\tLabel string\n}\n\nfunc (i *IPhoneBreaker) Break(tool phonebreaker.PhoneBreakingTool) int {\n\tr := 100\n\tr -= tool.Hand()\n\tswitch t := tool.(type) {\n\tcase IPhoneBreakingTool:\n\t\tr -= t.Hammer()\n\t}\n\treturn r\n}\n\n```\n\n`IPhoneBreaker`는 `PhoneBreakable` 인터페이스를 구현합니다. 조금 다른 점이라면 아이폰 전용 파괴 도구인 망치를 사용할 수 있을 경우 망치의 데미지를 추가로 가할 수 있습니다.\n\n### GalaxyBreaker\n\n```go\npackage galaxybreaker\n\nimport \"prac/phonebreaker\"\n\ntype GalaxyBreakingTool interface {\n\tAxe() int\n}\n\ntype GalaxyBreaker struct {\n\tLabel string\n}\n\nfunc (g *GalaxyBreaker) Break(tool phonebreaker.PhoneBreakingTool) int {\n\tr := 100\n\tr -= tool.Hand()\n\tswitch t := tool.(type) {\n\tcase GalaxyBreakingTool:\n\t\tr -= t.Axe()\n\t}\n\treturn r\n}\n\n```\n\n`GalaxyPhoneBreaker`도 아이폰과 마찬가지로 휴대폰을 파괴할 수 있고 전용 도구인 도끼를 사용하면 추가 데미지를 줄 수 있습니다.\n\n### LumiaBreaker\n\n```go\npackage lumiabreaker\n\nimport \"prac/phonebreaker\"\n\ntype LumiaBreakingTool interface {\n\tGun() int\n}\n\ntype LumiaBreaker struct {\n\tLabel string\n}\n\nfunc (l *LumiaBreaker) Break(tool phonebreaker.PhoneBreakingTool) int {\n\tr := 100\n\tr -= tool.Hand()\n\tswitch t := tool.(type) {\n\tcase LumiaBreakingTool:\n\t\tr -= t.Gun()\n\t}\n\treturn r\n}\n\n```\n\n`LumiaBreaker`도 아이폰, 갤럭시와 마찬가지로 휴대폰 파괴 가능 기술을 소지하고 있으며 루미아 파괴 전용 도구인 총을 사용하면 더 효과적으로 파괴할 수 있습니다.\n\n## 시연\n\n### 연장 준비\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"prac/phonebreaker\"\n\t\"prac/phonebreaker/galaxybreaker\"\n\t\"prac/phonebreaker/iphonebreaker\"\n\t\"prac/phonebreaker/lumiabreaker\"\n)\n\ntype HandPhoneBreakingTool struct {\n\tLabel string\n}\n\nfunc (h *HandPhoneBreakingTool) Hand() int {\n\treturn 10\n}\n\ntype IPhoneBreakingTool struct {\n\tLabel string\n\t*HandPhoneBreakingTool\n}\n\nfunc (i *IPhoneBreakingTool) Hammer() int {\n\treturn 85\n}\n\ntype GalaxyBreakingTool struct {\n\tLabel string\n\t*HandPhoneBreakingTool\n}\n\nfunc (g *GalaxyBreakingTool) Axe() int {\n\treturn 75\n}\n\ntype LumiaBreakingTool struct {\n\tLabel string\n\t*HandPhoneBreakingTool\n}\n\nfunc (l *LumiaBreakingTool) Gun() int {\n\treturn 60\n}\n```\n\n이제 각각의 도구를 구현합니다. 기본적으로 손을 이용한 파괴 데미지는 10이고 아이폰의 망치는 85, 갤럭시의 도끼는 75, 루미아의 총은 60의 데미지를 각각의 스마트폰에게만 줍니다.\n\n### 파괴 시작\n\n```go\nfunc main() {\n\tbreakers := []phonebreaker.PhoneBreakable{\n\t\t&iphonebreaker.IPhoneBreaker{Label: \"iphone\"},\n\t\t&galaxybreaker.GalaxyBreaker{Label: \"galaxy\"},\n\t\t&lumiabreaker.LumiaBreaker{Label: \"lumia\"},\n\t}\n\n\ttools := []phonebreaker.PhoneBreakingTool{\n\t\t&HandPhoneBreakingTool{Label: \"hand breaking tool\"},\n\t\t&IPhoneBreakingTool{Label: \"iphone breaking tool\"},\n\t\t&GalaxyBreakingTool{Label: \"galaxy breaking tool\"},\n\t\t&LumiaBreakingTool{Label: \"lumia breaking tool\"},\n\t}\n\n\tfor _, breaker := range breakers {\n\t\tfor _, tool := range tools {\n\t\t\tscore := breaker.Break(tool)\n\t\t\tfmt.Println(breaker, tool, score)\n\t\t}\n\t}\n}\n```\n\n각각 아이폰, 갤럭시, 루미아 파괴자를 준비합니다. 각 파괴자는 자신의 스마트폰을 파괴할 때마다 새로운 각 스마트폰이 주어질 것입니다. 그리고 각 기본, 아이폰, 갤럭시, 루미아 파괴 도구들을 준비합니다. 파괴자를 각 한번씩 도구를 돌아가며 써서 자신의 스마트폰을 파괴할 것입니다. 그 결과는 다음과 같습니다.\n\n```bash\n&{iphone} &{hand breaking tool} 90\n&{iphone} &{iphone breaking tool <nil>} 5\n&{iphone} &{galaxy breaking tool <nil>} 90\n&{iphone} &{lumia breaking tool <nil>} 90\n&{galaxy} &{hand breaking tool} 90\n&{galaxy} &{iphone breaking tool <nil>} 90\n&{galaxy} &{galaxy breaking tool <nil>} 15\n&{galaxy} &{lumia breaking tool <nil>} 90\n&{lumia} &{hand breaking tool} 90\n&{lumia} &{iphone breaking tool <nil>} 90\n&{lumia} &{galaxy breaking tool <nil>} 90\n&{lumia} &{lumia breaking tool <nil>} 30\n```\n\n다 동일한 `break(phonebreaker.PhoneBreakingTool)`을 이용했지만 각 상황에 맞게 다르게 스마트폰을 파괴했습니다.  \n이 방법을 이용하면 동일한 함수 시그니처를 활용해야하면서 다른 패러미터를 가져야할 때 유연하게 값을 전달할 수 있으며 인터페이스 타입 단언을 하는 과정을 통해 디폴트 패러미터도 흉내낼 수 있을 것입니다.\n\n### 하지만\n\n1.18beta1 기준으로, 아직은 parameter leakage가 발생할 확률이 높아서 힙 얼록이 될 것입니다.  \n그리고 패러미터로 전달할 타입을 만드는데 드는 코드가 짧지 않기 때문에 더 귀찮을 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/005_parameter-ducktyping/", "created_date": "2022-01-04T18:54:54Z"}
{"title": "Option과 default parameter", "content": "## option\n\n`option` 패키지는 직전 포스트에 작성한 `result` 패키지에서 에러 메시지가 빠진 형태입니다.\n\n### 구조체\n\n```go\ntype Option[T any] struct {\n\tvalue any\n}\n\nfunc Some[T any](value T) *Option[T] {\n\treturn &Option[T]{value: value}\n}\n\nfunc None[T any]() *Option[T] {\n\treturn &Option[T]{value: nil}\n}\n```\n\n구조체는 `result`와같이 `any` 타입을 가진 멤버 하나만 존재합니다. `Some` 생성자는 인자의 타입에 따른 `Option` 구조체를 생성하고 멤버 변수에 인자를 대입합니다. `None` 생성자는 타입 인자만 하나 받으며 해당 타입에 대한 `Option` 구조체를 반환하지만 멤버 변수는 `nil`이 대입됩니다.\n\n### 메서드\n\n#### Ok, Unwrap\n\n```go\nsomeInt := option.Some(100)\nnoneInt := option.None[int]()\n\nswitch someInt.Ok() {\n    case true:\n        fmt.Println(someInt.Unwrap())\n    case false:\n        fmt.Println(\"nothing in someInt\")\n}\n\nif !noneInt.Ok() {\n    log.Fatal(errors.New(\"noneInt is not empty\"))\n}\n```\n\n`option`에도 `Ok()` 메서드와 `Unwrap()` 메서드가 그대로 존재합니다. 대신 에러가 포함된 형태가 아니기에 `Ok()` 메서드가 `false`라면 그냥 값이 없구나 하고 코드를 작성해야합니다. 하지만 여러가지 상황에서 값이 없을 때 기본값을 꺼내야하는 코드가 필요할 것입니다.\n\n#### UnwrapOr\n\n```go\na := option.None[int]()\n\nb := a.UnwrapOr(10)\n\nfmt.Println(b)\n```\n\n`a` 변수는 `None` 생성자를 통해 생성되어 내부에 값이 없는 상황이라 `b` 변수에는 당연히 제로값인 0이 들어가야하지만 `UnwrapOr()` 메서드를 사용하여 기본값을 지정함으로 `b`에는 10이 들어가게 됩니다.\n\n## default parameter\n\n```go\nfunc Add(a *option.Option[int], b *option.Option[int]) int {\n\treturn a.UnwrapOr(100) + b.UnwrapOr(100)\n}\n\nfunc main() {\n\tfmt.Println(Add(option.Some(1), option.Some(2)))\n\tfmt.Println(Add(option.None[int](), option.Some(2)))\n\tfmt.Println(Add(nil, nil))\n}\n```\n\n간단한 `Add()` 함수를 작성하였습니다. 두개의 `Option` 구조체를 받고 `UnwrapOr()` 메서드로 값을 가져와서 연산한 후 돌려줍니다. 값이 주어질 때는 해당 값으로, 없다면 기본값인 100으로 연산을 하고 있고 엔트리 함수에서는 각기 다른 경우의 반환값을 출력합니다.\n\n```bash\n3\n102\n200\n```\n\n각각 예상된 대로 3, 102, 200이 나오는 것을 확인할 수 있습니다.\n\n이 방식이 해야할 것은 많지만 필요할 때 적은 수의 곳에 디폴트 패러미터를 써야한다면 유용하게 쓰일거라 기대합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/003_option/", "created_date": "2021-12-30T21:09:32Z"}
{"title": "프로젝트 설계에 대해", "content": "## 개요\n\n코드를 구조화하는 건 생각보다 어렵지 않다고 생각합니다. 하지만 생각보다 어렵기도 하죠. 그래서 사실 이걸 어떻게 표현해야할 지는 모르겠는데, 그냥 편하게 아키텍처를 만드는 것에서 프로젝트 구조를 짜는 것, 코드를 작성하는 것까지의 제 나름대로의 룰을 정리해보려고 합니다.\n\n## 룰\n\n### 모듈 혹은 서비스를 분리하세요.\n\n> 모듈을 서비스와 동치해서 서술합니다.\n\n아키텍처나 프로젝트 전반에서 한번에 거대한 문제를 해결하려고 하지 않습니다.  \n흔히 말하는 분할-정복 방식을 적용합니다. 각 문제를 분할하여 모듈화하고, 각 모듈을 독립적으로 개발 후 통합합니다.  \n이런 구조를 가지면 여러 레이어로 나누어진 모듈의 역할이 분리된 만큼 많은 이점이 생깁니다.\n\n1. 각 모듈의 R&R을 이해하기 쉽습니다.\n2. 각 모듈의 역할이 분리되어 있기 때문에, 각 모듈의 테스트가 용이합니다.\n3. 각 모듈에 추가 기능을 구현할 때, 다른 모듈에 영향을 미치지 않습니다.\n4. 각 모듈 간의 의존성이 낮아져, 모듈 간의 결합도가 낮아집니다.\n5. 필요에 따라 모듈을 재사용할 수 있습니다.\n6. 각 모듈을 독립적으로 배포할 수 있습니다.\n7. 장애가 발생했을 때, 특정 모듈의 문제임을 빠르게 파악할 수 있습니다.\n8. 각 모듈의 문제가 전체 시스템에 영향을 미치지 않습니다.\n\n### 서비스 간 통신에 메시지 버스를 적극적으로 활용하세요.\n\n각 서비스 간 통신에 여전히 메시지 버스는 유용한 도구이며 아키텍처입니다.    \n메시지 버스를 활용하면 다음 2가지 방법의 정보 교환이 가능합니다.\n\n1. 특정 토픽(혹은 서브젝트)를 통한 데이터 교환\n2. 특정 토픽(혹은 서브젝트)를 통한 시그널 교환 및 직접 연결\n\n이 중 첫번째는 당연히 메시지 버스에 직접 큰 페이로드를 전달하는 방식입니다.  \n그리고 두번째 방식은 메시지 버스를 통해 각 서비스는 자신의 연결 정보를 전달하는 방식입니다.  \n이 방식은 다음 과정을 거칩니다.\n\n1. A 서비스 군은 특정 토픽(혹은 서브젝트)를 구독합니다.\n2. B 서비스 군은 A 서비스에 연결을 해야할 때, 해당 토픽(혹은 서브젝트)에 메시지를 요청합니다.\n3. 구독 중이던 A 서비스 중 하나가 메시지를 받아, B 서비스에 연결 정보를 전달합니다.\n4. B 서비스는 A 서비스에 직접 연결합니다. (TCP나 HTTP, gRPC 등)\n\n이런 방식은 각 서비스 간의 결합도를 낮추고, 서비스 간의 통신을 효율적으로 할 수 있습니다.\n\n### 모듈을 작성할 때 레이어를 구분하세요.\n\n모듈을 작성할 때, 데이터의 흐름에 따른 레이어를 구분합니다.  \n레이어를 분리하게 되면, 다음 이점이 생깁니다.\n\n1. 각 레이어의 역할이 분리되어 있기 때문에, 각 레이어의 테스트가 용이합니다.\n2. 각 레이어의 R&R이 명확해지기 때문에, 각 레이어의 역할을 이해하기 쉽습니다.\n3. 각 레이어 간의 결합도가 낮아집니다. 하지만 필요에 따라 응집성은 높아집니다.\n4. 언제라도 같은 레이어 내의 다른 모듈로 교체할 수 있습니다.\n5. 데이터의 흐름에 따라 레이어를 구분하기에, 각 모듈의 선후 관계를 명확히 할 수 있습니다.\n6. 데이터의 흐름을 역행하는 참조를 미연에 방지하여 더 안전한 구조를 가질 수 있습니다.\n\n### 하나의 모듈과 서비스는 가급적 하나의 역할을 수행하세요.\n\n모듈과 서비스는 하나의 역할을 수행하는 것이 가장 이상적입니다.  \n만약 단일 모듈이나 서비스가 여러 역할을 수행하게 될 경우에, 다음과 같은 문제가 발생할 수 있습니다.\n\n1. 각 역할의 R&R이 명확하지 않아, 각 역할의 테스트가 어려워집니다.\n2. 각 역할의 의존성이 높아져, 각 역할 간의 결합도가 높아집니다. 이는 한 역할의 문제가 다른 역할에 영향을 미칠 수 있습니다.\n3. 각 역할의 역할이 분리되어 있지 않아, 각 역할의 역할을 이해하기 어려워집니다.\n4. 하나의 역할이 다른 역할에 영향을 미치지 않는다는 보장이 없어집니다. 장애가 전파될 수 있습니다.\n\n### 어그리게이터를 적극적으로 활용하세요.\n\n어그리게이터는 여러 모듈과 서비스에서 데이터를 수집하여, 하나의 데이터로 만드는 역할을 수행합니다.  \n이러한 어그리게이터는 위의 하나의 모듈과 서비스는 하나의 역할을 수행한다는 원칙을 지키게 해줍니다.  \n어그리게이터는 그 자체로 데이터를 수집하고 하나의 데이터로 만드는 역할을 수행하기 때문입니다.  \n이는 레이어를 나눈다는 개념에서도 우수한 이점을 가집니다.\n\n### 같은 곳에서의 입력과 출력을 담당하는 기능은 같은 모듈에 작성하세요.\n\n같은 곳에서의 입력과 출력을 담당하는 기능은 같은 모듈에 작성하는 것은 불필요한 실수와 소통의 오류를 줄일 수 있습니다.  \n예를 들어 DB나 레디스에 데이터를 저장하고, 그 데이터를 읽어오는 기능을 생각해봅시다.  \n만약 서로 다른 모듈에서 쓰기 기능과 읽기 기능을 작성한다면, 다음과 같은 문제가 발생할 수 있습니다.\n\n1. 쓰기 기능과 읽기 기능이 서로 다른 모듈에 작성되어 있기 때문에, 서로 다른 모듈 간의 스키마 혹은 데이터 형식에 대한 일치가 필요합니다.\n2. 쓰기 기능과 읽기 기능이 서로 다른 모듈에 작성되어 있기 때문에, 서로 다른 모듈 간의 테스트가 어려워집니다.\n3. 쓰기 기능이나 읽기 기능 중 하나가 추가 및 삭제, 변경 사항이 발생했을 때, 다른 곳에 영향을 미칠 수 있습니다.\n\n이러한 문제를 방지하기 위해, 같은 곳에서의 입력과 출력을 담당하는 기능은 같은 모듈에 작성합니다.\n\n### 패러미터는 가급적 구조체나 클래스로 전달하세요.\n\n패러미터는 가급적 구조체나 클래스로 전달하는 것이 좋습니다.  \n만약 전통적으로 함수의 인자에 여러 데이터 타입을 나열하는 방식으로 전달한다면, 다음과 같은 문제가 발생할 수 있습니다.\n\n1. 함수의 인자가 많아질수록, 함수의 호출이 복잡해집니다.\n2. 함수의 시그니처가 불가피하게 변경되면, 함수를 호출하는 모든 곳에서 변경이 필요합니다.\n\n이러한 문제를 방지하기 위해, 패러미터는 가급적 구조체나 클래스로 전달합니다.  \n이는 주로 환경 설정이나, 컴파일 타임 의존성 주입같은 부분에서 유용합니다.\n\n### 반응형 프로그래밍을 적극적으로 활용하세요.\n\n반응형 프로그래밍은 데이터의 흐름을 이벤트 스트림으로 처리하는 프로그래밍 패러다임입니다.  \n하지만 이는 단순히 프로그래밍 패러다임을 넘어, 아키텍처에도 적용할 수 있습니다.\n\n1. 데이터의 흐름을 이벤트 스트림으로 처리하기 때문에, 데이터의 흐름을 추적하기 쉽습니다.\n2. 필요할 때만 데이터를 처리하기 때문에, 불필요한 데이터 처리를 줄일 수 있습니다.\n3. 그때그때 데이터를 처리하기 때문에, 데이터의 처리 속도가 빨라집니다.\n\n이를 효율적으로 사용하는 좋은 방법은 채널(혹은 메일박스), 메시지큐, 이벤트 버스 등을 사용하는 것입니다.\n\n### TTL을 적극적으로 활용하세요.\n\nTTL은 Time To Live의 약자로, 데이터의 유효 시간을 의미합니다.  \n모든 데이터는 영원히 유효할 수는 없습니다.  \n데이터의 유효 시간을 설정하면, 다음과 같은 이점이 생깁니다.\n\n1. 불필요한 데이터를 줄일 수 있습니다.\n2. 피치 못할 사유로 인해 데이터가 무한정 쌓이는 것을 방지할 수 있습니다.\n3. 불필요한 데이터를 처리하지 않아도 되기 때문에, 전반적인 성능 향상을 기대할 수 있습니다.\n\n### 모든 동작에 timeout과 delay를 적용하세요.\n\n모든 동작에 timeout과 delay를 적용하는 것은 매우 중요합니다.  \n만약 timeout과 delay를 적용하지 않는다면, 다음과 같은 문제가 발생할 수 있습니다.\n\n1. 네트워크나 I/O 등의 문제로 인해, 동작이 무한정 지연될 수 있습니다.\n2. 필요 이상으로 오래 걸리는 연산으로 인해 데드락이 발생할 수 있습니다.\n3. 불필요한 요청을 지속적으로 보내는 것을 방지할 수 있습니다.\n\n### 기능을 수정하기보다 추가하세요.\n\n기능을 수정하는 것보다 추가하는 것이 더 좋습니다.  \n기능을 수정하게 되면, 기존에 작성된 코드에 영향을 미칠 수 있습니다.  \n하지만 기능을 추가하게 되면, 기존에 작성된 코드에 영향을 미치지 않습니다.  \n이는 다음과 같은 이점을 가집니다.\n\n1. 기존 코드의 안정성을 보장할 수 있습니다.\n2. 기존에 검증된 테스트를 다시 검증할 필요가 없습니다.\n3. 기존에 작성된 코드를 수정할 필요가 없습니다.\n   \n### 함수나 변수 이름이 정책을 나타내도록 작성하세요.\n\n함수나 변수 이름이 정책을 나타내도록 작성하는 것은 매우 중요합니다.  \n만약 함수나 변수 이름이 정책을 나타내지 않는다면, 우리는 그것을 파악하기 위해 히스토리를 찾아야 합니다.  \n하지만 함수나 변수 이름이 정책을 나타내도록 작성한다면, 우리는 그것을 파악하기 위해 이름만 보면 됩니다.\n\n예를 들어, `ageUnder18`이라는 변수가 있다고 가정해봅시다.  \n이 변수는 18세 미만의 나이를 나타내는 변수일 것입니다.  \n이름을 보고 18세 미만의 나이를 나타낸다는 것을 알 수 있습니다만, 왜 존재하는 지는 알 수 없습니다.  \n하지만 이름이 `isAdult`라면, 성인인지 아닌지를 파악하기 위한 함수임을 알 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/030_architect/", "created_date": "2024-06-19T16:45:37Z"}
{"title": "vcpkg를 cgo에 사용하기", "content": "## vcpkg?\n\n[`vcpkg`](https://github.com/microsoft/vcpkg)는 Microsoft에서 발표한 C/C++ 패키지 매니저입니다.  \nvcpkg의 레포에 등록된 패키지들을 `install`을 통해 손쉽게 설치하고 이용할 수 있습니다.\n\n### vcpkg 설치\n\nvcpkg는 전역으로 사용할 수 있지만 이번 글에서는 프로젝트 내에서 사용하도록 설정하겠습니다.\n\n```bash\nmkdir prac\ngit clone https://github.com/microsoft/vcpkg.git\n```\n\n`prac` 폴더를 생성한 후, vcpkg를 클론합니다.  \n클론하면 vcpkg 폴더가 생성되고, 최초 1회 `bootstrap-vcpkg.sh`(or 윈도우의 경우, `bootstrp-vcpkg.bat`)을 실행해야 합니다.\n\n```bash\ncd vcpkg\n./bootstrap-vcpkg.sh\n```\n\n그럼 vcpkg 폴더에 `vcpkg` 파일이 생성됩니다.  \n이 파일을 실행함으로 패키지를 설치할 수 있습니다.\n\n### 패키지 설치\n\n저는 ffmpeg를 설치해보겠습니다.\n\n```bash\n./vcpkg install ffmpeg\n```\n\n저는 맥이라 그런지, `nasm`을 설치하라고 떠서 `brew install nasm`으로 설치했습니다.\n\n```bash\n-- Performing post-build validation\nStored binaries in 1 destinations in 7.9 s.\nElapsed time to handle ffmpeg:arm64-osx: 1.9 min\nTotal install time: 1.9 min\nTo use ffmpeg add the following to your CMake project:\n\n    find_package(FFMPEG REQUIRED)\n    target_include_directories(main PRIVATE ${FFMPEG_INCLUDE_DIRS})\n    target_link_directories(main PRIVATE ${FFMPEG_LIBRARY_DIRS})\n    target_link_libraries(main PRIVATE ${FFMPEG_LIBRARIES})\n```\n\n설치가 끝나면 위와 같은 내용이 출력됩니다.  \n설치가 잘 되었고, 하단 4 라인을 CmakeLists.txt에 추가하여 사용하면 된다는 뜻입니다.  \n하지만 저희는 cgo를 사용할 것이기에 다른 메뉴얼을 참고해야합니다.\n\n### 수동 통합\n\nvcpkg는 [수동 통합](https://learn.microsoft.com/ko-kr/vcpkg/users/buildsystems/manual-integration) 문서를 통해 `include`와 `lib` 폴더를 지정해 주는 방법을 소개하고 있습니다.  \ncgo에서는 어쩔 수 없이 이 방식을 채택했습니다.\n\n## cgo\n\ncgo는 익히 아시다시피 go 언어에서 C 코드를 실행할 수 있게 도와주는 도구입니다.\n\n### 프로젝트 생성\n\n먼저 `prac` 폴더로 돌아가서 go 프로젝트를 생성합니다.\n\n```bash\ngo mod init prac\n```\n\n그리고 `main.go` 파일을 생성합니다.\n\n```go\npackage main\n\nimport \"C\"\n\nfunc main() {}\n```\n\ncgo를 사용할 것이기 때문에 `import \"C\"` 구문을 추가합니다.  \n그리고 빌드할 때, cgo 관련 플래그를 추가할 수 있지만, 저는 `go build`만 사용해서 빌드하고 싶으므로 미리 플래그를 추가하겠습니다.\n\n```go\npackage main\n\n/*\n#cgo LDFLAGS: -L./vcpkg/installed/arm64-osx/lib\n#cgo CFLAGS: -I./vcpkg/installed/arm64-osx/include\n*/\nimport \"C\"\n\nfunc main() {}\n```\n\n`#cgo` 구문을 통해 `LDFLAGS`와 `CFLAGS`를 추가했습니다.  \n이제 빌드할 때, 별도의 플래그 없이 해당 include 폴더와 lib 폴더를 참조할 수 있습니다.\n\n### vscode c 설정\n\nC 헤더를 가져오는 건 잘 되겠지만, vscode에서 인식하지 못 하면 많이 불편할 것입니다.  \n아님 말구요.\n\n저는 자동 완성과 인텔리센스가 없으면 안 되는 몸이 되어버렸기 때문에 관련 설정을 해주겠습니다.\n\nvscode에서 커맨드 팔레트를 열어서 `C/C++: Edit Configurations (JSON)`을 선택합니다.  \n그럼 프로젝트 내에 `.vscode` 폴더 밑에 `c_cpp_properties.json` 파일이 생성됩니다.  \n해당 `json` 파일은 제 맥을 기준으로 아래와 같았습니다.\n\n```json\n{\n    \"configurations\": [\n        {\n            \"name\": \"Mac\",\n            \"includePath\": [\n                \"${workspaceFolder}/**\"\n            ],\n            \"defines\": [],\n            \"macFrameworkPath\": [\n                \"/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks\"\n            ],\n            \"compilerPath\": \"/usr/bin/clang\",\n            \"cStandard\": \"c17\",\n            \"cppStandard\": \"c++17\",\n            \"intelliSenseMode\": \"macos-clang-arm64\"\n        }\n    ],\n    \"version\": 4\n}\n```\n\n이 json 파일에서 `\"includePath\"` 내부에 vcpkg 내부의 include 폴더를 추가합니다.\n\n```json\n{\n    \"configurations\": [\n        {\n            \"name\": \"Mac\",\n            \"includePath\": [\n                \"${workspaceFolder}/**\",\n                \"./vcpkg/installed/arm64-osx/include\"\n            ],\n            \"defines\": [],\n            \"macFrameworkPath\": [\n                \"/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks\"\n            ],\n            \"compilerPath\": \"/usr/bin/clang\",\n            \"cStandard\": \"c17\",\n            \"cppStandard\": \"c++17\",\n            \"intelliSenseMode\": \"macos-clang-arm64\"\n        }\n    ],\n    \"version\": 4\n}\n```\n\n그럼 vscode에서 C 헤더파일들을 인식해서 cgo 내에서도 도구의 힘을 빌릴 수 있습니다. hooray!\n\n### 크로스플랫폼 설정\n\nvcpkg 내부의 installed 폴더 아래는 플랫폼 별로 폴더가 나뉘어져 있습니다.  \n흔히 쓰이는 실리콘 맥의 경우엔 `arm64-osx`이고, amd64 윈도우의 경우엔 `x64-windows`입니다.  \n그리고 amd64 리눅스는 `x64-linux`입니다.\n\n맥만 생각하고 작성했기에 방금 `main.go`같은 형식이 나올 수 있었지만, amd64 windows와 linux에 대해서도 별도의 설정 없이 동작할 수 있게 파일을 생성하겠습니다.\n\n- osx.go\n\n```go\n//go:build darwin\n\npackage main\n\n/*\n#cgo LDFLAGS: -L./vcpkg/installed/arm64-osx/lib\n#cgo CFLAGS: -I./vcpkg/installed/arm64-osx/include\n*/\nimport \"C\"\n```\n\n- windows.go\n\n```go\n//go:build windows\n\npackage main\n\n/*\n#cgo LDFLAGS: -L./vcpkg/installed/x64-windows/lib\n#cgo CFLAGS: -I./vcpkg/installed/x64-windows/include\n*/\nimport \"C\"\n```\n\n- linux.go\n\n```go\n//go:build linux\n\npackage main\n\n/*\n#cgo LDFLAGS: -L./vcpkg/installed/x64-linux/lib\n#cgo CFLAGS: -I./vcpkg/installed/x64-linux/include\n*/\nimport \"C\"\n```\n\n빌드 컨트랙트를 통해 각 플랫폼 별로 적용될 플래그를 다르게 설정하였습니다.\n\n## ffmpeg 사용해보기\n\n이제 ffmpeg를 사용해보겠습니다.\n\n```go\npackage main\n\n/*\n#cgo LDFLAGS: -L ./vcpkg/installed/arm64-osx/lib -lavcodec -lavformat -lavutil\n#cgo CFLAGS: -I ./vcpkg/installed/arm64-osx/include\n\n#include <libavformat/avformat.h>\n#include <libavcodec/avcodec.h>\n#include <libavutil/avutil.h>\n*/\nimport \"C\"\nimport (\n\t\"fmt\"\n)\n\nfunc main() {\n\tfmt.Println(C.avcodec_configuration())\n\tfmt.Println(C.avcodec_license())\n\tfmt.Println(C.avcodec_version())\n}\n```\n\n```bash\n0x100117872\n0x100117eac\n3933028\n```\n\n아마 잘(?) 실행됩니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/023_cgo_with_vcpkg/", "created_date": "2023-10-12T00:21:28Z"}
{"title": "HTTP API 에러 쓰는 법", "content": "## 왜?\n\n### HTTP API에 에러 쓰는 건 개인 취향 혹은 조직에서 정한 룰 아닌가요?\n\n저도 그렇게 생각하고 있습니다.  \n모름지기 학사 과정을 수료했든, 학원을 수료했든, 부트캠프를 수료했든, 백엔드 개발자라면 알아서 잘 남길 수 있을 것입니다.\n\n### 그럼 왜 이 글이 있는 거죠?\n\nRFC에 HTTP API의 에러에 관한 내용이 있더라구요. 흥미로워서 가져왔습니다.\n\n## RFC7807\n\nRFC7807은 `Problem Details for HTTP APIs`라는 이름의 문서입니다.  \n이름 그대로 HTTP API에서 발생하는 오류 응답 형식을 적어놓은 문서죠.\n\n이 문서에서는 `application/problem+json` MIME 타입을 소개합니다.\n\n### application/problem+json\n\n이 이상한 타입의 `json`에 대해 RFC7807에서는 아래와같은 예제를 몇가지 던져줍니다.\n\n```json\nHTTP/1.1 403 Forbidden\nContent-Type: application/problem+json\nContent-Language: en\n\n{\n \"type\": \"https://example.com/probs/out-of-credit\",\n \"title\": \"You do not have enough credit.\",\n \"detail\": \"Your current balance is 30, but that costs 50.\",\n \"instance\": \"/account/12345/msgs/abc\",\n \"balance\": 30,\n \"accounts\": [\"/account/12345\",\n              \"/account/67890\"]\n}\n```\n\n```json\nHTTP/1.1 400 Bad Request\nContent-Type: application/problem+json\nContent-Language: en\n\n{\n\"type\": \"https://example.net/validation-error\",\n\"title\": \"Your request parameters didn't validate.\",\n\"invalid-params\": [ {\n                      \"name\": \"age\",\n                      \"reason\": \"must be a positive integer\"\n                    },\n                    {\n                      \"name\": \"color\",\n                      \"reason\": \"must be 'green', 'red' or 'blue'\"}\n                  ]\n}\n```\n\n대략적으로 이런 형태를 가집니다.  \n규칙성이 보이는 부분은 `type`과 `title`이 필수라는 점과 딱 봐도 도메인 특화된 필드를 추가로 넣을 수 있다는 게 있을 것입니다.  \n아래 파트에서 조금만 자세히 확인해보겠습니다.\n\n### 필드에 대해\n\n- `type`: 문자열 타입으로 이 필드는 해당 에러에 대해 더 자세한 정보를 확인할 수 있는 URL입니다. 만약 여러분들이 Azure Cosmos DB의 서비스 페이지를 개발하고 있다면, [해당 페이지](https://learn.microsoft.com/en-us/rest/api/cosmos-db/http-status-codes-for-cosmosdb)를 `type`으로 쓸 수 있을 것입니다. 이 필드는 필수값입니다. 부가적으로 이 필드의 링크는 html 형식이길 권장합니다.\n- `title`: 문자열 타입으로 이 필드는 문자 그대로 해당 문제에 대한 타이틀입니다. 필수 항목이며, 동일한 문제에는 동일한 타이틀이 제공되어야 합니다. 그리고 사람이 읽을 수 있는 형식으로 제공되어야 합니다.\n- `status`: 정수 타입으로 origin 서버로부터 받거나, 현재 서버에서 발생한 HTTP 상태 코드입니다. 선택 사항입니다.\n- `detail`: 이 문제에 대해 사람이 읽을 수 있는 상세한 내용입니다. 선택 사항입니다.\n- `instance`: 이 문제가 발생한 URI입니다. 선택 사항이며, `/api/auth/login`처럼 작성합니다.\n\n그리고 추가적인 정보를 전달하기 위해 `Extension Members`(확장 멤버)를 제공합니다.  \n이 부분이 아까 예시에 있었던, `balance`나 `accounts`, `invalid-params`에 해당합니다.\n\n확장 멤버는 본인이 필요한 케이스가 아닐 경우, 읽지 않아야 합니다.  \n이를 통해 오류 응답 형식을 확장하여 더 많은 필드를 쓸 수 있게 해줍니다.\n\n### 주의 사항\n\n오류 응답 형식을 사용할 때 다음 사항에 대해 주의를 당부합니다.\n\n1. 디버깅을 위한 도구가 되어선 안됩니다.\n2. 일반적인 HTTP 상태 코드를 통한 표현을 우선시 해야합니다.\n\n### 오류 응답 형식 확장\n\n새로운 오류 응답 형식을 생성할 때는 다음 사안을 고려합니다.\n\n1. 새로운 오류에 대한 적절한 `type` 값\n2. 새로운 오류에 붙일 수 있는 고유한 `title`\n3. 새로운 오류에 어울리는 `status` 코드\n\n부가적으로 상세 정보(`detail`), 발생한 URI(`instance`), 확장 멤버(`Extension Members`)를 추가할 수 있습니다.  \n새로운 오류 응답 형식이 만들어지면, 적절한 DTO로 확장 및 공유하여 사용합니다.\n\n네이밍 및 값에 대한 형식과 제한은 `json`의 그것을 따릅니다.  \n문서 자체에서는 영문자, 숫자, 언더바(`_`)만 사용하기를 권장합니다.\n\n## 이번에도 당연히\n\nRFC7807을 읽으며, 필요한 사용법을 위해 직접 라이브러리를 만들었습니다.\n\n[snowmerak/httperror](https://github.com/snowmerak/httperror)", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/029_rfc_7807/", "created_date": "2024-05-21T18:21:32Z"}
{"title": "대는 소를 포함한다", "content": "## ?\n\n`대는 소를 포함한다`는 말은 법률상으로 상위 개념과 하위 개념이 존재할 경우, 상위 개념에 대한 법을 하위 개념에도 적용하는 것입니다. 쉽게 이야기하면, 법률적으로는 자전거가 자동차의 하위 개념이니 자동차에 적용하는 다양한 법을 자전거에도 같이 적용하는 것입니다. 하지만 제가 법률 전문가도 아니고, 법과 관련된 이야기를 하고 싶은 건 아닙니다.\n\n## 소프트웨어 설계에서\n\n### 범용은 전용을 포함한다\n\n저희는 때때로 범용으로 무언가를 만들고, 전용으로 사용하는 방법을 취합니다. 대표적인 예로 postgres는 많은 데이터를 저장하고 쿼리할 수 있지만, 저희는 스키마를 한정해서 한정적인 데이터 형태를 저장하고 활용합니다. MongoDB같은 경우엔 그렇지 않다구요? 하지만 높은 확률로 로직이나 환경적으로 들어갈 자료, 해야할 쿼리는 정해져 있을 겁니다. 그게 정녕 비정형 JSON을 저장하기 위한 data lake라 할 지라도요. 그리고 그를 위한 별도 모듈이 존재할 것입니다.\n\n제가 좋아하는 Redis나 NATS는 더 특수하게 만들 수 있습니다. 저는 프로젝트나 아키텍처를 설계할 때에 `어떤 데이터를 캐싱할 건지?`, `어떤 데이터를 시그널링할 건지?`, `어떤 데이터를 브로드캐스팅할 건지?` 같은 특수성을 가지고 범용적인 레디스와 나츠의 클라이언트를 제한합니다. 물론 이는 법률적 해석과는 어느정도 차이를 보이는 해석입니다. \n\n### 예를 들어,\n\n고 언어에서 제가 자주 구성하는 대로 레디스 관련 패키지를 구성하면, `prac/lib/client/redis`에 레디스의 연결에 대한 정보를 작성할 것입니다.\n\n```go\n// prac/lib/client/redis/client.go\npackage redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/redis/rueidis\"\n)\n\nconst namespace = \"practice\"\n\nfunc WrapKey(key string) string {\n\treturn namespace + \":\" + key\n}\n\ntype Config struct {\n\tAddr     []string `env:\"ADDR\"`\n\tUsername string   `env:\"USERNAME\"`\n\tPassword string   `env:\"PASSWORD\"`\n}\n\nfunc NewConfig() *Config {\n\treturn &Config{\n\t\tAddr:     []string{\"localhost:6379\"},\n\t\tUsername: \"\",\n\t\tPassword: \"\",\n\t}\n}\n\ntype Client struct {\n\tcli rueidis.Client\n}\n\nfunc New(ctx context.Context, cfg *Config) (*Client, error) {\n\tcli, err := rueidis.NewClient(rueidis.ClientOption{\n\t\tInitAddress: cfg.Addr,\n\t\tUsername:    cfg.Username,\n\t\tPassword:    cfg.Password,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create redis client: %w\", err)\n\t}\n\n\tcontext.AfterFunc(ctx, func() {\n\t\tcli.Close()\n\t})\n\n\treturn &Client{cli: cli}, nil\n}\n\nfunc (c *Client) Do(ctx context.Context, runnable func(ctx context.Context, cli rueidis.Client) error) error {\n\tif err := runnable(ctx, c.cli); err != nil {\n\t\treturn fmt.Errorf(\"failed to execute redis command: %w\", err)\n\t}\n\n\treturn nil\n}\n```\n\n일단 가장 단순하지만 필요한 형태로 레디스에 대한 `Config`와 `Client`를 구성합니다. 조금 특이한 거라면, `client`를 생성할 때 컨텍스트가 만료되는 시점에 같이 종료되도록 하는 사후 동작이 추가되어 있습니다. `rueidis`는 연결에도 많은 옵션을 줄 수 있고, 더 많은 사후 동작을 지정할 수 있지만, 지금은 이렇게만 작성하겠습니다. \n\n이제 JWT 토큰의 상세 데이터 및 차단 여부를 확인할 수 있는 하위 패키지를 작성합니다.\n\n```go\n// prac/lib/client/redis/tokencache/cache.go\npackage tokencache\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"github.com/redis/rueidis\"\n\t\"prac/lib/client/redis\"\n\t\"strings\"\n\t\"time\"\n)\n\nvar namespace = redis.WrapKey(\"token_cache\")\n\nconst (\n\tdata    = \"data\"\n\tblocked = \"blocked\"\n\n\tdefaultTokenDataCacheTTL  = 7 * 24 * time.Hour // 1 week\n\tdefaultTokenBlockCacheTTL = 7 * 24 * time.Hour // 1 week\n\tdefaultClientSideCacheTTL = 60 * 60            // 1 hour\n)\n\nfunc WrapKey(key ...string) string {\n\treturn namespace + \":\" + strings.Join(key, \":\")\n}\n\ntype TokenCache struct {\n\tcli *redis.Client\n}\n\nfunc New(_ context.Context, cli *redis.Client) *TokenCache {\n\treturn &TokenCache{\n\t\tcli: cli,\n\t}\n}\n\nfunc (tc *TokenCache) GetTokenData(ctx context.Context, token string) (string, error) {\n\tkey := WrapKey(data, token)\n\tvalue := \"\"\n\tif err := tc.cli.Do(ctx, func(ctx context.Context, cli rueidis.Client) error {\n\t\tv, err := cli.DoCache(ctx, cli.B().Get().Key(key).Cache(), defaultClientSideCacheTTL).ToString()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get token data: %w\", err)\n\t\t}\n\n\t\tvalue = v\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn value, nil\n}\n\nfunc (tc *TokenCache) SetTokenData(ctx context.Context, token, data string) error {\n\tkey := WrapKey(data, token)\n\tif err := tc.cli.Do(ctx, func(ctx context.Context, cli rueidis.Client) error {\n\t\tif err := cli.Do(ctx, cli.B().Set().Key(key).Value(data).Ex(defaultTokenDataCacheTTL).Build()).Error(); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to set token data: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (tc *TokenCache) DeleteTokenData(ctx context.Context, token string) error {\n\tkey := WrapKey(data, token)\n\tif err := tc.cli.Do(ctx, func(ctx context.Context, cli rueidis.Client) error {\n\t\tif err := cli.Do(ctx, cli.B().Del().Key(key).Build()).Error(); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to delete token data: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (tc *TokenCache) BlockToken(ctx context.Context, token string) error {\n\tkey := WrapKey(blocked, token)\n\tif err := tc.cli.Do(ctx, func(ctx context.Context, cli rueidis.Client) error {\n\t\tif err := cli.Do(ctx, cli.B().Set().Key(key).Value(\"\").Ex(defaultTokenBlockCacheTTL).Build()).Error(); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to block token: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (tc *TokenCache) IsTokenBlocked(ctx context.Context, token string) (bool, error) {\n\tkey := WrapKey(blocked, token)\n\tok := true\n\tif err := tc.cli.Do(ctx, func(ctx context.Context, cli rueidis.Client) error {\n\t\t_, err := cli.DoCache(ctx, cli.B().Get().Key(key).Cache(), defaultClientSideCacheTTL).ToString()\n\t\tif err != nil {\n\t\t\tok = false\n\t\t\tif errors.Is(err, rueidis.Nil) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"failed to get token block status: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn ok, err\n\t}\n\n\treturn ok, nil\n}\n\nfunc (tc *TokenCache) UnblockToken(ctx context.Context, token string) error {\n\tkey := WrapKey(blocked, token)\n\tif err := tc.cli.Do(ctx, func(ctx context.Context, cli rueidis.Client) error {\n\t\tif err := cli.Do(ctx, cli.B().Del().Key(key).Build()).Error(); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to unblock token: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```\n\n코드가 좀 길지만, 결과적으로 외부에서 `tokencache` 패키지를 사용할 때 알아야 할 것은, 각 메서드와 거기에 `token`만 입력하면 해당 메서드의 이름 대로 동작할 것이라는 것입니다. 이렇게 작성함으로 저희는 언제나 `tokencache`를 사용할 때에 실수를 최소화할 수 있습니다. 거기에 더해, 저희는 모든 레디스에 타임아웃 정책을 추가하고 싶다면, `prac/lib/client/redis/client.go` 파일의 `Do` 메서드를 다음과 같이 수정할 수도 있습니다.\n\n```go\nfunc (c *Client) Do(ctx context.Context, runnable func(ctx context.Context, cli rueidis.Client) error) error {\n    ctx, cancel := context.WithTimeout(ctx, time.Second)\n    defer cancel()\n\n\tif err := runnable(ctx, c.cli); err != nil {\n\t\treturn fmt.Errorf(\"failed to execute redis command: %w\", err)\n\t}\n\n\treturn nil\n}\n```\n\n그러면 이 글에서 구현한 `tokencache` 뿐만 아니라, 모든 `*redis.Client`를 이용해 구현한 서브 패키지들은 1초 타임아웃의 영향을 받을 것입니다. 물론 모든 쿼리에 1초의 타임아웃을 적용하는 건 불합리할 수 있으므로, `WithShortTimeout`, `WithLongTimeout`같은 추가적인 메서드를 구현해도 좋아 보입니다. 이렇게 `rueidis` -> `redis` -> `tokencachde` 순으로 점점 범용에서 전용으로 제한해 가면서 구성할 수 있습니다.\n\n### 그래서\n\n이러한 범용에서 전용으로 진행되는 패키지 구조를 통해 저희는 로직을 한 곳에 모아 코드 중복을 방지하고, 각 함수가 특정 기능만을 담당하도록 하여 단일 책임 원칙을 준수하며, 명확한 인터페이스를 제공하여 사용 시 실수를 줄일 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/038_genus_et_species/", "created_date": "2024-10-03T21:40:46Z"}
{"title": "분산 서비스에서의 R&R에 대한 고찰", "content": "> 이것 또한 최근 포스트와 마찬가지로, 최근에 진행했던 프로젝트의 회고입니다.\n\n## 개요\n\n해당 프로젝트에는 개인적으로 몇가지 이해가 어려운 부분들이 좀 있었습니다. 그 중 R&R에 대해 제가 품은 의문과 그에 대한 제 나름의 해결 방법을 서술하고자 합니다.\n\n## 본문\n\n### 어떤 부분이?\n\n1. API GW와 서비스만 책임 분리\n2. API GW와 인증 및 인가 책임\n3. 각 서비스 간의 역할과 책임 분리\n\n### API GW와 서비스간의 책임 분리에 대해\n\n설계 당시부터 지금까지 모든 서비스는 k8s에 pod로써 운영되고 있습니다. 대략적으로 네트워크 요청 트래픽은 `ingress -> API GW -> each service`로 흐릅니다. 이 중 API GW는 솔루션이 아니라 자사에서 내부적으로 만든 것입니다. 기능으론 일반적인 API GW와 유사하게 요청에 대해 특정 서비스로의 라우팅과 요청에 대한 인증 토큰 검증 등이 있습니다. 그리고 각 서비스는 특정 데이터를 다루는 서비스와 저장하는 DB를 세트로 구성됩니다. 제가 보는 문제는 이 구조로 인해 생성되는 gray zone이 있습니다.\n\n> 그래서 데이터 집계는 누가 할건데?\n\nA 서비스가 B 서비스의 데이터가 필요하면, 어떤 식으로 데이터를 요청하고 처리할 건가요? 만약 웹에서는 C 서비스의 데이터까지 필요한데, 모바일 앱에서는 C 서비스의 데이터가 필요없다면, 굳이 요청해서 가져올건가요? 누가 요청자가 필요한 데이터인지 파악하고, 요청을 스케쥴링하며, 종합해서 돌려줄 건가요? 어떤 구조를 추가하고, 어떤 통신 방식을 선택할 건가요?\n\n너무 심플한 구조를 가진 나머지, 위 의문을 해결하는 것에 어려움을 겪을 수밖에 없습니다. 실제 제가 본 케이스에선 API GW에서 데이터를 집계하는 방식과 각 서비스들이 통신해서 데이터를 집계하는 방식, 둘 다 쓰이고 있었습니다. 제가 생각해봤을 때, 이 해답은 크게 2가지로 해결할 수 있어 보입니다.\n\n1. aggregator 패턴과 BFF(Backend-For-Front) 패턴을 활용한 레이어 추가\n2. MQ나 서비스 메시를 활용한 비동기 통신을 위한 방향성을 상하로 제한\n\naggregator, 혹은 BFF를 사용하게 되면, 필연적으로 네트워크 요청 트래픽은 `ingress -> API GW -> aggregator/BFF -> each services`로 흐르게 됩니다.\n\n> 글을 단순하게 만들기 위해 BFF는 어그리게이터로 통칭하겠습니다.\n\n위에 제가 작성한 모든 문제를 어그리게이터에게 일임하게 되면, 저 질문은 굉장히 단순한 형태를 띄게 됩니다.\n\n1. 여러 서비스의 데이터가 필요한 API 콜에 대해서, 어그리게이터가 여러 서비스에 요청을 하면 됩니다.\n2. 특정 플랫폼에서만 필요한 데이터는 라우팅을 달리해서, 특정 라우트마다 추가 동작을 수행하도록 할 수 있습니다.\n3. 요청 스케쥴링과 집계는 문자 그대로, 어그리게이터가 수행하면 됩니다.\n\n만약 API GW에서 어그리게이터를 거쳐서 서비스를 호출하는 것이 부담이 된다면, API GW 자체를 여러개로 나누어서 하나의 ALB -> API GW가 아니라 하나의 ALB -> multiple aggregator로 바꾸는 방법도 제안할 수 있습니다. 어떤 방식으로 구현하든, 차후 필요에 따라 MQ나 서비스 메시를 통해서 aggregator와 서비스 간에 요청과 응답을 주고 받을 수 있게 프로토콜을 정의하고 구성하면 될 것같습니다.\n\n### API GW와 인증 및 인가 책임\n\n단일 API GW만 존재한다면, 인증 서비스 하나만 있어도 큰 문제가 되진 않을 것입니다. 다만 인증 자체를 ALB에서 하지 않거나, 단일 진입점에서 처리하지 않을 경우, 그리고 네임스페이스가 분리되어 서비스들이 구성되면서 통합되는 형태로 흘러갈 경우에는 인증 서비스가 특정 한 네임스페이스에 일부분으로써 포함되는 것 등에서 문제가 될 수 있습니다.\n\n만약 A 네임스페이스가 인증 서비스를 포함하고, 인가도 수행하며 검증도 하게 된다면, B 네임스페이스는 보안이 필요한 요청에 대한 처리가 필요할 때마다 A 네임스페이스 중 인증 서비스에 직접 어떤 식으로든 통신을 하거나, A 네임스페이스가 내보내준 어떤 인터페이스를 통해 토큰 전체를 제공해줘야합니다.\n\n하지만 토큰을 전부 제공하는 건 중간에 탈취되었을 때의 위험성만 증가시키고, A 네임스페이스와 A 네임스페이스의 인증 서비스에 전체적인 부담만 늘릴 뿐입니다. 그러면 A 네임스페이스의 인증 서비스가 어떤 식으로든 구멍을 뚫어서 B 네임스페이스가 직접 통신하게 되면 어떤가요? B 네임스페이스 전체가, 혹은 다른 네임스페이스들도 A 네임스페이스와 통신하는게 아닌, 자기보다 더 작은 인증 서비스와의 통신으로 그려지게 될 것입니다. 이렇게 되면 서로 다른 레이어 사이의 통신이 되므로, 전체적인 설계의 복잡도가 증가하게 됩니다.\n\n그런 면에서 기존에 있던 방식 중 하나인,\n\n1. A 네임스페이스의 인증 서비스가 비대칭키 서명으로 토큰을 생성 및 인가\n2. 다른 네임스페이스에서 검증이 필요할 경우 A 네임스페이스에 해당 유저와 토큰의 퍼블릭 키 요청\n3. A 네임스페이스는 내부에 요청을 전달하여 인증 서비스나 DB에서 조회 후 퍼블릭 키 제공\n4. 다른 네임스페이스에서는 해당 퍼블릭 키로 토큰 검증 후 프로세스 처리\n\n흐름을 생각했습니다. 이를 구성하는 표준안으로 JWK도 있으며, 필요에 따라 protobuf나 flatbuf로 쉽게 구현할 수 있을 것입니다.\n\n### 각 서비스 간 역할과 책임 분리\n\n개인적으로 스트레스를 많이 받았고, 받고 있는 부분입니다. 기본적으로 모든 라이브 및 VoD 영상에 대한 메타데이터와 파일은 저희 서비스와 DB가 가지고 있습니다. 하지만 별도의 서비스에서 VoD 전체 길이와 VoD 번호를 지정하게 되고, 거기서 파생되는 것들에 대한 정보를 저장하고 있습니다.\n\n다른 케이스로는 RTMP 서버는 저희 서비스에 포함되어 있지만, 별도의 다른 서비스가 publish key를 가지고 있으며, 재생성에 대한 유저의 요청도 처리합니다. 저희 쪽에서 방송 시작 요청이 RTMP로 들어오면, 저희 서비스에서 요청을 보내서 검증을 하게 됩니다.\n\n방송 종료에 대한 처리 또한, 저희 서비스에서도 별도 DB에 기록하며 관리하기도 합니다. 다만 특정 채널에 대해 라이브 중인지 아닌지에 대한 판단을 별도 서비스에서 하기 때문에, 해당 서비스에 메시지를 보내서 방송 상태 변경을 처리하게 됩니다. 이러한 구조 때문에, 방송 상태에 대한 라이프타임 관리와 시청자가 메시지를 받을 때까지의 홉도 늘어나 문제가 발생할 구간도 커지며, 전체적인 구성도 복잡해집니다.\n\n왜 이러한 아키텍처를 가지게 되었는 지는 확실치 않습니다. 저희 서비스 내부에 대한 것 외에는 제가 결정에 참여하지 못 했기에, 어떠한 이유로 이렇게 되었는 지는 모릅니다. 다만 의심가는 부분은 몇가지 있습니다만, 그 중 가장 크다고 느낀 프론트엔드 관점에서 백엔드 아키텍처를 설계에 대해서만 서술하겠습니다.\n\n대체로 서비스들이 `ingress -> API GW -> each service`로 라우팅 되고 있습니다. 이는 모놀리스 아키텍처에서 프로세스만 분리해놓은 것과 차이가 없습니다. 그리고 전반적인 라우팅 구성이 프론트엔드의 요청 단위로 분리되어 있습니다. 그렇기 때문에, `프론트엔드에서 publish key에 대한 CRUD, 방송 개설 및 종료 여부에 대한 요청이 들어오니 별도 서비스로 분리`, `VoD에 대한 요청은 프론트엔드에서 하므로, VoD를 관리하는 별도 서비스에서 관리하도록 분리`같은 이상한 의사 결정이 들어가지 않았나 예상합니다.\n\n그러면서 자연스럽게 도메인 별로 네임스페이스와 서비스가 분리되지 못 하고, 업무 분장까지 이루어져서 아키텍처가 굳어지게 됩니다. 도메인 별로 아키텍처가 구성되지 못 하여, 어떠한 작업을 할 때도 서로의 영역에 대해 신경을 써야하고, 전반적으로 경직되게 조직이 굴러가게 됩니다. 그를 타파하기 위해 R&R을 애매하게 가져가서 모두에게 모든 걸 하라고 하면, 일부 사람들의 일만 가중되고, 하다보면 내로남불과 같은 케이스가 발생하게 될 수 있으며, 불만이 생기게 되면 조직의 퇴화로 이어지게 될 것입니다.\n\n개인적으로는 프론트엔드의 요청은 API GW에서 생각하고, 백엔드에서의 데이터 구조와 처리에 대해선 도메인 단위로 가져가는 게 좋다고 봅니다. 그 사이에서의 절충안을 어그리게이터(BFF 포함)에서 해결 하는 것이죠.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/034_RnR/", "created_date": "2024-08-01T21:37:19Z"}
{"title": "git, buf, 그리고 패키지 매니저로 프로토버퍼 관리하기", "content": "## 프로토버퍼?\n\n프로토버퍼는 구글에서 개발한 직렬화 라이브러리입니다.  \n프로토버퍼는 다양한 언어를 지원하며, 단일 IDL(Interface Description Language)을 사용하여 여러 언어에 대한 DTO 및 시리얼라이저를 생성할 수 있습니다.\n\n그렇기에 서버와 클라이언트 사이에 서로 다른 언어를 사용하더라도, 같은 IDL만 공유할 경우 서로가 주고 받는 메시지의 종류만 안다면, 별도의 DTO를 문서를 보고 만들 필요 없이 바로 사용할 수 있습니다.\n\n### 프로토버퍼의 문제점\n\n프로토버퍼는 `.proto` 확장자를 가진 IDL 파일을 각 언어에 맞게 컴파일해서 사용해야한다는 단점이 있습니다.\n\n그래서 프로토버퍼 파일 버전이 상이할 경우에, 같은 메시지(DTO)를 주고 받는 다더라도, 예외가 발생할 수 있어 주의가 필요합니다.\n\n### 그럼에도 불구하고\n\n프로토버퍼는 다양한 언어를 지원하며, 빠르고, 직렬화된 데이터의 크기가 작아서, 여전히 많은 곳에서 사용되고 있습니다.\n\n그리고 프로토버퍼로 생성된 메시지는 JSON으로 직렬화가 가능해서 범용성도 좋습니다.  \n클라이언트가 웹이라면, 같은 메시지를 JSON으로 직렬화해서 주고 받을 수도 있습니다.\n\n그렇다면, 딱 2개 있는 문제점, 컴파일이 필요하다는 것과 버전 관리가 별도로 이루어져야 한다는 걸 해결해야합니다.\n\n## 컴파일 자동화\n\n프로토버퍼는 다양한 언어를 지원하는 만큼, 각 언어에 맞는 컴파일러 플러그인이 존재합니다.  \n그리고 필요한 언어에 맞는 플러그인을 매번 설치하고 설치 스크립트를 작성하는 것이 그다지 단순하게 쉬운 일은 아닐 것입니다.\n\n하지만 다행스럽게도, [buf](https://buf.build)에서 `buf`라는 툴을 제공해서 컴파일을 자동화할 수 있습니다.\n\n### buf 설치\n\n`buf` 툴은 아래와 같은 명령어들로 설치할 수 있습니다.\n\n1. homebrew: `bbrew install bufbuild/buf/buf`\n2. npm: `npm install @bufbuild/buf`\n3. go: `GO111MODULE=on GOBIN=/usr/local/bin go install github.com/bufbuild/buf/cmd/buf@v1.28.1`\n4. scoop: `scoop install buf`\n\n이외에 직접 [github](https://github.com/bufbuild/buf/releases)에서 바이너리를 다운로드 해서 `PATH`에 등록해서 쓸 수도 있습니다.\n\n### buf 세팅\n\n루트 디렉토리에 `buf.gen.yaml` 파일을 만들어 아래 내용을 작성합니다.\n\n```yaml\nversion: v1\nmanaged:\n  enabled: true\n  go_package_prefix:\n    default: github.com/snowmerak/common\nplugins:\n  - plugin: buf.build/protocolbuffers/go\n    out: output\n    opt: paths=source_relative\n  - plugin: buf.build/grpc/go\n    out: output\n    opt: paths=source_relative\n```\n\n이 내용은 `buf`가 컴파일할 때, `go_package_prefix`를 `github.com/snowmerak/common`으로 설정하고, `output` 디렉토리에 컴파일 결과물을 저장하도록 설정한 것입니다.\n\n경로는 `source_relative`로 설정해서, `buf.gen.yaml`이 있는 디렉토리를 기준으로 상대 경로로 설정하도록 했습니다.\n\n당연하게도 각 프로토버퍼 파일 간의 의존성 경로도 루트 디렉토리를 기준으로 작성해야합니다.\n\n고 이외의 플러그인은 [buf 공식 플러그인 페이지](https://buf.build/plugins)를 참고하시면 됩니다.\n\n저희는 nodejs와 dart 코드에 대해서도 컴파일을 할 예정이므로 이 둘을 추가하도록 하겠습니다.\n\n아래에 추가로 다음 항목을 추가하여 `buf.gen.yaml`을 완성합니다.\n\n```yaml\nversion: v1\nmanaged:\n  enabled: true\n  go_package_prefix:\n    default: github.com/snowmerak/common\nplugins:\n  - plugin: buf.build/protocolbuffers/go:v1.31.0\n    out: lib/src/go\n    opt: paths=source_relative\n  - plugin: buf.build/grpc/go:v1.3.0\n    out: lib/src/go\n    opt: paths=source_relative\n  - plugin: buf.build/community/timostamm-protobuf-ts:v2.9.3\n    out: lib/src/ts\n  - plugin: buf.build/protocolbuffers/dart:v21.1.2\n    out: lib/src/dart\n```\n\n이제 `buf`를 실행하면, `buf.gen.yaml`에 설정한 대로 컴파일이 진행됩니다.\n\n### buf 컴파일\n\n컴파일은 가볍게 `buf generate`를 실행하면 됩니다.\n\n## 예시 프로토버퍼 파일\n\n이제 프로토버퍼 파일을 작성해보겠습니다.\n\n이번에는 가장 기초적인 회원가입과 결과, 로그인과 세션키를 반환하는 예시로 작성해보겠습니다.\n\n우선 폴더를 확실하게 분리하기 위해, `proto` 폴더를 만들고, 그 안에 `auth` 폴더를 만들어서 `auth.proto` 파일을 만들어 아래 내용을 작성합니다.\n\n```proto\nsyntax = \"proto3\";\n\noption go_package = \"github.com/common/auth\";\n\nmessage SignUpRequest {\n    string email = 1;\n    string password = 2;\n}\n\nmessage SignUpResponse {\n    string token = 1;\n}\n\nmessage SignInRequest {\n    string email = 1;\n    string password = 2;\n}\n\nmessage SignInResponse {\n    string token = 1;\n}\n```\n\n여기서 `option go_package`는 프로토버퍼가 가질 고 코드의 패키지 경로를 설정하는 것입니다.\n\n그리고 `proto` 폴더 밑에  `buf mod init`을 실행해서 buf 모듈를 생성합니다.\n\n```yaml\nversion: v1\nbreaking:\n  use:\n    - FILE\nlint:\n  use:\n    - DEFAULT\n```\n\n그러면 위 yaml 파일을 가진 `buf.yaml` 파일이 생성됩니다.\n\n이제 다시 루트 디렉토리로 돌아와서 `buf generate`를 하면 결과물이 생성될 것입니다.\n\n## 버전 관리\n\n버전 관리라곤 하나, 각 언어의 패키지 매니저를 통해 배포하고, 각 프로젝트에서 업데이트하는 것을 의미합니다.\n\n### go\n\n우선 `go`의 경우 `go.mod` 파일을 생성하고, `go get`을 통해 패키지를 설치하면 됩니다.\n\n```bash\ngo mod init github.com/snowmerak/common\n```\n\n그러면 이제 외부 프로젝트에서 `go get github.com/snowmerak/common`를 통해 패키지를 설치하여 사용할 수 있습니다.\n\n### node\n\n우선 npm 패키지를 생성합니다.\n\n```bash\nnpm init -y\n```\n\n위 명령어를 실행하면 `package.json` 파일이 생성됩니다.  \n그리고 저희가 사용한 프로토버퍼 생성 플러그인인 `timostamm-protobuf-ts`이 제공하는 런타임을 설치합니다.\n\n```bash\nnpm install @protobuf-ts/runtime@^2.9.3 @protobuf-ts/runtime-rpc@^2.9.3\n```\n\n그리고 타입스크립트도 설치합니다.\n\n```bash\nnpm install typescript\n```\n\n이후 `tsc --init`을 실행해서 `tsconfig.json` 파일을 생성합니다.  \n타입스크립트 설정 파일에서 원하시는 옵션으로 수정하신 후, `tsc`를 실행하면 `gen/ts` 폴더의 타입스크립트 파일에서 자바스크립트 파일이 생성됩니다.\n\n그리고 `index.js`를 루트 폴더에 생성한 후, 다음 내용을 입력합니다.\n\n```js\nconst { Auth } = require('./lib/src/ts/proto/auth/auth');\n\nexport default {\n    Auth,\n};\n```\n\n저의 경우엔 `index.js`가 없어도 잘 돌아는 갔습니다만, 잘 배운 노드 개발자가 아니라서 확실치 않습니다.\n\n그러면 이제 외부 프로젝트에서 `npm install git+https://github.com/snowmerak/common.git`을 통해 패키지를 설치하여 사용할 수 있습니다.\n\n### dart\n\n다트는.. 라이브러리를 생성하면 프로젝트가 더러워집니다.\n\n```bash\ndart create --pub --force -t package .\n```\n\n위 명령어를 통해 루트 프로젝트에 다트 프로젝트를 생성합니다.  \n주의할 점은 `README.md` 파일이 초기화되므로 주의해주시기 바랍니다.  \n그리고 `buf generate` 및 `tsc` 등을 다시 실행합니다.\n\n그러면 `lib/src`에 각 언어들에 대한 프로토버퍼 코드가 생성되었을 것입니다.\n\n그럼 이제 `lib/common.dart`를 열어서 다음과 같이 수정합니다.\n\n```dart\n/// Support for doing something awesome.\n///\n/// More dartdocs go here.\nlibrary;\n\nexport 'src/dart/proto/auth/auth.pb.dart';\nexport 'src/dart/proto/auth/auth.pbenum.dart';\nexport 'src/dart/proto/auth/auth.pbjson.dart';\nexport 'src/dart/proto/auth/auth.pbserver.dart';\n\n// TODO: Export any libraries intended for clients of this package.\n```\n\n마지막으로 `dart pub add protobuf`를 실행해서 프로토버퍼 라이브러리를 추가합니다.\n\n그럼 이제 외부 프로젝트에서 `pubspec.yaml`에 다음과 같이 추가하고, `pub get`을 실행하면 패키지를 설치하여 사용할 수 있습니다.\n\n```yaml\ndependencies:\n  common:\n    git: https://github.com/snowmerak/common.git\n```\n\n## 마치며\n\n이렇게 한번 구성함으로 프로토버퍼를 수정하고 생성한 후 업로드하기만 하면, go나 node, dart 프로젝트에서 지속적으로 의존성을 업데이트함으로 최신 버전 메시지(DTO)를 사용할 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/025_protobuf_with_buf/", "created_date": "2023-12-13T21:42:22Z"}
{"title": "리스코프 치환 법칙에 대한 고찰", "content": "## Liskov Substitution Principle with inheritance\n\n리스코프 치환 법칙은 객체지향 프로그래밍에서 중요한 법칙 중 하나입니다.  \n\n> 서브 타입은 언제나 슈퍼 타입으로 대체될 수 있어야 한다.\n\n개인적으로는 살짝 헷갈린 적이 있는 표현이지만, 코드 내의 인스턴스 타입을 교체하는 케이스로 이해하면 쉽습니다.\n\n### 상속을 활용한 케이스\n\n이 법칙은 일반적인 상속이 존재하는 객체지향 지향 언어에서 쉽게 설명되는 법칙입니다.  \n예를 들어 보통 자바에선 이런 식으로 많이 예제를 작성합니다.\n\n```java\nclass Shape {\n    int width;\n    int height;\n}\n\nclass Rectangle extends Shape {\n    void setWidth(int width) {\n        this.width = width;\n    }\n\n    void setHeight(int height) {\n        this.height = height;\n    }\n}\n\nclass Square extends Shape {\n    void setWidth(int width) {\n        this.width = width;\n        this.height = width;\n    }\n\n    void setHeight(int height) {\n        this.width = height;\n        this.height = height;\n    }\n}\n```\n\n위 코드에서 `Rectangle`과 `Square`는 `Shape`를 상속받고 있습니다.  \n그리고 `Square`와 `Rectangle`을 주고 받을 수 있는 곳은 `Shape`로 대체할 수 있습니다.  \n그리고 상속받은 객체를 다음과 같이 수퍼 타입으로 받을 수 있습니다.\n\n```java\nclass main {\n    public static void main(String[] args) {\n        var shape = new Rectangle();\n        setShape(shape);\n    }\n\n    void setShape(Shape shape) {\n        shape.setWidth(10);\n        shape.setHeight(20);\n    }\n}\n```\n\n위 코드에서 `setShape` 메소드는 `Shape`를 인자로 받고 있습니다.  \n그리고 `Rectangle`을 넘겨줬지만 수퍼 타입인 `Shape`로 변환되어 문제 없이 동작합니다.\n\n### LSP의 의의\n\n이렇게 특정 서브 타입을 슈퍼 타입으로 대체할 수 있는 것은 다형성을 활용할 수 있게 해줍니다.  \n그리고 다형성은 객체의 역할과 책임을 제한하여, 잘못된 동작을 막거나 내부 상태를 숨기는 데에 도움을 줍니다.\n\n## Listkov Substitution Principle with interface and composite pattern\n\n하지만 이러한 상속을 통한 객체지향은 상속을 지원하는 언어에서만 사용할 수 있습니다.  \n그리고 스프링에서 보다시피, 상속 깊이가 깊어질 수록 상속 구조가 복잡해질 수 있으며,  \n다형성을 위한 행위 뿐만 아니라 내부 상태도 상속 받기 때문에, 개발자가 신경 써야할 부분이 늘어납니다.\n\n### 객체지향을 다시 생각해서\n\n객체지향의 핵심은 역할과 책임을 기능으로써 객체에게 부여하는 것입니다.  \n우리는 이 기능을 부여하기 위해 인터페이스와 [덕 타이핑](https://ko.wikipedia.org/wiki/%EB%8D%95_%ED%83%80%EC%9D%B4%ED%95%91)을 사용할 수 있습니다.  \n그리고 당연하게도, class 중심이 아니라 interface를 중심으로, 상속이 아니라 합성 관계를 중점으로 다시 생각해볼 수 있습니다.\n\n### 고 언어에서\n\n고 언어에서는 상속 대신 합성(composite)를 이용해서 다형성을 구성합니다.  \n다음 예제를 통해 간단하게 고 언어에서의 합성을 볼 수 있습니다.\n\n```go\nfunc main() {\n    f, err := os.Open(\"test.txt\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer f.Close()\n\n    f.Write([]byte(\"Hello, World!\"))\n}\n```\n\n이 코드는 `test.txt` 파일을 열고, `Hello, World!`를 쓰는 코드입니다.  \n여기서 `f`는 `*os.File` 타입입니다.  \n그리고 이 `*os.File`은 `io.WriteCloser` 인터페이스를 구현하고 있습니다.  \n`io.WriteCloser` 인터페이스를 구현함으로 `*os.File` 타입은 데이터 쓰기를 받을 수 있고, 연결을 닫을 수 있게 됩니다.\n\n```go\ntype WriteCloser interface {\n    Writer\n    Closer\n}\n\ntype Writer interface {\n\tWrite(p []byte) (n int, err error)\n}\n\ntype Closer interface {\n\tClose() error\n}\n```\n\n그리고 `*os.File`은 `io.WriteCloser` 인터페이스로 대체할 수 있습니다.  \n이렇게 말이죠!\n\n```go\nfunc writeHelloWorld(wc io.WriteCloser) {\n    wc.Write([]byte(\"Hello, World!\"))\n}\n\nfunc main() {\n    f, err := os.Open(\"test.txt\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer f.Close()\n\n    writeHelloWorld(f)\n}\n```\n\n또 위에 있다시피 `io.WriteCloser`는 `io.Writer`와 `io.Closer` 인터페이스를 포함하고 있습니다.  \n그러면 `*os.File`은 `io.Writer`와 `io.Closer` 인터페이스로 대체할 수 있습니다.\n\n```go\nfunc writeHelloWorld(w io.Writer) {\n    w.Write([]byte(\"Hello, World!\"))\n}\n\nfunc closeFile(c io.Closer) {\n    c.Close()\n}\n\nfunc main() {\n    f, err := os.Open(\"test.txt\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer closeFile(f)\n\n    writeHelloWorld(f)\n}\n```\n\n이 포함 관계, 어쩌면 상속 관계를 이렇게 나타낼 수 있습니다.\n\n![01](/img/023/01.png)\n\n단순한 인터페이스의 포함 관계로 Go 언어는 충분히 객체지향의 핵심적인 개념인 메시지 패싱과 다형성을 활용할 수 있습니다. 거디가 다음 규칙을 준수하며, 추가로 몇몇 다른 SOLID 원칙을 지킬 수 있습니다.\n\n1. 객체는 자신의 내부 구현을 숨기고, 인터페이스를 통해 다른 객체와 소통합니다. 이는 캡슐화 개념과도 연결됩니다.\n2. 객체는 자신이 구현하는 인터페이스의 규약을 반드시 준수해야 합니다. 이는 LSP의 핵심 요구사항과 일치합니다.\n3. 서브 인터페이스 타입, 혹은 인터페이스 구현 타입은 슈퍼 인터페이스로 대체할 수 있어야 합니다. 이는 DIP(의존성 역전 원칙)을 실현할 수 있도록 합니다.\n4. 하나의 인터페이스는 자신이 담당하는 역할에만 집중해야 합니다. 이는 SRP(단일 책임 원칙)과 ISP(인터페이스 분리 원칙)을 준수할 수 있도록 합니다.\n\n### Go 언어 외에선\n\nGo 언어에서는 언어 차원에서 이를 지원하고 독려하기 때문에 이러한 패턴을 쉽게 사용할 수 있습니다. 하지만 다른 언어에서도 이러한 패턴을 어렵지 않게 사용할 수 있습니다.\n\n몇몇 언어의 예시를 간단하게 작성해보겠습니다.\n\n- Java\n\n```java\ninterface Writer {\n    void write(char[] cbuf, int off, int len) throws IOException;\n}\n\ninterface Closer {\n    void close() throws IOException;\n}\n\ninterface WriteCloser extends Writer, Closer {\n    // 추가적인 메서드는 필요하지 않음\n}\n\nclass FileWriter implements WriteCloser {\n    private final FileWriter fileWriter;\n\n    public FileWriter(String fileName) throws IOException {\n        fileWriter = new FileWriter(fileName);\n    }\n\n    @Override\n    public void write(char[] cbuf, int off, int len) throws IOException {\n        fileWriter.write(cbuf, off, len);\n    }\n\n    @Override\n    public void close() throws IOException {\n        fileWriter.close();\n    }\n}\n```\n\n- C#\n\n```csharp\ninterface IWriter {\n    void Write(char[] buffer, int index, int count);\n}\n\ninterface ICloser {\n    void Close();\n}\n\ninterface IWriteCloser : IWriter, ICloser {\n    // 추가적인 메서드는 필요하지 않음\n}\n\nclass FileWriter : IWriteCloser {\n    private readonly StreamWriter streamWriter;\n\n    public FileWriter(string fileName) {\n        streamWriter = new StreamWriter(fileName);\n    }\n\n    public void Write(char[] buffer, int index, int count) {\n        streamWriter.Write(buffer, index, count);\n    }\n\n    public void Close() {\n        streamWriter.Close();\n    }\n}\n```\n\n- Typescript\n\n```typescript\ninterface Writer {\n    write(buffer: string): void;\n}\n\ninterface Closer {\n    close(): void;\n}\n\ninterface WriteCloser extends Writer, Closer {\n    // 추가적인 메서드는 필요하지 않음\n}\n\nclass FileWriter implements WriteCloser {\n    private readonly fs: fs.promises;\n\n    constructor(fileName: string) {\n        this.fs = require('fs').promises;\n    }\n\n    write(buffer: string) {\n        this.fs.writeFile(buffer);\n    }\n\n    close() {\n        this.fs.close();\n    }\n}\n```\n\n- Rust\n\n```rust\nuse std::io::{Write, Read};\n\ntrait WriteCloser: Write {\n    fn close(&mut self) -> std::io::Result<()>;\n}\n\nstruct File {\n    // ... (파일 관련 필드)\n}\n\nimpl WriteCloser for File {\n    fn close(&mut self) -> std::io::Result<()> {\n        // ... (파일 닫기 로직)\n    }\n\n    // Write trait의 메서드들을 여기에 직접 구현\n    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {\n        // ... (파일 쓰기 로직)\n    }\n\n    fn flush(&mut self) -> std::io::Result<()> {\n        // ... (버퍼 비우기 로직)\n    }\n}\n```\n\n- Dart\n\n```dart\nimport 'dart:io';\n\nabstract class Writer {\n  void write(List<int> bytes);\n}\n\nabstract class Closer {\n  void close();\n}\n\nabstract class WriteCloser implements Writer, Closer {\n  // 추가적인 메서드는 필요하지 않음\n}\n\nclass MyFile implements WriteCloser {\n  final File _file;\n\n  MyFile(String path) : _file = File(path);\n\n  @override\n  void write(List<int> bytes) {\n    _file.writeAsBytesSync(bytes);\n  }\n\n  @override\n  void close() {\n    _file.closeSync();\n  }\n}\n\nvoid main() {\n  final file = MyFile('example.txt');\n  file.write('Hello, world!'.codeUnits);\n  file.close();\n}\n```\n\n## 결론\n\n리스코프 치환 법칙은 객체지향 프로그래밍에서 중요한 법칙 중 하나입니다.  \n기존의 언어들은 class 기반의 상속을 통해 이를 달성하고자 했지만, 최근의 언어나 패러다임 중에는 interface와 composite 패턴만으로 달성하고자 하는 케이스가 있습니다.  \n이러한 패턴은 무리없이 리스코프 치환 원칙을 지킬 수 있으며, 객체지향을 구성하는 데에 일조할 수 있습니다.\n하지만 인터페이스를 너무 많이 분리하거나, 인터페이스에 역할을 너무 많이 부여하는 것은 오히려 코드의 복잡성을 높일 수 있으니 적절한 수준에서 사용하는 것이 중요합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/031_liskov_substitution_principle/", "created_date": "2024-07-23T18:54:27Z"}
{"title": "DevContainer로 개발환경 구성하기", "content": "## Dev Container\n\n`Dev Container`는 도커 컨테이너를 이용하여 개발 환경을 구축하는 방법입니다.  \n이 방식을 이용하면 쉽게 어느 곳에서나 동일한 개발 환경을 구축할 수 있습니다.  \n이 글에서는 `Go` 언어를 사용하는 개발 환경을 구축하는 방법을 소개합니다.\n\n### devcontainer 폴더 구성\n\n프로젝트 폴더 내에 `.devcontainer` 폴더를 생성합니다.  \n그리고 다시 한번 그 안에 원하는 개발 환경 이름으로 폴더를 생성합니다.\n\n```bash\n$ mkdir -p .devcontainer/practice-go\n```\n\n### devcontainer 설정 파일 작성\n\n`.devcontainer/practice-go` 폴더 내에 `devcontainer.json` 파일을 생성합니다.  \n저희는 전반적인 구성을 `docker-compose`로 하기 때문에 `docker-compose.yml` 파일을 참조하도록 설정합니다.\n\n```json\n{\n  \"name\": \"practice-go\",\n  \"dockerComposeFile\": [\n    \"docker-compose.yml\"\n  ],\n  \"service\": \"workspace\",\n  \"workspaceFolder\": \"/workspace\",\n  \"shutdownAction\": \"stopCompose\",\n  \"customizations\": {\n    \"vscode\": {\n      \"extensions\": [\n        \"golang.go-nightly\",\n      ]\n    }\n  }\n}\n\n```\n\n마지막의 `customizations` 부분은 `VSCode`에서 사용할 확장 프로그램을 설정하는 부분입니다.  \n일반적인 `golang.go` 확장을 쓰는 걸 권장하지만, 제 경우엔 `golang.go-nightly`를 사용하고 있어 그렇게 작성했습니다.\n\n### dockerfile 파일 작성\n\n`.devcontainer/practice-go` 폴더 내에 `Dockerfile` 파일을 생성합니다.  \n`Go` 언어를 사용하는 경우, `golang` 이미지를 사용하는 것이 일반적입니다.\n\n```dockerfile\nFROM golang:1.22-bookworm\n\nCMD [\"/bin/sh\", \"-c\", \"while true; do sleep 30; done;\"]\n```\n\n마지막의 `CMD` 부분은 컨테이너가 종료되지 않도록 하는 설정입니다.  \n이 설정이 없으면 컨테이너가 실행되자마자 종료되어 ssh 접속이 불가능합니다.\n\n만약 `sqlc`와 같은 도구를 사용하는 경우, 해당 도구를 설치하는 설정을 추가합니다.\n\n```dockerfile\nFROM golang:1.22-bookworm\n\nRUN go install github.com/sqlc-dev/sqlc/cmd/sqlc@latest && \\\n    echo 'export PATH=$PATH:$HOME/go/bin' >> /etc/profile\n\nCMD [\"/bin/sh\", \"-c\", \"while true; do sleep 30; done;\"]\n```\n\n### docker-compose 파일 작성\n\n`.devcontainer/practice-go` 폴더 내에 `docker-compose.yml` 파일을 생성합니다.\n\n```yaml\nversion: '3'\nservices:\n  workspace:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - ../../.:/workspace:cached\n```\n\n`volumes` 부분은 프로젝트 폴더를 컨테이너 내의 `/workspace` 폴더에 마운트하는 설정입니다.  \n뒤로 2번 가는 이유는 해당 도커 컴포즈 파일이 프로젝트 내의 `.devcontainer/practice-go` 폴더에 있기 때문입니다.\n\n여기서 굳이 `docker-compose`를 사용하는 이유는 필요한 경우 다른 서비스를 추가하기 위함입니다.\n\n### postgres 추가하기\n\n예를 들어, `PostgreSQL`을 사용하는 경우 `docker-compose.yml` 파일을 다음과 같이 수정합니다.\n\n```yaml\nversion: '3'\nservices:\n  workspace:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n      - POSTGRES_DB=postgres\n      - POSTGRES_HOST=localhost\n      - POSTGRES_PORT=5432\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ../../.:/workspace:cached\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER=postgres\n      POSTGRES_PASSWORD=postgres\n      POSTGRES_DB=postgres\n    network-mode: service:workspace\n```\n\n일반적인 도커 컴포즈와 유사하지만, `network-mode` 부분은 흔히 볼 수 없는 설정입니다.  \n이 설정은 `postgres` 컨테이너가 `workspace` 컨테이너와 같은 네트워크를 사용하도록 하는 설정입니다.  \n이렇게 하면 `localhost`로 접근이 가능합니다.\n\n마지막으로 `postgres` 컨테이너가 `workspace` 컨테이너의 네트워크를 사용하기 떄문에,  \n`postgres` DB에 호스트 머신에서 접근하려면 `workspace`에서 5432 포트를 포워딩해야 합니다.\n\n### 환경 변수 분리\n\n`docker-compose.yml` 파일에 환경 변수를 직접 적는 것은 너무 번거롭습니다.  \n그래서 환경 변수를 별도의 파일로 분리하고, `docker-compose.yml` 파일에서 참조하도록 합니다.  \n먼저 `.env` 파일을 생성합니다.\n\n```bash\n$ touch .env\n```\n\n그리고 다음과 같이 환경 변수를 설정합니다.\n\n```bash\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres\nPOSTGRES_DB=postgres\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\n```\n\n마지막으로 `docker-compose.yml` 파일을 다음과 같이 수정합니다.\n\n```yaml\nversion: '3'\nservices:\n  workspace:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=${POSTGRES_DB}\n      - POSTGRES_HOST=${POSTGRES_HOST}\n      - POSTGRES_PORT=${POSTGRES_PORT}\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ../../.:/workspace:cached\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER=${POSTGRES_USER}\n      POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      POSTGRES_DB=${POSTGRES_DB}\n    network-mode: service:workspace\n```\n\n그러면 `docker-compose`가 `.env` 파일을 읽어 환경 변수를 설정하여, 중복된 설정을 줄일 수 있습니다.\n\n### Dev Container 실행\n\n#### VSCode\n\n`VSCode`에서 `Dev Containers`와 `Remote Development` 확장을 설치합니다.  \n그리고 `Command(or Ctrl) + Shift + P`를 눌러 `Remote-Containers: Reopen in Container`를 검색하고 실행합니다.  \n그러면 `VSCode`가 재시작되고, `Dev Container`를 빌드한 후 실행됩니다.\n\n#### IntelliJ\n\n최신 버전의 `IntelliJ`에서는 `Remote Development` 기능을 지원합니다.  \n시작 화면에서 `Remote Development`를 선택하고, `Dev Container`를 선택하면 됩니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/028_dev_container/", "created_date": "2024-04-05T19:30:42Z"}
{"title": "요청 수를 줄이고, 캐시 수용량은 늘리기", "content": "## valkey?\n\n여전히 valkey보다 redis가 더 익숙한 분들이 계실 겁니다. valkey는 Redis Inc.의 redis에 대한 라이센스 변경 및 독점적 지위 확보 시도에 대한 반발로 redis 7.2.4를 베이스로 추가 개발되고 있는 레디스 대체제입니다. 현재는 레디스보다도 먼저 8.0 버전을 출시하는 등 굉장히 활발하게 개발 및 유지보수 되고 있습니다.\n\n당연한 이야기로 레디스의 포크인 만큼 레디스와 동일하게 다양한 곳에서 캐시로써 활용되고 있습니다. 이 글에서는 그 중 가장 보편적인 흐름 중 하나인 유저가 API 서버에 질의 후, API 서버는 캐시를 조회, 실패 시 RDB에서 조회하는 방식을 사용하는 패턴에서 효과적으로 밸키에 대한 읽기를 감소 시키는 방법을 소개하고자 합니다.\n\n## CDN\n\n가장 쉬운 접근 법은 CDN을 쓰는 겁니다. 내부적으로 어떻게 되어 있는 지는 담당 서비스 개발자가 아니라 알 수 없으나, 다양한 방법을 이용해 단 시간 내에 들어온 중복된 요청에 대해서는 뒤 API나 스토리지를 검색하지 않고, 캐시된 데이터를 요청자에게 반환할 겁니다.\n\n우리 코드를 수정할 필요도 없고, 버튼 몇개 누르는 선에서 아마 가능할 테지만, 추가 비용이 많이 들 수 있다는 건 굉장한 단점입니다. 그럼 다음으로 소개할 두 기법으로 우리가 CDN처럼 요청을 필터링해서 부하를 줄여보도록 하겠습니다.\n\n## Single-Flight\n\n싱글플라이트는 간단하게 설명하면, 같은 결과가 나올 것으로 예상되는 동 시점에 발생한 다수의 요청에 대하여 한번의 처리만 수행하는 걸 의미합니다. 이런 경우를 가정해봅시다. 여러분들은 커뮤니티 서비스를 하나 개발했습니다. 해당 서비스에는 인기 게시글 랭킹을 실시간으로 제공해주고 있습니다. 동시 접속 유저가 적을 때는 모든 요청을 각각 조회해도 부하가 되지 않겠지만, 만약 동접이 매우 늘어나게 된다면 모든 요청에 대해 DB나 캐시를 조회하는 건 합리적이지 못할 겁니다.\n\n이런 경우에, 어차피 1초 간격으로 들어온 요청들은 같은 내용의 인기 게시글 랭킹을 응답으로 받아갈 가능성이 높습니다. 그렇다면 첫 요청으로부터 1초 안에 들어온 모든 요청들은, 혹은 요청 처리에 어느정도 시간이 소요된다면 첫 요청으로부터 처리가 끝나기 전에 들어온 모든 요청들은 굳이 처리하지 않고, 하나의 응답을 공유하면 될 겁니다. 이러한 역할을 해주는 기능이 싱글플라이트입니다.\n\n### 예제\n\n먼저, 저희는 고 언어로 할테니 고 프로젝트를 만들어 줍니다.\n\n```sh\ngo mod init practice\n```\n\n그리고, x 라이브러리로 분류되어 있는 `sync/singleflight`를 가져옵니다.\n\n```sh\ngo get golang.org/x/sync/singleflight\n```\n\n하나의 500ms가 걸리는 작업을 시뮬레이트 하는 코드를 작성해보겠습니다.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"log\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n)\n\nfunc main() {\n\tsigChan := make(chan os.Signal, 1)\n\tsignal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM, syscall.SIGKILL)\n\n\tserver := new(http.Server)\n\tserver.Addr = \":8080\"\n\tserver.Handler = http.HandlerFunc(LongRequestHandler)\n\n\tgo func() {\n\t\tlog.Printf(\"Starting server on %s\", server.Addr)\n\t\tif err := server.ListenAndServe(); err != nil {\n\t\t\tlog.Printf(\"Error starting server: %s\", err)\n\t\t}\n\t}()\n\n\t<-sigChan\n\n\tlog.Printf(\"Shutting down server...\")\n\n\tif err := server.Shutdown(context.TODO()); err != nil {\n\t\tlog.Printf(\"Error shutting down server: %s\", err)\n\t}\n\n\tlog.Printf(\"Server shutdown complete\")\n}\n\nfunc LongRequestHandler(w http.ResponseWriter, r *http.Request) {\n\tlog.Printf(\"Received request from %s\", r.RemoteAddr)\n\t// Simulate a long request\n\tresult := SearchDB(\"query\")\n\tif err := json.NewEncoder(w).Encode(result); err != nil {\n\t\tlog.Printf(\"Error encoding response: %s\", err)\n\t}\n\n\tlog.Printf(\"Request from %s complete\", r.RemoteAddr)\n}\n\nfunc SearchDB(query string) []string {\n\t// Simulate a long database query\n\ttime.Sleep(500 * time.Millisecond)\n\treturn []string{\"result1\", \"result2\", \"result3\"}\n}\n```\n\n이 코드는 어떤 라우트로 들어오든 간에 `LongRequestHandler`에 할당될 것이고, `SearchDB`라는 500ms 짜리 메서드를 실행할 겁니다. 그럼 모든 요청은 공평하게 최소 500ms 후에 받게되겠죠. 만약 단순 time sleep이 아니라 더 큰 무언가라면 서비스 운영에 문제가 되었을 겁니다. 너무 많은 read로 인해 write가 지연된다거나 하는 형식으로 실시간성을 지킬 수 없게 될 수도 있죠.\n\n미리 추가한 싱글플라이트 패키지로 이 코드를 중복 요청을 줄여봅시다.\n\n```go\nvar searchDBSingleFlightGroup = singleflight.Group{}\n\nfunc SearchDB(query string) []string {\n\tresponse, err, shared := searchDBSingleFlightGroup.Do(query, func() (interface{}, error) {\n\t\ttime.Sleep(500 * time.Millisecond)\n\t\treturn []string{\"result1\", \"result2\", \"result3\"}, nil\n\t})\n\tif err != nil {\n\t\tlog.Printf(\"Error searching DB: %s\", err)\n\t\treturn nil\n\t}\n\n\tlog.Printf(\"is shared: %v\", shared)\n\n\treturn response.([]string)\n}\n```\n\n> 여기서는 전역 변수로 활용하였으나, 실제로 작업할 때는 객체에 넣어서 쓰시는 편이 더 좋습니다.\n\n`searchDBSingleFlightGroup`을 새로 만들어서, 하나의 싱글플라이트 그룹을 생성합니다. 그리고 `SearchDB` 메서드 내에서 각 쿼리에 따라 응답을 공유받도록 합니다. 이때, 반환된 `shared`를 통해 내 요청이 오리지널이 아니라는 걸 확인할 수 있습니다.\n\n이 코드 흐름 대로 동작한다면, 아무리 많은 요청이 들어와서 DB에 대한 조회는 500ms 마다 딱 한번 하는 것에 그칠 겁니다. 이것만으로 충분히 많은 양의 요청 수를 줄일 수 있습니다. 하지만 저희가 조회하는 곳이 밸키인 만큼 저희는 한가지 더 효과적인 실제 요청 수 감소를 만들 수 있습니다.\n\n## Server Assisted Client Side Cache\n\n이론적인 부분은 제가 고수다 블로그에 쓴 [글](https://gosuda.org/ko/blog/posts/improving-responsiveness-with-redis-client-side-caching-zb711e502)을 참고해주세요.\n\n해당 글에 거의 다 있는 얘기입니다. 밸키 또한 레디스의 포크인 만큼 server assisted client side cache 기능을 제공합니다. 간략하게 설명드리면, 해당 기능은 밸키에서 값이 갱신/삭제 되었을 시에 한번이라도 해당 키에 접근한 기록이 있는 클라이언트에게 \"해당 키가 변경되었으니 로컬 캐시를 만료시켜\"라는 메시지를 전송하여, 다음 조회부터 로컬 캐시가 아니라 밸키를 조회하게 만들고, 해당 값으로 로컬 캐시를 갱신하도록 도와줍니다.\n\n그럼 `valkey-go` 라이브러리를 받습니다. 위 고수다 블로그 글의 `rueidis`를 포크한 것이 `valkey-go`입니다.\n\n```sh\ngo get github.com/valkey-io/valkey-go\n```\n\n마찬가지로 밸키 클라이언트를 적용해줍니다.\n\n```go\nvar valkeyClient, _ = valkey.NewClient(valkey.ClientOption{\n\tInitAddress: []string{\"localhost:7001\", \"localhost:7002\", \"localhost:7003\"},\n})\n\nfunc SearchDB(query string) []string {\n\tresponse, err, shared := searchDBSingleFlightGroup.Do(query, func() (interface{}, error) {\n\t\tresp, err := valkeyClient.DoCache(context.TODO(), valkeyClient.B().Lrange().Key(\"favorite_posts\").Start(0).Stop(10).Cache(), 10*time.Second).AsStrSlice()\n\t\tif err != nil {\n\t\t\tlog.Printf(\"Error caching response: %s\", err)\n\t\t\treturn []string{}, nil\n\t\t}\n\n\t\treturn resp, nil\n\t})\n\tif err != nil {\n\t\tlog.Printf(\"Error searching DB: %s\", err)\n\t\treturn nil\n\t}\n\n\tlog.Printf(\"is shared: %v\", shared)\n\n\treturn response.([]string)\n}\n```\n\n> 이번에도 편의상 전역변수를 사용했습니다. 작업할 때는 객체로 처리해주길 바랍니다.\n\n해당 로직은 이제 밸키 클라이언트가 밸키에서 키 변경이 일어나지 않는다면, 최초 조회 이후 10초 동안은 로컬 캐시에서 데이터를 읽게 됩니다.\n\n그러면 최선의 경우에서 싱글플라이트로 500ms 마다 한번 로컬 캐시를 읽고, 밸키에 대해서는 10초에 한번 요청하게 됩니다.\n\n## 결론\n\n이 글에서 API가 실제 데이터 소스에 접근하는 회수를 줄이는 방법에 대해 설계된 부분을 공유 드리고자 했습니다. 이 방식은 로직 복잡도가 조금 있을 수는 있지만, 효과 자체는 우수하다고 보고 있습니다. 예를 들어, API 게이트웨이를 운영한다면 뒤에 존재하는 각 서비스로의 실제 IO 요청 자체를 줄이는 것이 OS에 대한 부하 자체를 줄이는 효과를 불러일으켜 CPU와 메모리를 아끼는 효과 또한 가져올 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/045_single_flight_and_client_side_cache/", "created_date": "2025-03-09T00:14:15Z"}
{"title": "Flutter 설치", "content": "## 개요\n\nflutter를 설치하는 걸 어려워 하시는 분들이 적잖이 계신 것같아 작성합니다.\n\n## OS 별로 설치 방법\n\n### Windows\n\n#### Script를 이용해서\n\nWritten by Gemini.\n\nPowerShell 스크립트입니다.  \n아무 곳이나 `get_flutter.ps1`같은 파일을 만들어 아래 스크립트를 붙여넣기 하여 저장하고 실행합니다.  \n그럼 버전을 입력해야하는데, [flutter 아카이브](https://docs.flutter.dev/release/archive)에서 확인 후 적절한 버전을 입력하면 됩니다.\n\n```powershell\n# PowerShell 스크립트: Flutter SDK 다운로드\n\n# --- 설정 ---\n# Flutter 다운로드 URL의 기본 구조\n$flutterBaseUrl = \"https://storage.googleapis.com/flutter_infra_release/releases/stable/windows/flutter_windows_\"\n$flutterUrlSuffix = \"-stable.zip\"\n\n# --- 사용자 입력 받기 ---\n# 사용자에게 다운로드할 Flutter 버전 번호 요청 (예: 3.29.2)\n# 사용자가 입력할 때까지 스크립트가 여기서 대기합니다.\n$flutterVersionInput = Read-Host -Prompt \"다운로드할 Flutter 버전을 입력하세요 (예: 3.29.2). 정확한 버전 번호는 Flutter 웹사이트에서 확인하세요\"\n\n# 입력값 유효성 검사 (간단히 비어있는지만 확인)\nif ([string]::IsNullOrWhiteSpace($flutterVersionInput)) {\n    Write-Error \"버전 번호를 입력해야 합니다. 스크립트를 종료합니다.\"\n    # 스크립트 비정상 종료 (오류 코드 1)\n    exit 1\n}\n\n# 입력받은 버전으로 전체 다운로드 URL 구성\n# 사용자가 '3.19.6'을 입력하면, URL은 \".../flutter_windows_3.19.6-stable.zip\" 형태가 됩니다.\n$flutterZipUrl = \"$($flutterBaseUrl)$($flutterVersionInput)$($flutterUrlSuffix)\"\n\n# 다운로드 받을 임시 파일 경로 설정\n$tempDir = $env:TEMP\n# 파일 이름에도 버전을 포함시켜 구분 용이하게 함 (선택 사항)\n$downloadFileName = \"flutter_sdk_$($flutterVersionInput).zip\"\n$tempDownloadPath = Join-Path $tempDir $downloadFileName\n\n# 압축 해제 대상 폴더 (사용자 홈 폴더)\n$extractDestination = $HOME\n\n# --- 스크립트 실행 ---\nWrite-Host \"Flutter SDK v$($flutterVersionInput) 다운로드 및 설치를 시작합니다...\" -ForegroundColor Cyan\n\n# 1. Flutter SDK 다운로드\nWrite-Host \"Flutter SDK 다운로드 중... ($flutterZipUrl)\"\nWrite-Host \"임시 저장 경로: $tempDownloadPath\"\ntry {\n    # Invoke-WebRequest를 사용하여 파일 다운로드\n    Invoke-WebRequest -Uri $flutterZipUrl -OutFile $tempDownloadPath -ErrorAction Stop\n    Write-Host \"다운로드 완료.\" -ForegroundColor Green\n} catch {\n    # 오류 발생 시, 입력한 버전이나 URL이 잘못되었을 가능성을 명시\n    Write-Error \"Flutter SDK v$($flutterVersionInput) 다운로드 실패: $($_.Exception.Message)\"\n    Write-Error \"입력한 버전 번호('$flutterVersionInput')가 정확한지, 해당 버전이 stable 채널에 존재하는지 확인하세요.\"\n    Write-Error \"시도한 URL: $flutterZipUrl\"\n    # 실패 시 임시 파일 삭제 (존재하는 경우)\n    if (Test-Path $tempDownloadPath) {\n        Remove-Item -Path $tempDownloadPath -Force -ErrorAction SilentlyContinue\n    }\n    # 스크립트 중단\n    exit 1\n}\n\n# 2. Flutter SDK 압축 해제\nWrite-Host \"Flutter SDK 압축 해제 중... ($extractDestination)\"\ntry {\n    # Expand-Archive를 사용하여 zip 파일 압축 해제\n    Expand-Archive -Path $tempDownloadPath -DestinationPath $extractDestination -Force -ErrorAction Stop\n    Write-Host \"압축 해제 완료.\" -ForegroundColor Green\n\n    # 압축 해제 확인\n    $expectedFlutterPath = Join-Path $extractDestination \"flutter\"\n    if (Test-Path $expectedFlutterPath) {\n        Write-Host \"Flutter SDK가 '$expectedFlutterPath' 경로에 성공적으로 압축 해제되었습니다.\"\n    } else {\n        Write-Warning \"'$expectedFlutterPath' 경로를 찾을 수 없습니다. 압축 해제 결과를 확인하세요.\"\n    }\n\n} catch {\n    Write-Error \"Flutter SDK 압축 해제 실패: $($_.Exception.Message)\"\n    # 압축 해제 실패 시에도 임시 파일 삭제\n    if (Test-Path $tempDownloadPath) {\n        Write-Host \"압축 해제 오류 발생. 임시 파일 삭제 중: $tempDownloadPath\"\n        Remove-Item -Path $tempDownloadPath -Force -ErrorAction SilentlyContinue\n    }\n    # 스크립트 중단\n    exit 1\n}\n\n# 3. 임시 다운로드 파일 삭제\nWrite-Host \"임시 다운로드 파일 삭제 중... ($tempDownloadPath)\"\ntry {\n    if (Test-Path $tempDownloadPath) {\n        Remove-Item -Path $tempDownloadPath -Force -ErrorAction Stop\n        Write-Host \"임시 파일 삭제 완료.\" -ForegroundColor Green\n    } else {\n        Write-Host \"임시 파일이 이미 삭제되었거나 존재하지 않습니다.\"\n    }\n} catch {\n    Write-Warning \"임시 파일 '$tempDownloadPath' 삭제 실패: $($_.Exception.Message)\"\n}\n\n# 추가할 경로 정의\n$flutterBinPathToAdd = Join-Path $HOME \"flutter\\bin\"\n$variableName = \"Path\"\n$variableScope = \"User\" # 사용자 범위 지정\n\n# 현재 사용자 PATH 값 가져오기\n$currentUserPath = [System.Environment]::GetEnvironmentVariable($variableName, $variableScope)\n\n# 경로가 이미 포함되어 있는지 확인 (중복 추가 방지)\nif ($currentUserPath -split ';' -notcontains $flutterBinPathToAdd) {\n    # 기존 PATH 끝에 세미콜론(;)과 새 경로 추가\n    # 기존 PATH가 비어있거나 세미콜론으로 끝나지 않는 경우 고려\n    $newUserPath = ($currentUserPath, $flutterBinPathToAdd) -join ';' -replace ';{2,}', ';' # 여러 개의 세미콜론을 하나로 정리\n\n    # 새 PATH 값 설정\n    [System.Environment]::SetEnvironmentVariable($variableName, $newUserPath, $variableScope)\n\n    Write-Host \"사용자 환경 변수 '$variableName'에 '$flutterBinPathToAdd' 경로를 추가했습니다.\" -ForegroundColor Green\n    Write-Host \"변경 사항을 적용하려면 새 PowerShell 창을 열거나 로그아웃 후 다시 로그인하세요.\"\n} else {\n    Write-Host \"'$flutterBinPathToAdd' 경로는 이미 사용자 '$variableName' 환경 변수에 존재합니다.\" -ForegroundColor Yellow\n}\n\n# --- 완료 메시지 및 다음 단계 안내 ---\nWrite-Host \"--------------------------------------------------\" -ForegroundColor Cyan\nWrite-Host \"Flutter SDK v$($flutterVersionInput) 설치 스크립트가 완료되었습니다.\" -ForegroundColor Cyan\nWrite-Host \"다음 단계를 진행하세요:\"\nWrite-Host \"- 새 PowerShell 창을 열고 'flutter doctor' 명령어를 실행하여 설치 상태를 확인하세요.\"\nWrite-Host \"--------------------------------------------------\" -ForegroundColor Cyan\n```\n\n#### scoop을 이용해서\n\n[Scoop](https://scoop.sh/)은 windows에서 사용할 수 있는 패키지 매니저입니다. PowerShell에서 편하게 쓸 수 있습니다.\n\n```powershell\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nInvoke-RestMethod -Uri https://get.scoop.sh | Invoke-Expression\n```\n\n```powershell\nscoop bucket add extras\nscoop install extras/flutter\nflutter doctor\n```\n\n### MacOS\n\n#### homebrew를 이용해서\n\nhomebrew는 MacOS에서 쓸 수 있는 보편적인 패키지 매니저입니다.\n\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install --cask flutter\n```\n\n#### Script를 통해서\n\nWritten by gemini.\n\n테스트는 맥이 없어서 못 해봤습니다.\nhomebrew 쓰세요.\n\n```sh\n#!/bin/zsh\n\n# macOS용 Flutter SDK 설치 스크립트 (Zsh)\n\n# --- 설정 및 아키텍처 감지 ---\necho \"macOS 아키텍처 확인 중...\"\narch=$(uname -m)\nif [[ \"$arch\" == \"arm64\" ]]; then\n  platform_arch=\"macos_arm64\"\nelif [[ \"$arch\" == \"x86_64\" ]]; then\n  platform_arch=\"macos_x64\"\nelse\n  echo \"오류: 지원되지 않는 아키텍처입니다: $arch\" >&2\n  exit 1\nfi\necho \"감지된 아키텍처: $arch ($platform_arch)\"\n\nflutter_base_url=\"https://storage.googleapis.com/flutter_infra_release/releases/stable/macos/\"\nflutter_suffix=\"-stable.zip\"\n\n# --- 사용자 입력 받기 ---\necho -n \"다운로드할 Flutter 버전을 입력하세요 (예: 3.19.6). 정확한 버전 번호는 Flutter 웹사이트에서 확인하세요: \"\nread flutter_version_input\n\n# 입력값 유효성 검사 (간단히 비어있는지만 확인)\nif [[ -z \"$flutter_version_input\" ]]; then\n    echo \"\\n오류: 버전 번호를 입력해야 합니다. 스크립트를 종료합니다.\" >&2\n    exit 1\nfi\n\n# --- 다운로드 URL 및 경로 설정 ---\nflutter_zip_url=\"${flutter_base_url}flutter_${platform_arch}_${flutter_version_input}${flutter_suffix}\"\n\n# 임시 다운로드 경로 설정 (TMPDIR 환경 변수 사용, 없으면 /tmp 사용)\ntemp_dir=\"${TMPDIR:-/tmp}\"\ndownload_filename=\"flutter_sdk_${platform_arch}_${flutter_version_input}.zip\"\ntemp_download_path=\"$temp_dir/$download_filename\"\n\n# 압축 해제 대상 폴더 (사용자 홈 디렉토리)\nextract_destination=\"$HOME\"\nexpected_flutter_path=\"$extract_destination/flutter\" # 압축 해제 후 예상되는 경로\n\n# --- 스크립트 실행 ---\necho \"\\nFlutter SDK v$flutter_version_input ($platform_arch) 다운로드 및 설치를 시작합니다...\"\n\n# 1. Flutter SDK 다운로드\necho \"Flutter SDK 다운로드 중... ($flutter_zip_url)\"\necho \"임시 저장 경로: $temp_download_path\"\n# curl 옵션: -L (리디렉션 따라가기), -o (출력 파일 지정), -# (진행률 표시)\ncurl -L -o \"$temp_download_path\" -# \"$flutter_zip_url\"\n\n# curl 명령어 성공 여부 확인 ($? 는 마지막 명령어의 종료 코드)\nif [[ $? -ne 0 ]]; then\n  echo \"\\n오류: Flutter SDK 다운로드 실패.\" >&2\n  echo \"입력한 버전 번호('$flutter_version_input')가 정확한지, 해당 버전/아키텍처가 stable 채널에 존재하는지 확인하세요.\" >&2\n  echo \"시도한 URL: $flutter_zip_url\" >&2\n  # 실패 시 임시 파일 삭제 시도 (파일이 존재할 경우)\n  rm -f \"$temp_download_path\"\n  exit 1\nfi\necho \"다운로드 완료.\"\n\n# 2. Flutter SDK 압축 해제\necho \"Flutter SDK 압축 해제 중... ($extract_destination)\"\n# unzip 옵션: -q (조용히 실행), -d (대상 디렉토리 지정)\n# zip 파일 내의 'flutter' 폴더가 $HOME 아래에 풀리도록 함\nunzip -q \"$temp_download_path\" -d \"$extract_destination\"\n\n# unzip 명령어 성공 여부 확인\nif [[ $? -ne 0 ]]; then\n  echo \"\\n오류: Flutter SDK 압축 해제 실패.\" >&2\n  # 실패 시에도 임시 파일 삭제 시도\n  rm -f \"$temp_download_path\"\n  exit 1\nfi\n\n# 압축 해제 후 예상 경로 확인\nif [[ -d \"$expected_flutter_path\" ]]; then\n   echo \"압축 해제 완료. Flutter SDK 경로: $expected_flutter_path\"\nelse\n   # 압축 해제는 성공했지만 예상 경로가 없는 경우 (zip 파일 구조가 다르거나 예외 상황)\n   echo \"\\n경고: 압축 해제는 완료되었으나 예상 경로($expected_flutter_path)를 찾을 수 없습니다.\" >&2\n   echo \"홈 디렉토리($extract_destination) 내용을 확인하세요.\" >&2\n   # 이 경우에도 임시 파일은 삭제\nfi\n\n# 3. 임시 다운로드 파일 삭제\necho \"임시 다운로드 파일 삭제 중... ($temp_download_path)\"\nrm -f \"$temp_download_path\"\nif [[ $? -ne 0 ]]; then\n    echo \"\\n경고: 임시 파일 '$temp_download_path' 삭제 중 오류 발생.\" >&2\nelse\n    echo \"임시 파일 삭제 완료.\"\nfi\n\n\n# 4. PATH 환경 변수 설정 확인 및 추가 (.zshrc)\nflutter_bin_path=\"$expected_flutter_path/bin\" # ~/flutter/bin\nzsh_config_file=\"$HOME/.zshrc\"\n# .zshrc 파일에 추가될 실제 라인 (파일 내에서는 $HOME, $PATH가 해석되어야 함)\npath_export_line=\"export PATH=\\\"\\$HOME/flutter/bin:\\$PATH\\\"\"\n\necho \"\\n환경 변수 PATH 설정을 확인합니다 ($zsh_config_file)...\"\n\n# .zshrc 파일이 존재하는지 확인, 없으면 생성\nif [[ ! -f \"$zsh_config_file\" ]]; then\n  echo \"알림: '$zsh_config_file' 파일이 없어 새로 생성합니다.\"\n  touch \"$zsh_config_file\"\nfi\n\n# .zshrc 파일 내에 Flutter 경로가 이미 설정되어 있는지 확인 (grep -q: 결과 출력 없이 종료 코드로만 확인, -F: 고정 문자열 검색)\nif grep -qF \"$flutter_bin_path\" \"$zsh_config_file\"; then\n  echo \"'$flutter_bin_path' 경로는 이미 '$zsh_config_file'에 설정되어 있는 것 같습니다.\"\nelse\n  echo \"알림: '$zsh_config_file' 파일에 Flutter 경로를 추가합니다.\"\n  # 파일 끝에 주석과 함께 export 라인 추가\n  echo \"\\n# Flutter SDK PATH\" >> \"$zsh_config_file\"\n  echo \"$path_export_line\" >> \"$zsh_config_file\"\n  echo \"'$zsh_config_file' 파일 업데이트 완료.\"\n  echo \"\\n\\033[1;33m중요: 변경 사항을 적용하려면 터미널을 새로 시작하거나 다음 명령을 실행하세요:\\033[0m\" # 노란색 강조\n  echo \"source $zsh_config_file\"\nfi\n\n# --- 완료 메시지 및 다음 단계 안내 ---\necho \"\\n--------------------------------------------------\"\necho \"\\033[1;32mFlutter SDK v$flutter_version_input 설치 스크립트가 완료되었습니다.\\033[0m\" # 녹색 강조\necho \"다음 단계를 진행하세요:\"\necho \"1. (필요시) 터미널을 새로 시작하거나 'source $zsh_config_file' 명령을 실행하여 PATH 설정을 적용하세요.\"\necho \"2. 새 터미널에서 'flutter doctor' 명령어를 실행하여 설치 상태를 확인하세요.\"\necho \"--------------------------------------------------\"\n\nexit 0\n```\n\n### Linux\n\n우분투 개발자십니까? snap 쓰십시오.\n\n```sh\nsudo snap install flutter --classic\n```", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/046_flutter_getting_start/", "created_date": "2025-03-28T21:28:42Z"}
{"title": "logstream", "content": "이 글은 [제 로그 라이브러리](https://github.com/diy-cloud/logstream)를 작성하며 생각한 것을 작성한 것입니다.\n\n## 개요\n\n`logstream` 라이브러리는 로그를 생성하고 소비하는 패턴을 구현하기 위해 만든 일종의 고 프로그램 전역에서 돌아가는 메시지 큐입니다. 다른 메시지 큐와 마찬가지로 여러 생산자가 토픽에 값을 추가하고 소비자가 토픽에서 값을 꺼내서 사용합니다.\n\n![logstream](/img/006/logstream.png)\n\n생산자(producer)는 코드 상 어디라도 될 수 있으며 스트림을 내포하고 있는 객체는 글로벌큐(global queue)로 전역 객체로 생성될 것입니다. 소비자(customer)는 글로벌큐 내부 스트림을 통해 순차적으로 생산자가 만들어낸 로그를 받아서 소비합니다. 소비는 가급적이면 모든 소비자 중 하나가 한번 소비하면 로그가 사라지는 게 아니라 모든 소비자에게 돌아가는 걸 목표로 합니다.\n\n## 구조\n\n```bash\n.\n├── LICENSE\n├── README.md\n├── go.mod\n├── go.sum\n├── log\n│   ├── interface.go\n│   ├── logbuffer\n│   │   ├── logbuffer.go\n│   │   ├── logqueue\n│   │   │   ├── pq.go\n│   │   │   └── readme.md\n│   │   ├── logring\n│   │   │   ├── rb.go\n│   │   │   └── readme.md\n│   │   └── logstream\n│   │       ├── globalque\n│   │       │   └── globalque.go\n│   │       └── logstream.go\n│   ├── loglevel\n│   │   ├── level.go\n│   │   └── readme.md\n│   ├── readme.md\n│   ├── struct.go\n│   └── writable\n│       ├── nats\n│       │   └── nats.go\n│       ├── readme.md\n│       ├── stdout\n│       │   └── stdout.go\n│       └── writable.go\n└── test\n    └── main.go\n```\n\n제 프로젝트 구조는 레이어드 구조를 흉내낸 의식의 흐름 기법으로 되어 있습니다. 상위 디렉토리일수록 더욱 코어에 가깝고 하위 디렉토리일수록 더욱 활용에 가깝습니다.\n\n## log\n\n`log` 패키지에는 로그 구조체를 정의하는 부분과 하위 패키지에서 사용할 인터페이스를 정의한 부분이 있습니다.\n\n### Log\n\n```go\ntype Log struct {\n\tMessage string\n\tLevel   loglevel.LogLevel\n\tTime    time.Time\n}\n```\n\n원래 로그에는 메시지만 존재했습니다. 그러다가 논란의 중심이었던 `log4j`를 참고해서 로그 레벨을 넣게 되었고 언제 생성되었는지 파악하기 위한 `time.Time` 객체도 추가하게 되었습니다. 그리고 다른 로그 라이브러리들을 보던 중 로그에 매개변수를 넣는 것을 보았고 유용한 기능일 수도 있다는 생각에 매개변수를 추가하게 되었습니다. 하지만 이걸 어떠한 생성자 패턴도 없이 작성하는 건 엔드유저에게 가혹한 일이었기에 빌더 패턴도 만들게 되었습니다.\n\n#### LogFactory\n\n```go\ntype LogFactory struct {\n\tTime    time.Time\n\tMessage strings.Builder\n\tLevel   loglevel.LogLevel\n\n\thasParam bool\n}\n\nfunc New(level loglevel.LogLevel, message string) *LogFactory\n```\n\n로그 팩토리는 생성될 때 로그 레벨과 메시지를 받고 시간을 `New` 호출 시기로 지정합니다.\n\n```go\nfunc (l *LogFactory) AddParamString(key string, value string) *LogFactory\n\nfunc (l *LogFactory) AddParamInt(key string, value int) *LogFactory\n\nfunc (l *LogFactory) AddParamUint(key string, value uint) *LogFactory\n\nfunc (l *LogFactory) AddParamBool(key string, value bool) *LogFactory\n\nfunc (l *LogFactory) AddParamFloat(key string, value float64) *LogFactory\n\nfunc (l *LogFactory) AddParamComplex(key string, value complex128) *LogFactory\n\nfunc (l *LogFactory) End() Log\n```\n\n그리고 총 6개의 메서드를 통해 매개변수를 메시지에 추가할 수 있게 하였습니다. 마지막으로 `End()` 메서드를 호출함으로 로그 인스턴스를 생성한 후 스택이 사라지면 소멸합니다. \n\n## loglevel\n\n로그 레벨 패키지는 로그 디렉토리의 바로 한단계 하위에 존재합니다. 로그 레벨 패키지는 `LogLevel` 타입의 `enum`만을 위한 패키지로 내부에 선언된 코드는 이게 전부입니다.\n\n```go\npackage loglevel\n\ntype LogLevel int8\n\nconst (\n\tAll LogLevel = iota\n\tDebug\n\tInfo\n\tWarn\n\tError\n\tFatal\n\tOff\n)\n\nfunc WrapColor(level LogLevel, message string) string {\n\tswitch level {\n\tcase Debug:\n\t\treturn \"\\033[0;36m\" + message + \"\\033[0m\"\n\tcase Info:\n\t\treturn \"\\033[0;32m\" + message + \"\\033[0m\"\n\tcase Warn:\n\t\treturn \"\\033[0;33m\" + message + \"\\033[0m\"\n\tcase Error:\n\t\treturn \"\\033[0;31m\" + message + \"\\033[0m\"\n\tcase Fatal:\n\t\treturn \"\\033[0;35m\" + message + \"\\033[0m\"\n\tdefault:\n\t\treturn \"\\033[0;37m\" + message + \"\\033[0m\"\n\t}\n}\n\nfunc Available(criterion, loglevel LogLevel) bool {\n\treturn criterion <= loglevel\n}\n```\n\n다른 프레임워크나 라이브러리와 달리 하나의 enum 타입을 위해 패키지를 분리한 이유는 로그 레벨이 로그에 바로 쓰이기 때문이고 로그 패키지에서 호출하는 것보다 로그 레벨이라는 패키지에서 불러와서 쓰는 게 더 직관적이기 때문이라고 판단했습니다. 실제로 `log.LogLevelDebug`보다 `loglevel.Debug`로 쓰는 게 더 읽기 쉽고 자동 완성도 활용할 수 있을 거라 생각합니다.\n\n추가적으로 `WrapColor` 함수는 각 로그 레벨에 따른 색상을 입히는 함수이고 `Available` 함수는 기준이 되는 로그 레벨에서 입력받은 로그 레벨이 출력 가능한지를 반환합니다. 두 함수 다 각 소비자 구현체에 하드코딩해도 좋지만 일관성을 위해 로그 레벨 패키지에 함수로 선언하고 불러오는 방식을 취했습니다.\n\n## logbuffer\n\n로그 버퍼 패키지에는 하나의 인터페이스만 존재합니다. \n\n```go\npackage logbuffer\n\nimport \"github.com/snowmerak/logstream/log\"\n\ntype LogBuffer interface {\n\tPush(log log.Log) error\n\tPop() (log.Log, error)\n\tSize() int\n}\n```\n\n이 디렉토리부터 하위에 존재하는 패키지는 2가지 종류로 나뉘게 됩니다. 이 인터페이스는 구현한 객체이거나 이 인터페이스를 구현한 객체를 활용하는 패키지입니다. `LogBuffer` 인터페이스가 요구하는 건 로그를 넣는 메서드와 로그를 추출하는 메서드 뿐입니다. `Size()` 메서드의 구현 여부는 중요하지 않습니다.\n\n### logqueue\n\n먼저 `LogBuffer`를 구현한 객체 중 하나인 로그 큐입니다.\n\n```go\npackage logqueue\n\nimport (\n\t\"errors\"\n\n\t\"github.com/Workiva/go-datastructures/queue\"\n\t\"github.com/snowmerak/logstream/log\"\n\t\"github.com/snowmerak/logstream/log/logbuffer\"\n)\n\ntype LogQueue struct {\n\tqueue *queue.PriorityQueue\n}\n\nfunc New(size int) logbuffer.LogBuffer {\n\treturn &LogQueue{\n\t\tqueue: queue.NewPriorityQueue(size, false),\n\t}\n}\n\nfunc (lq *LogQueue) Push(log log.Log) error {\n\tif lq == nil {\n\t\treturn errors.New(\"LogQueue.Push: LogQueue is nil\")\n\t}\n\treturn lq.queue.Put(log)\n}\n\nfunc (lq *LogQueue) Pop() (log.Log, error) {\n\tif lq == nil {\n\t\treturn log.Log{}, errors.New(\"LogQueue.Pop: LogQueue is nil\")\n\t}\n\titem, err := lq.queue.Get(1)\n\tif err != nil {\n\t\treturn log.Log{}, err\n\t}\n\treturn item[0].(log.Log), nil\n}\n\nfunc (lq *LogQueue) Size() int {\n\tif lq == nil {\n\t\treturn 0\n\t}\n\treturn lq.queue.Len()\n}\n```\n\n로그 큐 구조체는 `github.com/Workiva/go-datastructures/queue`의 우선순위 큐를 래핑한 구조체입니다. 빈 인터페이스를 처리하게 만들어진 우선순위 큐를 가지고 와서 `LogBuffer` 인터페이스가 충족되도록 로그를 넣고 뺼 수 있는 구조체로 만들었습니다. \n\n### logring\n\n로그 링 패키지 또한 로그 큐와 마찬가지로 로그를 주고 받게 `github.com/Workiva/go-datastructures/queue`의 링버퍼를 래핑한 것입니다.\n\n```go\npackage logring\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/Workiva/go-datastructures/queue\"\n\t\"github.com/snowmerak/logstream/log\"\n\t\"github.com/snowmerak/logstream/log/logbuffer\"\n)\n\ntype LogRingBuffer struct {\n\tringbuffer *queue.RingBuffer\n}\n\nfunc New(size int) logbuffer.LogBuffer {\n\treturn &LogRingBuffer{\n\t\tringbuffer: queue.NewRingBuffer(uint64(size)),\n\t}\n}\n\nfunc (lrb *LogRingBuffer) Push(value log.Log) error {\n\treturn lrb.ringbuffer.Put(value)\n}\n\nfunc (lrb *LogRingBuffer) Pop() (log.Log, error) {\n\titem, err := lrb.ringbuffer.Get()\n\tif err != nil {\n\t\treturn log.Log{}, fmt.Errorf(\"LogRingBuffer.Pop: %w\", err)\n\t}\n\treturn item.(log.Log), nil\n}\n\nfunc (lrb *LogRingBuffer) Size() int {\n\treturn int(lrb.ringbuffer.Len())\n}\n```\n\n큰 차이는 존재하지 않습니다. 그리고 이건 따로 추상화하거나 정규화하지는 않은 규칙이지만 뒤에 나올 로그 스트림 패키지에서 이 둘을 활용하기 위해 두 구조체의 생성자가 동일한 시그니처(`func(int) logbuffer.LogBuffer`)를 하고 있습니다.\n\n### 왜 두가지 구현체가 필요한지\n\n원래는 링버퍼만 있었습니다. 링버퍼 구현체만 가지고 테스트를 하던 중 당연하지만 무심코 넘긴 부분을 발견했습니다. 당연히 동시적으로 실행하게 만들어져 있기에 뮤텍스 락을 누가 먼저 잡느냐에 따라 짧은 시간에 많은 로그가 들어오게 되면 순서가 엉키게 되는데 이걸 간과하고 있었던 것입니다. 그래서 같은 `github.com/Workiva/go-datastructures`의 큐 패키지에 있는 우선순위 큐를 가지고 와서 우선순위 큐를 기반으로 한 로그 스트림 코어를 만들게 되었습니다.\n\n우선순위 큐로 만든 구현체는 당연하게도 대체로 스테어블하게 로그를 출력했습니다. 그럼에도 입출력 성능 자체는 링버퍼가 우선순위 큐보다 빠르고 순서가 꼬이는 경우가 그렇게 많이 존재하지는 않을 거라 판단해서 링버퍼 구현체도 그대로 유지하고 로그 스트림 백엔드로 둘 다 채용 가능하도록 작성했습니다. 이 부분은 이미 인터페이스로 추상화 시켜놓아서 크게 어렵지는 않았습니다.\n\n## logstream\n\n로그스트림은 `LogBuffer` 인터페이스를 활용하는 패키지입니다. 로그스트림 패키지는 따로 인터페이스를 가지지 않고 단일 구현체만을 가지게 설계 되었으므로 바로 구조체를 정의합니다.\n\n```go\npackage logstream\n\nimport (\n\t\"errors\"\n\n\t\"github.com/Workiva/go-datastructures/trie/ctrie\"\n\t\"github.com/snowmerak/logstream/log\"\n\t\"github.com/snowmerak/logstream/log/logbuffer\"\n\t\"github.com/snowmerak/logstream/log/logbuffer/logqueue\"\n)\n\ntype LogStream struct {\n\ttrie              *ctrie.Ctrie\n\tbufferSize        int\n\tsignals           map[string]chan struct{}\n\tbufferConstructor func(int) logbuffer.LogBuffer\n}\n\nfunc New(bufferSize int, bufferConstructor func(int) logbuffer.LogBuffer) *LogStream {\n\treturn &LogStream{\n\t\ttrie:              ctrie.New(nil),\n\t\tbufferSize:        bufferSize,\n\t\tsignals:           map[string]chan struct{}{},\n\t\tbufferConstructor: bufferConstructor,\n\t}\n}\n```\n\n로그 스트림 구조체는 `github.com/Workiva/go-datastructures` 라이브러리의 `ctrie`를 트라이 구현체로 사용합니다. 토픽 분류를 하기 위해 락프리에 가까운 트라이가 필요했는데 딱 락프리 트라이가 존재해서 적용했습니다. 버퍼 사이즈는 `LogBuffer` 구현체의 인스턴스를 만들 때 쓰는 크기이고 버퍼 생성자는 먼저 만든 `logqueue`, `logring`의 생성자를 받는 걸 전제로 하고 있습니다.\n\n시그널은 각 토픽 별로 버퍼에 값이 들어왔음을 알리기 위한 채널을 저장합니다. `struct{}`를 주고 받음으로 메모리 사용을 최대한 줄여보려 했습니다.\n\n### topic\n\n```go\nfunc (e *LogStream) AddTopic(topic string, signal chan struct{}) {\n\tkey := []byte(topic)\n\tif _, ok := e.trie.Lookup(key); !ok {\n\t\te.trie.Insert(key, e.bufferConstructor(e.bufferSize))\n\t}\n\tif _, ok := e.signals[topic]; !ok {\n\t\te.signals[topic] = signal\n\t}\n}\n\nfunc (e *LogStream) RemoveTopic(topic string) {\n\tkey := []byte(topic)\n\tif _, ok := e.trie.Lookup(key); ok {\n\t\te.trie.Remove(key)\n\t}\n\tdelete(e.signals, topic)\n}\n```\n\n`AddTopic`으로 토픽을 추가하게 되면 각 토픽에 해당하는 트라이에 `LogBuffer` 인터페이스 구현체가 생성되고 시그널에는 매개변수로 입력받은 채널이 추가됩니다. `RemoveTopic`으로 토픽을 제거하면 이 과정을 반대로 수행합니다.\n\n이 구조를 가지게 된 이유는 `LogStream` 객체가 그 자체로 토픽에 따라 로그를 넣고 빼는 일종의 pub/sub MQ 역할에 충실하게 만들기 위함입니다.\n\n### enqueue, dequeue\n\n```go\nfunc (e *LogStream) EnQueue(topic string, value log.Log) {\n\tkey := []byte(topic)\n\tif _, ok := e.trie.Lookup(key); !ok {\n\t\te.trie.Insert(key, logqueue.New(e.bufferSize))\n\t}\n\tp, _ := e.trie.Lookup(key)\n\tringBuffer := p.(logbuffer.LogBuffer)\n\tringBuffer.Push(value)\n\tif e.signals[topic] != nil {\n\t\te.signals[topic] <- struct{}{}\n\t}\n}\n\nfunc (e *LogStream) DeQueue(topic string) (log.Log, error) {\n\tkey := []byte(topic)\n\tif _, ok := e.trie.Lookup(key); !ok {\n\t\treturn log.Log{}, errors.New(\"LogBuffer.DeQueue: topic not found\")\n\t}\n\tp, _ := e.trie.Lookup(key)\n\tringBuffer := p.(logbuffer.LogBuffer)\n\treturn ringBuffer.Pop()\n}\n```\n\n두 메서드 다 토픽과 로그, 혹은 토픽을 받고 결과를 돌려줍니다. `Enqueue`는 특이하게 시그널에서 토픽에 해당하는 채널을 찾아 값이 들어옴을 알려줍니다. 그러면 사용자는 `AddTopic`을 했을 때 넘겨준 채널을 감시하여 로그 스트림의 각 버퍼에 값이 들어옴을 확인하고 추출할 수 있습니다. 로그 스트림으로 전체 구조 중 MQ 부분에 해당하는 코드를 만들었습니다.\n\n## writable\n\n로그 패키지의 바로 아래에 있는 소비자 부분을 담당해줄 `writable` 인터페이스를 선언한 패키지입니다.\n\n```go\npackage writable\n\nimport \"github.com/snowmerak/logstream/log\"\n\ntype Writable interface {\n\tWrite(log log.Log) error\n\tClose() error\n}\n```\n\n### stdout\n\n`Writable` 인터페이스를 구현한 `Stdout` 구조체가 선언된 패키지입니다.\n\n```go\npackage stdout\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"os\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/snowmerak/logstream/log\"\n\t\"github.com/snowmerak/logstream/log/loglevel\"\n\t\"github.com/snowmerak/logstream/log/writable\"\n)\n\ntype Stdout struct {\n\tsync.Mutex\n\tlevel     loglevel.LogLevel\n\twriter    *bufio.Writer\n\tconverter func(log.Log) string\n\tctx       context.Context\n}\n\nfunc New(ctx context.Context, level loglevel.LogLevel, converter func(log.Log) string) writable.Writable {\n\ts := &Stdout{\n\t\twriter:    bufio.NewWriter(os.Stdout),\n\t\tlevel:     level,\n\t\tconverter: converter,\n\t\tctx:       ctx,\n\t}\n\treturn s\n}\n\nfunc (s *Stdout) Write(value log.Log) error {\n\ts.Lock()\n\tdefer s.Unlock()\n\tif loglevel.Available(s.level, value.Level) {\n\t\tif s.converter == nil {\n\t\t\ts.writer.Write([]byte(value.Time.Format(time.RFC3339Nano)))\n\t\t\ts.writer.Write([]byte(\" \"))\n\t\t\ts.writer.Write([]byte(value.Message))\n\t\t} else {\n\t\t\ts.writer.Write([]byte(s.converter(value)))\n\t\t}\n\t\ts.writer.WriteByte('\\n')\n\t\ts.writer.Flush()\n\t}\n\treturn nil\n}\n\nfunc (s *Stdout) Close() error {\n\treturn nil\n}\n```\n\n특이한 건 없습니다. `bufio.NewWriter`로 buffered write를 표준 출력에 하는 객체를 저장하고 출력할 기준 로그 레벨, 이 로그 레벨을 기준으로 `loglevel.Available` 함수가 허용하는 로그만이 출력됩니다. 그리고 `converter`라는 함수를 받습니다. 컨버터는 로그를 받아서 문자열로 만드는 함수로 사용자가 자신에게 편한 방식으로 로그를 가공하여 출력할 수 있게 해줍니다.\n\n`Write` 메서드에서는 컨버터가 `nil`인지 아닌지에 따라 결과가 달라집니다. `nil`이면 제가 기본으로 설정한 시간과 메시지를 출력하게 되고, 그렇지 않으면 컨버터의 실행결과가 출력됩니다. 이 코드가 가장 간단한 표준 출력 소비자를 담당하게 됩니다.\n\n## globalque\n\n마지막으로 스트림과 소비자를 이어줄 글로벌 큐 패키지입니다.\n\n```go\npackage globalque\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"sync\"\n\n\t\"github.com/snowmerak/logstream/log\"\n\t\"github.com/snowmerak/logstream/log/logbuffer\"\n\t\"github.com/snowmerak/logstream/log/logbuffer/logstream\"\n\t\"github.com/snowmerak/logstream/log/loglevel\"\n\t\"github.com/snowmerak/logstream/log/writable\"\n)\n\ntype GlobalQueue struct {\n\tctx     context.Context\n\tbuf     *logstream.LogStream\n\twriters map[string]writer\n\tlock    *sync.Mutex\n\tbufSize int\n}\n\nfunc New(ctx context.Context, bufConstructor func(int) logbuffer.LogBuffer, bufSize int) *GlobalQueue {\n\treturn &GlobalQueue{\n\t\tctx:     ctx,\n\t\tbuf:     logstream.New(bufSize, bufConstructor),\n\t\twriters: map[string]writer{},\n\t\tlock:    &sync.Mutex{},\n\t\tbufSize: bufSize,\n\t}\n}\n```\n\n글로벌 큐는 컨텍스트와 스트림 백엔드로 쓰일 로그 버퍼의 생성자, 버퍼의 크기를 받습니다. 이 중 뒤의 두가지는 바로 스트림을 생성하는데 쓰입니다.\n\n```go\ntype writer struct {\n\tlist   []writable.Writable\n\tsignal chan struct{}\n}\n\nfunc (ls *GlobalQueue) ObserveTopic(topic string, writers ...writable.Writable) error {\n\tls.lock.Lock()\n\tdefer ls.lock.Unlock()\n\tif _, ok := ls.writers[topic]; ok {\n\t\treturn errors.New(\"LogStream.AddTopic: topic already exists\")\n\t}\n\tls.writers[topic] = writer{\n\t\tlist:   writers,\n\t\tsignal: make(chan struct{}, ls.bufSize),\n\t}\n\tls.buf.AddTopic(topic, ls.writers[topic].signal)\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ls.ctx.Done():\n\t\t\t\tls.lock.Lock()\n\t\t\t\tfor _, w := range ls.writers[topic].list {\n\t\t\t\t\tw.Close()\n\t\t\t\t}\n\t\t\t\tls.buf.RemoveTopic(topic)\n\t\t\t\tclose(ls.writers[topic].signal)\n\t\t\t\tdelete(ls.writers, topic)\n\t\t\t\tls.lock.Unlock()\n\t\t\t\treturn\n\t\t\tcase <-ls.writers[topic].signal:\n\t\t\t\tl, err := ls.buf.DeQueue(topic)\n\t\t\t\tif err != nil {\n\t\t\t\t\tl = log.New(loglevel.Fatal, err.Error()).End()\n\t\t\t\t}\n\t\t\t\tfor _, w := range ls.writers[topic].list {\n\t\t\t\t\tw.Write(l)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\treturn nil\n}\n```\n\n글로벌 큐의 `ObserveTopic` 메서드로 각 토픽에 대해 소비자(`writable.Writable`)을 연결할 수 있습니다. `writer` 구조체는 이때 넘겨 받은 라이터블 리스트와 시그널 채널을 저장하는 역할을 합니다. 미리 로그 스트림에 만들어놓은 `AddTopic` 메서드를 호출하여 토픽과 채널을 등록하고 고루틴 하나를 만들어 감시합니다.\n\n고루틴 내부는 무한반복하면서 두가지 경우 중 하나를 실행하게 되는데, 하나는 `writer` 구조체의 시그널 채널에서 값을 받을 때 로그 스트림에서 토픽에 해당하는 값을 추출하여 각 소비자의 `Write` 메서드를 통해 로그를 소비시킵니다. 또 다른 경우는 초기에 입력받은 `context.Context`의 `Cancel`이 실행되었을 때로 연결된 모든 `writable.Writable`을 닫고 토픽을 지우고 채널을 닫은 후 고루틴을 종료합니다.\n\n### 생산자\n\n```go\nfunc (ls *GlobalQueue) Write(topic string, l log.Log) {\n\tls.buf.EnQueue(topic, l)\n}\n```\n\n생산자 포지션은 로그를 전송하기만 하면 되기에 간단하게 글로벌 큐에 `Write` 메서드를 만들어서 사용도록 두었습니다. 미리 로그 스트림에 정의해놓은 대로 토픽에 해당하는 로그 버퍼에 값이 추가되고 시그널 채널을 통해 값이 들어옴을 확인한 고루틴이 값을 소비하도록 할 것입니다.\n\n## 후회\n\n좀 더 패키지 구성을 잘 설계했으면 더 직관적으로 쉬운 구성과 코드를 만들 수 있을 거란 아쉬움이 많이 남습니다. 역할 배분도 로그 스트림에서 시그널 채널 부분을 사실 글로벌 큐에서 관리했어야 하는 게 아닌가 하는 부분도 아쉽습니다.  \n마지막으로 문서를 만들면서 작성한게 아니라 일단 코드를 짜고 그 뒤 리팩토링을 하고 작성하다보니 헷갈리는 부분이 생기는 것입니다. godoc이라도 제대로 작성해놔야 덜 고생하겠다 싶었습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/006_logstream/", "created_date": "2022-01-06T13:08:40Z"}
{"title": "구조체 임베딩과 프로모션, 그리고 상속", "content": "## 구조체 임베딩\n\n구조체 임베딩은 구조체를 다른 구조체의 필드로 사용하는 것을 말합니다. 예를 들어 다음과 같은 구조체가 있다고 가정해봅시다.\n\n```go\ntype Person struct {\n    Name string\n    Age int\n}\n```\n\n그리고 이 구조체를 다른 구조체의 필드로 사용한다면 다음과 같이 사용할 수 있습니다.\n\n```go\ntype Student struct {\n    Person\n    Grade int\n}\n```\n\n그러면 마치 `Student` 구조체에 `Person` 구조체의 필드가 포함된 것처럼 사용할 수 있습니다.\n\n```go\nstudent := Student{\n    Person: Person{\n        Name: \"snowmerak\",\n        Age: 20,\n    },\n    Grade: 3,\n}\n\nfmt.Println(student.Name) // snowmerak\nfmt.Println(student.Age) // 20\nfmt.Println(student.Grade) // 3\n```\n\n## 프로모션\n\n위 내용에서 `Person` 구조체의 값을 `Student`에서 바로 쓸 수 있게 해주는 고 언어의 장치를 프로모션이라고 합니다.  \n위 예시에서는 멤버만 다루었으나, 메서드도 다룰 수 있습니다.\n\n프로모션은 임베딩 되는 여러 구조체가 동일한 이름의 필드나 메서드를 가질 경우에는 호출할 수 없습니다. 명확한 구분을 위해 직접 호출해야 합니다.\n\n간단한 예시를 들어보겠습니다.\n\n```go\ntype Person struct {\n    Name string\n    Age int\n}\n\nfunc (p *Person) Eat() {\n\tprintln(\"Person Eat\")\n}\n\nfunc (p *Person) Sleep() {\n\tprintln(\"Person Sleep\")\n}\n\nfunc (p *Person) Say() {\n\tprintln(\"Person Say\")\n}\n```\n\n간단한 `Person` 구조체입니다. 이 구조체는 `Name`과 `Age` 멤버가 있으며, `Eat`, `Sleep`, `Say` 메서드가 있습니다.\n\n```go\npackage student\n\nimport \"~/person\"\n\ntype Student struct {\n\tperson.Person\n}\n\nfunc (s *Student) Study() {\n\tprintln(\"Student Study\")\n}\n```\n\n그리고 간단한 `Student` 구조체입니다. 이 구조체는 `Person` 구조체를 임베딩하고 있습니다.  \n그렇기에 `Student` 구조체는 `Person` 구조체의 멤버와 메서드를 사용할 수 있습니다.  \n간단한 테스트 코드로 호출이 되는 지 확인할 수 있습니다.\n\n```go\npackage student\n\nimport (\n\t\"prac/lib/person\"\n\t\"testing\"\n)\n\nfunc TestStudent(t *testing.T) {\n\ts := &Student{\n\t\tPerson: person.Person{\n\t\t\tName: \"Tom\",\n\t\t\tAge:  18,\n\t\t},\n\t\tGrade: 7,\n\t}\n\ts.Eat()\n\ts.Sleep()\n\ts.Say()\n\ts.Study()\n}\n```\n\n```bash\n=== RUN   TestStudent\nTom\n18\nPerson Eat\nPerson Sleep\nPerson Say\nStudent Study\n--- PASS: TestStudent (0.00s)\nPASS\n```\n\n생성할 때는 비록 직접 `Person` 구조체를 참조해야하지만, 호출할 때는 그 무엇도 구분 없이 `Student`의 것처럼 사용할 수 있습니다.\n\n### 오버라이딩\n\n프로모션이 발생하지 않는 조건에는 임베딩 되는 구조체의 필드나 메서드의 이름 중, 임베딩 하는 구조체에 이미 그 이름이 존재할 경우에 프로모션이 발생하지 않는 것도 있습니다.\n\n그래서 오버라이딩과 비슷한 기능을 만들 수 있죠.\n\n```go\nfunc (s *Student) Eat() {\n\tprintln(\"Student Eat\")\n}\n\nfunc (s *Student) Sleep() {\n\tprintln(\"Student Sleep\")\n}\n\nfunc (s *Student) Say() {\n\tprintln(\"Student Say\")\n}\n```\n\n`Student` 구조체에 위와같은 메서드들을 추가로 작성해보겠습니다.\n\n그리고 테스트를 수행하면 결과가 달라져 있을 것입니다.\n\n```bash\n=== RUN   TestStudent\nTom\n18\nStudent Eat\nStudent Sleep\nStudent Say\nStudent Study\n--- PASS: TestStudent (0.00s)\nPASS\n```\n\n`Student` 구조체의 메서드가 호출되었습니다. 이는 프로모션이 발생하지 않았기 때문입니다.\n\n### 부모 메서드 호출\n\n그러면 `Person`의 메서드는 어떻게 호출할 수 있을까요?  \n테스트 코드를 아래와 같이 수정해보겠습니다.\n\n```go\nfunc TestStudent(t *testing.T) {\n\ts := &Student{\n\t\tPerson: person.Person{\n\t\t\tName: \"Tom\",\n\t\t\tAge:  18,\n\t\t},\n\t\tGrade: 7,\n\t}\n\tprintln(s.Name)\n\tprintln(s.Age)\n\ts.Person.Eat()\n\ts.Person.Sleep()\n\ts.Person.Say()\n\ts.Study()\n}\n```\n\n그러면 다시 `Student`의 것이 아닌 `Person`의 것이 호출되는 것을 확인할 수 있습니다.\n\n```bash\n=== RUN   TestStudent\nTom\n18\nPerson Eat\nPerson Sleep\nPerson Say\nStudent Study\n--- PASS: TestStudent (0.00s)\nPASS\n```\n\n### 다이아몬드 문제\n\n다이아몬드 문제는 다중 상속에서 발생하는 문제입니다.\n\n근데 고 언어에서는 다중 상속을 지원하지 않습니다. 그래서 다이아몬드 문제가 발생하지 않습니다.  \n라고 말하면 너무 이상하지 않나요? 왜 개념적으로 그렇게 될 수 없는 지 얘기해주지 않으니까요.\n\n그럼 `Person`을 임베딩하는 `Korean` 구조체를 만들어보겠습니다.\n\n```go\npackage korean\n\nimport \"~/person\"\n\ntype Korean struct {\n\tperson.Person\n\tAddress string\n}\n\nfunc (k *Korean) Eat() {\n\tprintln(\"냠냠\")\n}\n\nfunc (k *Korean) Sleep() {\n\tprintln(\"쿨쿨\")\n}\n\nfunc (k *Korean) Say() {\n\tprintln(\"안녕하세요\")\n}\n```\n\n그리고 한국에 사는 학생 구조체를 만들어보겠습니다.\n\n```go\npackage kstudent\n\nimport (\n\t\"~/korean\"\n\t\"~/student\"\n)\n\ntype KoreanStudent struct {\n\tkorean.Korean\n\tstudent.Student\n}\n```\n\n혹시 첫번째 프로모션이 발생하지 않는 조건을 기억하시나요?  \n`Korean`과 `Student` 둘다 `Name`과 `Age`라는 필드를 가지고 있습니다.  \n그럼 어느 쪽의 필드를 사용할 지 모호해집니다. 그래서 프로모션이 발생하지 않습니다.  \n이게 임베딩에서의 다이아몬드 문제라고 생각합니다.\n\n```go\nfunc TestKoreanStudent(t *testing.T) {\n\tks := &KoreanStudent{}\n\tprintln(ks.Name)\n\tprintln(ks.Age)\n\tks.Eat()\n\tks.Sleep()\n\tks.Say()\n\tks.Study()\n}\n```\n\n이런 테스트 코드를 작성하면 `Eat`, `Sleep`, `Say` 메서드는 명확하지 않은 메서드를 사용하려고 했기에 컴파일 에러가 발생합니다.\n\n```bash\n.\\ks_test.go:7:13: ambiguous selector ks.Name\n.\\ks_test.go:8:13: ambiguous selector ks.Age\n.\\ks_test.go:9:5: ambiguous selector ks.Eat\n.\\ks_test.go:10:5: ambiguous selector ks.Sleep\n.\\ks_test.go:11:5: ambiguous selector ks.Say\n```\n\n그래서 이런 경우에는 `KoreanStudent`의 코드를 수정해야합니다.\n\n```go\ntype KoreanStudent struct {\n\tkorean.Korean\n\tstudent.Student\n\tName string\n\tAge  int\n}\n\nfunc (ks *KoreanStudent) Eat() {\n\tks.Korean.Eat()\n}\n\nfunc (ks *KoreanStudent) Sleep() {\n\tks.Korean.Sleep()\n}\n\nfunc (ks *KoreanStudent) Say() {\n\tks.Korean.Say()\n}\n```\n\n아쉽게도 `KoreanStudent` 구조체에 `Korean`과 `Student` 구조체의 중복된 필드와 메서드를 모두 재정의해주어야 합니다.\n\n그리고 위 테스트 코드를 다시 작성하고, 실행해보겠습니다.\n\n```go\nfunc TestKoreanStudent(t *testing.T) {\n\tks := &KoreanStudent{\n\t\tName: \"snowmerak\",\n\t\tAge:  20,\n\t}\n\tprintln(ks.Name)\n\tprintln(ks.Age)\n\tks.Eat()\n\tks.Sleep()\n\tks.Say()\n\tks.Study()\n}\n```\n\n```bash\n=== RUN   TestKoreanStudent\nsnowmerak\n20\n냠냠\n쿨쿨\n안녕하세요\nStudent Study\n--- PASS: TestKoreanStudent (0.00s)\nPASS\n```\n\n이제 의도한 대로 동작하는 것을 확인할 수 있습니다.\n\n다행히 고는 해당하는 케이스의 코드가 작성되면 IDE가 알려주거나, 컴파일 에러를 발생시켜주기 때문에 쉽게 발견할 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/020_embedding/", "created_date": "2023-08-14T19:01:08Z"}
{"title": "고루틴과 채널을 활용한 유사 액터 모델", "content": "## 액터 모델?\n\n액터 모델은 동시성 문제를 해결하기 위해 등장한 개념입니다. 각 액터는 독립된 스레드에서 실행되며, 메시지를 통해 상호작용합니다. 액터 모델은 다음과 같은 특징을 가집니다.\n\n1. 액터는 메모리를 공유하지 않습니다.\n2. 액터는 고유의 상태를 가질 수 있습니다.\n3. 액터 간의 통신은 메시지를 통해 이루어집니다.\n4. 액터 간의 통신은 비동기적입니다.\n5. 액터는 해당 메시지에 대응하는 동작을 수행합니다.\n\n### CSP와 너무 잘 맞지 않아?\n\n고 언어는 기반 패러다임에 CSP가 있는 만큼, 고루틴과 채널이라는 강력한 도구가 있습니다. 이를 이용해 타 언어에 비해 손 쉽게 액터 모델을 구현할 수 있습니다. 액터는 고루틴으로 띄우고, 메시지는 채널로 주고 받습니다. 그러면 자연스럽게 액터 간의 통신은 비동기적으로 동작합니다. 해당 메시지에 대해 라우팅이 필요한 경우에도, 여러 채널을 이용하여 `select` 문법을 통해 처리할 수 있습니다.\n\n## 간단한 동시성 처리 가능한 맵\n\n간단하게 동시성을 지원하는 맵을 가지는 액터를 만들어 보겠습니다.\n\n간단히 정리하면 다음 특징이 있습니다.\n\n1. `Set`, `Delete`, `Get` 메서드를 통해 맵에 접근합니다.\n2. 하나의 액터는 고루틴 하나로 이루어집니다.\n3. 하나의 고루틴 내에 `map[string]string` 타입의 상태를 가집니다.\n4. 사용이 끝난 후에는 `Stop` 메서드를 통해 액터를 종료 해야합니다.\n5. 컨텍스트(`context`)를 통해 상위 컨텍스트의 영향을 받을 수 있습니다.\n\n### tuple\n\n맵의 `Get` 메서드는 값을 반환해야하는데, 값 반환을 위해 채널을 사용해야합니다. 이를 위해 `Tuple` 구조체를 만들어 처리하겠습니다.\n\n```go\npackage tuple\n\ntype Tuple[K, V any] struct {\n    Key K\n    Value V\n}\n```\n\n### actor\n\n그리고 `cmap` 패키지를 만들어 아래 내용을 담습니다.\n\n```go\npackage cmap\n\nimport \"context\"\n\ntype Map struct {\n\tctx      context.Context\n\tcancel   context.CancelFunc\n\tsetCh    chan string\n\tdeleteCh chan string\n\tgetChCh  chan tuple.Tuple[string, chan string]\n\t\n    m        map[string]string\n}\n\nfunc NewMap(ctx context.Context, queueSize int) *Map {\n\tctx, cancel := context.WithCancel(ctx)\n\tdeleteCh := make(chan string, queueSize)\n\tgetCh := make(chan tuple.Tuple[string, chan string], queueSize)\n\tsetCh := make(chan string, tuple.Tuple[string, string], queueSize)\n    m := make(map[string]string)\n\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase value := <-setCh:\n                m[value.Key] = value.Value\n\t\t\tcase value := <-deleteCh:\n                delete(m, value)\n\t\t\tcase value := <-getCh:\n                value.Value <- m[value.Key]\n\t\t\t}\n\t\t}\n\t}()\n\treturn &Map{\n\t\tctx:      ctx,\n\t\tcancel:   cancel,\n\t\tgetCh:    getCh,\n\t\tsetCh:    setCh,\n\t\tdeleteCh: deleteCh,\n\t}\n}\n\nfunc (m *Map) Set(value string) {\n\tm.setCh <- value\n}\n\nfunc (m *Map) Delete(value string) {\n\tm.deleteCh <- value\n}\n\nfunc (m *Map) Get(value tuple.Tuple[string, chan string]) {\n\tm.getCh <- value\n}\n\nfunc (m *Map) Stop() {\n\tm.cancel()\n}\n```\n\n간단하게 `Set`, `Delete`, `Get` 메서드를 만들어 놓았습니다.  \n`Get` 메서드는 `Tuple`을 받아 `Key`를 통해 맵에서 값을 가져와 `Value`에 채널로 값을 보내주는 역할을 합니다.  \n`Set` 메서드는 `Tuple`을 받아 `Key`를 통해 맵에 값을 저장하는 역할을 합니다.  \n`Delete` 메서드는 `Key`를 받아 맵에서 값을 삭제하는 역할을 합니다.  \n`Stop` 메서드는 `context`를 취소하여 고루틴을 종료시킵니다.\n\n### actor system\n\n생성자를 통해 `ctx context.Context`를 받습니다.  \n이를 통해 상위 컨텍스트에서 `cancel`을 호출하면, 액터의 컨텍스트에서 `ctx.Done()`이 호출되어 고루틴이 종료됩니다.\n\n이 기능이 필요한 이유는 액터가 속한 액터 시스템이 액터를 컨트롤 할 수 있게 하기 위함입니다.  \n액터 시스템은 액터를 생성하고 관리하는 곳입니다.  \n액터 시스템의 특징으론 자신이 만든 액터의 전체 수명이 자신의 수명 범위에 포함되어야 합니다.  \n그렇기에 액터 시스템의 컨텍스트를 공유함으로 하위 액터의 수명을 일괄적으로 관리할 수 있습니다.\n\n마찬가지로 특정 액터가 하위 액터를 생성하는 경우에도 동일한 수명 한계를 가질 수 있게 도와줍니다.\n\n## 마지막으로\n\n간단한 `yaml` 파일로 액터 패키지를 만들 수 있는 `actor` 패키지를 만들어 보았습니다.\n\n제 [깃헙 레포지토리](github.com/snowmerak/gotor)에 업로드 해 놓았으니, 관심 있으시다면 `readme.md` 한번 읽어주시면 감사하겠습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/015_actor_model/", "created_date": "2023-03-19T18:28:34Z"}
{"title": "Context 패키지 (이론)", "content": "## 패키지 구성\n\n> Package context defines the Context type, which carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes.\n\n컨텍스트 패키지는 `Context` 타입을 구현하고 있는 패키지입니다. 이 컨텍스트 타입은 인터페이스로 실제로는 하부에 크게 5가지 타입의 컨텍스트 구현체가 존재합니다.\n\n## 컨텍스트 종류\n\n1. 아무 상태도 가지지 않은, `backgroundCtx`와 `todoCtx`\n2. 취소 시그널을 보내거나 무시할 수 있는, `cancelCtx`와 `withoutCancelCtx`\n3. 특정 시간만 동작하거나 특정 시각까지만 동작하는, `timerCtx`\n4. 키와 값을 저장하는, `valueCtx`\n5. 사후 처리를 담당하는, `afterFuncCtx`\n\n### backgroud, todo\n\n아마 세상에서 가장 많이 쓰이는 컨텍스트입니다.\n\n```go\nimport \"context\"\n\nfunc main() {\n    ctx := context.Background()\n}\n```\n\n> It is typically used by the main function, initialization, and tests, and as the top-level Context for incoming requests.\n\n`background` 컨텍스트에 대해서 패키지 주석으로 위와 같이 작성되어 있습니다.  \n즉, `main` 함수나 초기화, 테스트 등에서 사용되며, 들어오는 요청에 대한 최상위 컨텍스트로 사용됩니다.  \n원래 의도대로 라면 어떤 컨텍스트가 필요한 요청(예를 들면, DB 쿼리 등)에 `background` 컨텍스트를 사용하면 안됩니다.\n\n> Code should use context.TODO when it's unclear which Context to use or it is not yet available (because the surrounding function has not yet been extended to accept a Context parameter).\n\n그리고 `todo` 컨텍스트에 대해서는 위의 주석이 작성되어 있습니다.  \n즉, 어떤 컨텍스트를 사용해야 할지 모르거나, 아직 상위 컨텍스트를 사용할 수 없는 경우에 사용하라고 되어 있습니다.  \n그래서 위의 백그라운드 컨텍스트를 사용하면 안되는 예에 쓸 수 있는 적합한 컨텍스트입니다.  \n물론 어디까지나 컨텍스트를 어디서 가져오기 어려운 경우에서만 사용해야 합니다.\n\n그러면 `main` 엔트리 포인트에서는 `Background()`로 컨텍스트 하나를 생성 하겠습니다.\n\n```go\npackage main\n\nimport \"context\"\n\nfunc main() {\n    ctx := context.Background()\n}\n```\n\n### cancel\n\n`cancel` 컨텍스트는 취소 시그널을 보내어, 자기자신을 포함한 하위 컨텍스트의 작업을 중단 시킬 수 있습니다.\n\n해당 컨텍스트는 `WithCancel` 함수(`func WithCancel(parent Context) (ctx Context, cancel CancelFunc)`)와 `WithCancelCause` 함수(`func WithCancelCause(parent Context) (ctx Context, cancel CancelCauseFunc)`) 통해 생성할 수 있습니다.\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"errors\"\n    \"sync\"\n)\n\nfunc main() {\n    ctx, cancel := context.WithCancel(context.Background())\n    ctx, cancelCause := context.WithCancelCause(ctx)\n\n    wg := new(sync.WaitGroup)\n    wg.Add(2)\n    go func() {\n        defer wg.Done()\n        done := ctx.Done()\n        <-done\n        println(\"context canceled\")\n    }()\n    go func() {\n        defer wg.Done()\n        done := ctx.Done()\n        <-done\n        println(context.Cause(ctx))\n    }()\n\n    cancelCause(errors.New(\"context canceled\")) // 아래 고루틴만 취소합니다.\n    cancel() // 두 고루틴 모두 취소합니다.\n    wg.Wait()\n}\n```\n\n위의 예제는 `context`를 생성하고, `context`가 취소되면 `context canceled`를 출력하는 예제입니다.\n\n`cancel` 함수로 `ctx`가 취소되면, `ctx`를 포함한 모든 하위 컨텍스트의 `Done()` 메서드로 생성된 채널에 `struct{}` 타입의 값을 보냅니다. 그리고 해당 채널에서 값을 받으면, 컨텍스트가 취소 되었다고 판단하여 작업을 중단하게 됩니다.\n\n`cancelCause` 함수로 `ctx`가 취소되면, `cancel` 때와 동일한 동작을 보여줍니다. 한가지 다른 점은 `context.Cause` 함수로 취소된 이유를 알 수 있다는 점입니다. 이 구조는 에러를 전파하는 과정에 매우 중요한 역할을 합니다.\n\n많은 프로그래밍 언어는 에러가 기본적으로 상향식(bottom-up) 방식으로 전파됩니다. 그렇기에 문제가 발생해도 서로다른 스코프에 있거나, 수평적인 위치에 있는 다른 코드에서는 어떤 에러가 발생했는지 알 수 없습니다.\n\n하지만 `WithCancelCause`로 생성된 `cancelCtx`의 경우엔 에러가 하향식(top-down) 방식으로 전파됩니다. 에러는 하위 영역에서 발생하고, 그로 인해 상위 영역의 작업이 중지됩니다. 그리고 상위 영역에서 해당 에러를 이유로 컨텍스트를 취소 하게 되면, 모든 하위 영역에서 어떤 에러로 작업이 중지 되었는지 알 수 있게 됩니다.\n\n훌륭하게 해당 작업 영역 전체에 걸쳐 에러를 전파할 수 있는 것입니다!\n\n### withoutCancel\n\n`cancelCtx` 형태는 상위 영역에서의 작업이 취소되면, 이후에 작업을 계속 수행할 필요 없는 하위 영역에서의 작업들이 일괄적으로 종료되어 하드웨어 자원을 절약할 수 있습니다.\n\n하지만 상위 영역의 작업이 취소되어도, 하위 영역에서 무조건 작업을 완료해야 하는 경우가 존재합니다.  \n그 경우에는 `Done()` 메서드를 호출하지 않고 채널에서 값을 안 받아와도 됩니다. 하지만 하위 영역에 상위 영역의 컨텍스트를 상속 받으면서, 취소 시그널은 무시해야 하는 경우엔 문제가 될 수 있습니다.\n\n그럴 때 사용할 수 있는 컨텍스트가 `withoutCancel` 컨텍스트입니다.  \n해당 컨텍스트는 `WithoutCancel` 함수(`func WithoutCancel(parent Context) Context`)로 생성할 수 있습니다.\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"time\"\n)\n\nfunc main() {\n    ctx, cancel := context.WithCancel(context.Background())\n    ctx = context.WithoutCancel(ctx)\n\n    go func() {\n        <-ctx.Done()\n        println(\"context canceled\")\n    }()\n\n    cancel()\n    time.Sleep(1 * time.Second)\n}\n```\n\n위 예제는 `context`를 생성하고, `context`가 취소되면 `context canceled`를 출력하는 예제입니다. 하지만 `context`가 취소되어도 `context canceled`를 출력하지 않습니다. `WithoutCancel` 함수로 생성된 `ctx`는 `Done()` 메서드를 호출해도 `nil` 채널을 받아와서 `cancel` 함수가 호출되어도 영향을 받지 않습니다.\n\n이 상태로 `ctx`를 상속 받은 하위 영역에서 작업을 수행하면, 기존 상위 컨텍스트가 취소되어도 하위 영역의 작업은 별도의 작업으로 분리되어 계속 수행됩니다.\n\n### withDeadline, withTimeout\n\n`timerCtx`는 다음 4가지 함수로 생성할 수 있습니다.\n\n1. `WithDeadlineCause(parent Context, d time.Time, cause error) (Context, CancelFunc)`  \n   1. `WithDeadlineCause` 함수가 이 중 가장 베이스가 되는 함수입니다.  \n   2. 입력받은 컨텍스트를 상속받아서, `d` 시각이 되면 `ctx`를 입력받은 이유(`cause`)로 취소하는 `timerCtx`를 생성합니다.  \n   3. `context.Cause`를 통해 해당 이유를 조회할 수 있습니다.\n2. `WithDeadline(parent Context, d time.Time) (Context, CancelFunc)`\n   1. `WithDeadline` 함수는 위 `WithDeadlineCause` 함수를 호출하고, `cause`를 `nil`로 전달하는 함수입니다.  \n   2. 그래서 `context.Cause`로 취소 사유를 조회할 수 없습니다.\n3. `WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)`\n   1. `WithTimeout`은 `WithDeadline` 함수를 내부적으로 호출합니다.\n   2. `d`의 값으로 현재 시각(`time.Now()`)에 `timeout` 값을 더한 시각을 전달합니다.\n4. `WithTimeoutCause(parent Context, timeout time.Duration, cause error) (Context, CancelFunc)`\n   1. `WithTimeoutCause` 함수는 `WithTimeout` 함수처럼 시각을 계산해서 `WithDeadlineCause`의 `d` 매개변수로 넘겨주고, `cause`를 그대로 넘겨줍니다.\n\n이러한 `timerCtx`들은 특정 시점까지, 혹은 특정 시간 동안 작업을 수행해야하고 그 외에는 장애일 때 사용하게 됩니다.\n\n### withValue\n\n`valueCtx`는 `WithValue` 함수(`func WithValue(parent Context, key, val interface{}) Context`)로 생성할 수 있습니다.  \n그리고 저장된 값은 `context.Value` 함수(`func Value(key interface{}) interface{}`)로 조회할 수 있습니다.\n\n이 컨텍스트는 다음과같이 정의되어 있습니다.\n\n```go\ntype valueCtx struct {\n\tContext\n\tkey, val any\n}\n```\n\n대충 눈치채셨겠지만, `valueCtx` 하나 당 하나의 key-value 값을 가질 수 있습니다.  \n여러 값을 추가할 때, `WithValue`를 여러번 중첩해서 호출하여 `valueCtx`를 스택처럼 쌓아서 추가하게 됩니다.\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n)\n\ntype Key1 string\ntype Key2 string\ntype Key3 string\n\nfunc main() {\n    ctx := context.Background()\n    ctx = context.WithValue(ctx, Key1(\"key1\"), \"value1\")\n    ctx = context.WithValue(ctx, Key2(\"key2\"), \"value2\")\n    ctx = context.WithValue(ctx, Key3(\"key3\"), \"value3\")\n\n    fmt.Println(ctx.Value(Key1(\"key1\")))\n    fmt.Println(ctx.Value(Key2(\"key2\")))\n    fmt.Println(ctx.Value(Key3(\"key3\")))\n}\n```\n\n위 예제는 `context`에 `key1`, `key2`, `key3`를 각각 `value1`, `value2`, `value3`로 저장하고, 조회하는 예제입니다.  \n최종적으로 `ctx` 값은 `background` -> `valueCtx` -> `valueCtx` -> `valueCtx` 순으로 스택처럼 쌓이게 됩니다.  \n그리고 `ctx.Value` 함수는 마지막 `valueCtx`부터 역순으로 `key`를 조회하면서 값을 찾습니다.\n\n찾는 과정에서 `ctx.Value` 함수는 `key`의 값이 일치하는지 확인합니다.  \n하지만 고는 엄격한 강타입 언어이자, 정적타입 언어이기에 단순 비교(`==`)에 있어, 값 뿐만 아니라 타입까지 일치해야 합니다.  \n위 예시를 이렇게 수정해도 동일하게 동작합니다.\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"context\"\n)\n\ntype Key1 string\ntype Key2 string\ntype Key3 string\n\nfunc main() {\n    ctx := context.Background()\n    ctx = context.WithValue(ctx, Key1(\"key\"), \"value1\")\n    ctx = context.WithValue(ctx, Key2(\"key\"), \"value2\")\n    ctx = context.WithValue(ctx, Key3(\"key\"), \"value3\")\n\n    fmt.Println(ctx.Value(Key1(\"key\")))\n    fmt.Println(ctx.Value(Key2(\"key\")))\n    fmt.Println(ctx.Value(Key3(\"key\")))\n}\n```\n\n그래서 컨텍스트에 값을 저장할 때는, 서로 다른 타입을 명시적으로 만들어 주는 것을 권장합니다.  \n또한 연어처럼 거슬러 올라가서 일일이 대조하는 방식이기 때문에, `valueCtx`를 쌓을 때 여유를 두는 것이 좋습니다.\n\n### afterFunc\n\n`afterFuncCtx`는 `func AfterFunc(ctx Context, f func()) (stop func() bool)` 함수로 생성할 수 있습니다.  \n`AfterFunc` 함수는 `ctx` 컨텍스트가 취소되면 `f` 함수를 호출합니다.  \n호출 사유는 `cancelFunc` 호출이나, `deadline` 도달 등이 있습니다.\n\n이러한 이유로 컨텍스트가 중지 되었을 경우 딱 한번 자동으로 실행됩니다.  \n물론 명시적으로 `stop` 함수를 호출하여 실행시킬 수 있습니다.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"sync\"\n)\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\n\twg := &sync.WaitGroup{}\n\twg.Add(1)\n\tcontext.AfterFunc(ctx, func() {\n\t\tlog.Println(\"AfterFunc\")\n\t\twg.Done()\n\t})\n\n\tlog.Println(\"canceling context\")\n\tcancel()\n\tlog.Println(\"canceled context\")\n\twg.Wait()\n}\n```\n\n위 예제는 `context`가 취소되면 `AfterFunc` 함수를 호출하는 예제입니다.  \n`context`가 취소되면 `AfterFunc` 함수가 호출되고, `wg.Done` 함수가 호출되어 `wg.Wait` 함수가 종료됩니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/018_context/", "created_date": "2023-05-26T14:14:06Z"}
{"title": "Context 패키지 (실전)", "content": "## 기능 복기\n\n`TODO` 컨텍스트의 경우에 특이 케이스이므로 여기에 포함시키지 않겠습니다.\n\n### 컨텍스트 생성\n\n```go\npackage main\n\nimport \"context\"\n\nfunc main() {\n\tctx := context.Background()\n}\n```\n\n`context.Background()` 를 통해 새로운 컨텍스트를 생성할 수 있습니다.  \n반드시 이 컨텍스트를 생성할 때는, 모든 작업의 최상단에서 생성해야 합니다.\n\n### 취소 가능한 컨텍스트 생성\n\n1. `context.WithCancel()`\n\n```go\npackage main\n\nimport \"context\"\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n}\n```\n\n`context.WithCancel()` 을 통해 취소 가능한 컨텍스트를 생성할 수 있습니다.  \n이 함수는 새로운 컨텍스트와 취소 함수를 반환합니다.  \n취소 함수를 통해 현재 컨텍스트를 포함한 하위 컨텍스트들에게 취소 시그널을 줄 수 있습니다.  \n시그널은 `ctx.Done()` 메서드에서 반환된 `<-chan struct{}`에서 받을 수 있습니다.\n\n2. `context.WithCancelCause()`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n)\n\nvar errCanceled = errors.New(\"canceled\")\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx, cancel := context.WithCancelCause(ctx)\n\tdefer cancel(errCanceled)\n}\n```\n\n`context.WithCancelCause()` 을 통해 취소 가능한 컨텍스트를 생성할 수 있습니다.  \n이 함수는 새로운 컨텍스트와 취소 함수를 반환합니다.  \n취소 함수를 통해 현재 컨텍스트를 포함한 하위 컨텍스트들에게 취소 시그널과 그 이유를 전파할 수 있습니다.\n\n3. `context.WithTimeout()`, `context.WithDeadline()`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"time\"\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n    // ctx, cancel := context.WithDeadline(ctx, time.Now().Add(5*time.Second))\n\tdefer cancel()\n}\n```\n\n`context.WithTimeout()` 을 통해 시간 제한을 가진 컨텍스트를 생성할 수 있습니다.  \n함수의 동작은 `context.WithDeadline()`과 거의 동일합니다.  \n해당 시간이 지나거나, 해당 시각에 도달할 경우 컨텍스트가 자동으로 취소됩니다.\n\n4. `context.WithTimeoutCause()`, `context.WithDeadlineCause()`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"time\"\n)\n\nvar errTimeout = errors.New(\"timeout\")\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx, cancel := context.WithTimeoutCause(ctx, 5*time.Second, errTimeout)\n\t// ctx, cancel := context.WithDeadlineCause(ctx, time.Now().Add(5*time.Second), errTimeout)\n\tdefer cancel()\n}\n```\n\n`context.WithTimeoutCause()` 을 통해 시간 제한과 에러 메시지를 가진 컨텍스트를 생성할 수 있습니다.  \n함수의 동작은 `context.WithDeadlineCause()`과 거의 동일합니다.\n해당 시간이 지나거나, 해당 시각에 도달할 경우 자동으로 컨텍스트가 취소되며, 에러 메시지를 하위 컨텍스트에 전파합니다.\n\n5. `context.WithoutCancel()`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n)\n\nvar errCanceled = errors.New(\"canceled\")\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx, cancel := context.WithCancelCause(ctx)\n\tdefer cancel(errCanceled) // ignored cancellation\n\n\tctx = context.WithoutCancel(ctx) // // ignore defer cancellation\n}\n```\n\n`context.WithoutCancel()` 을 통해 취소 신호를 무시하는 컨텍스트를 생성할 수 있습니다.  \n이 함수는 상위 컨텍스트의 스택은 유지하면서, 취소 신호만 무시하는 컨텍스트입니다.  \n그래서 `defer cancel()` 을 통해 취소 함수를 호출해도, 취소 신호를 무시합니다.\n\n### 컨텍스트에서 값 다루기\n\n1. `context.WithValue()`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n)\n\ntype CtxKey int\n\nconst (\n\tFirstKey CtxKey = iota\n\tSecondKey\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx = context.WithValue(ctx, FirstKey, \"first value\")\n\tctx = context.WithValue(ctx, SecondKey, \"second value\")\n}\n```\n\n`context.WithValue()` 를 통해 컨텍스트에 값을 추가할 수 있습니다.  \n이 함수는 값을 포함하는 새로운 컨텍스트를 반환합니다.  \n추가하는 키와 값은 `any` 타입이지만 키는 각 패키지와 타입 마다 커스텀 타입을 사용함을 권장합니다.\n\n2. `context.Value()`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n)\n\ntype CtxKey int\n\nconst (\n\tFirstKey CtxKey = iota\n\tSecondKey\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx = context.WithValue(ctx, FirstKey, \"first value\")\n\tctx = context.WithValue(ctx, SecondKey, \"second value\")\n\n\tfmt.Println(ctx.Value(FirstKey))\n\tfmt.Println(ctx.Value(SecondKey))\n}\n```\n\n`context.Value()` 메서드를 통해 컨텍스트에 값을 가져올 수 있습니다.  \n이 메서드는 `any` 타입을 반환하므로 적절한 타입 단언이 필요합니다.  \n그리고 키를 비교할 때, 타입과 값을 동시에 검사하므로 정확하게 일치하는 타입과 값으로 넣어줘야합니다.  \n예를 들어 지금 위의 예시에서 `FirstKey`를 가져오기 위해, `0`을 넣게 되면 값을 가져오지 못합니다.\n\n### 컨텍스트 사후 처리\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"time\"\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\tstop := context.AfterFunc(ctx, func() {\n\t\tlog.Println(\"cancellation function called\")\n\t})\n\n    _ = stop\n    // stop() // cancel AfterFunc function\n\tcancel()\n\ttime.Sleep(1 * time.Second)\n}\n```\n\n`context.AfterFunc()` 을 통해 컨텍스트가 취소될 때, 실행할 함수를 등록할 수 있습니다.  \n이 함수는 자신 포함 상위 컨텍스트에서 오는 취소 신호를 받으면 딱 한번 실행됩니다.  \n`stop()` 함수를 반환하는데, 이 함수를 호출함으로 `AfterFunc()`로 등록한 함수의 실행을 취소할 수 있습니다.  \n마지막으로, 취소 신호에 동작하기 때문에 `WithTimeout()`이나 `WithDeadline()`의 시간 초과에도 동작합니다.\n\n## 문맥 지향\n\n고 프로그램을 하나의 큰 작업으로 본다면, `main` 엔트리 포인트에서 컨텍스트가 생성되어 전체 프로그램에 전파됩니다.\n\n그래서 이제부터 `main` 함수에서 컨텍스트를 생성하고, 하위 작업(단순 유틸리티는 제외입니다. 예를 들어, 문자열을 합친다거나, 정렬한다거나 하는 함수)에 컨텍스트를 무조건 전달한다고 가정하고 작성하겠습니다.\n\n### 작업 관점에서의 문맥\n\n컨텍스트를 전달함으로 특정 작업을 하나의 그룹으로 묶을 수 있습니다.\n\n![01](/img/019/01.svg)\n\n인증 서버와 검색 서버, 업데이트 서버를 각각 하나의 그룹으로 묶고 있습니다. 이때, 메인 함수에서 각 서버를 실행할 때, `WithCancel`을 통해 취소 함수를 생성하고, 각 서버에 전달합니다.\n\n그러면 메인 함수에서 원할 때, 각 서버를 단순히 종료할 수 있으며, 필요에 따라 다시 실행할 수 있습니다, 매우 우아하게 말이죠!\n\n당연히 `Done()` 메서드를 통해 작업이 종료되는 처리만 잘 되어 있다면, `WithTimeout`이나 `WithDeadline`을 통해 타임아웃을 설정할 수도 있습니다.\n\n이런 케이스는 주로 `sql` 패키지나 레디스, `http` 패키지의 요청을 처리하는 등, 외부 IO 작업을 처리할 때 많이 사용됩니다.\n\n### 요청 관점에서의 문맥\n\n컨텍스트가 값을 포함할 수 있고, 값을 탐색하는 방향이 추가의 역순, 즉 스택 형식이기 때문에 해당 요청에 필요한 값을 컨텍스트에 넣고, 하위 작업에 전달할 수 있습니다.\n\n![02](/img/019/02.svg)\n\n`WithValue`를 통해 메인 함수에서 레디스 커넥션 풀과 포스트그레스 커넥션 풀을 컨텍스트에 포함해서 인증 서버에 제공했습니다.\n\n그러면 인증 서버는 서버가 동작하는 동안 각 커넥션 풀에 대해 `key` 외에는 아무 정보도 몰라도 됩니다.\n\n필요에 따라\n1. 인증 토큰을 검사할 때, `TokenCheck` 함수에 컨텍스트를 같이 넘겨주면, 해당 함수가 컨텍스트에서 레디스 커넥션 풀을 `Value()` 메서드를 통해 가져와서 사용할 수 있습니다.\n2. 특정 유저의 정보를 조회하고 싶다면, `GetUserInfo` 함수에 컨텍스트를 같이 넘겨주면, 해당 함수가 컨텍스트에서 포스트그레스 커넥션 풀을 `Value()` 메서드를 통해 가져와서 사용할 수 있습니다.\n\n이런식으로 작성하면, 하나의 컨텍스트에서 싱글톤 인스턴스와 의존성을 관리할 수 있습니다.\n\n이 구조를 적극 활용하면, 객체만을 위한 패키지와 행위만을 위한 패키지를 분리할 수 있습니다.\n\n### 객체 관점에서의 문맥\n\n컨텍스트를 활용하면 효과적으로 특정 객체에 대한 생명 주기를 관리할 수 있습니다. 보통 다른 객체 지향 언어들의 경우에는 소멸자(destructor)를 통해 객체가 소멸할 때, 필요한 작업을 처리합니다.\n\n하지만 고의 경우엔 소멸자가 없기도 하고, `runtime.SetFinalizer`를 사용하기에는 GC 타이밍에 동작하는 특성 때문에, 타이밍 예측이 어렵고 GC에 영향을 준다는 단점이 있습니다.\n\n![03](/img/019/03.svg)\n\n위 UML에서 사용자가 서버에 요청을 하면 사용자 요청에 대한 컨텍스트가 생성됩니다. 그리고 이 컨텍스트는 요청이 수행되는 동안 유지됩니다. 요청이 완료되면 컨텍스트가 자동으로 취소 되게 `defer`를 통해 처리될 것입니다. \n\n여튼, 사용자의 요청을 받아줄 요청 객체와 응답을 담아줄 응답 객체를 각각 `sync.Pool`로 만든 풀에서 가져와서 사용합니다. 그리고 요청이 완료되면 다시 풀에 반환합니다. 컨텍스트가 없었다면, 적절한 위치에서 손으로 풀에 반환하거나 `runtime.SetFinalizer`를 통해 GC에 반환을 맡겨야 했을 것입니다.\n\n그러면 `sync.Pool`을 확장하여 `context.AfterFunc`를 활용해, 작업이 종료되면 자동으로 풀에 반환하는 패키지를 작성해 보겠습니다.\n\n```go\npackage pool\n\nimport (\n\t\"context\"\n\t\"sync\"\n)\n\ntype Pool[T any] struct {\n\tinner sync.Pool\n}\n\nfunc New[T any]() *Pool[T] {\n\treturn &Pool[T]{\n\t\tinner: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\treturn new(T)\n\t\t\t},\n\t\t},\n\t}\n}\n\nfunc (p *Pool[T]) Get(ctx context.Context) *T {\n\tv := p.inner.Get().(*T)\n\tcontext.AfterFunc(ctx, func() {\n\t\tp.inner.Put(v)\n\t})\n\treturn v\n}\n```\n\n위 `pool` 패키지는 `Get` 메서드를 호출할 때 받은 컨텍스트를 기반으로 `AfterFunc`을 통해 컨텍스트가 취소되면 자동으로 풀에 반환하도록 작성되어 있습니다. 이를 통해 풀에 반환하는 작업을 자동화할 수 있습니다.\n\n여기서 작성한 것처럼 단순 풀링만이 아니라, 어떤 객체가 가지는 수명을 특정 컨텍스트에 포함시킴으로 해당 객체에 대한 관리를 더욱 편리하게 할 수 있습니다.\n\n### 예외 관점에서의 문맥\n\n고에서 일반적으로 에러가 발생하면 상향식으로 전파됩니다. 그래서 일반적으로는 에러가 발생한 지점에서부터 에러를 처리하는 상단까지의 경로 밖에서는 에러가 발생한 것이나 정보를 파악할 수 없습니다. 하지만 `context.WithCancelCause`와 `context.Cause`를 사용하면 에러를 처리하는 곳에서부터 하향식으로 에러를 전파할 수 있습니다.\n\n![04](/img/019/04.svg)\n\n위 UML은 메인 함수에서 `context.WithCancelCause`를 통해 루트 컨텍스트를 생성하고, 그 컨텍스트들을 `JobA`, `JobB`에게 전달합니다. 그리고 두 작업은 각각의 하위에 `~1`과 `~2`라는 이름의 작업을 다시 병렬적으로 실행시킨 그림입니다.\n\n작업을 수행하던 중, `JobA1`이 모종의 이유로 에러가 발생했습니다. 발생된 에러는 상향식으로 `JobA`를 거쳐, 메인 함수에 도달하여 `cancel(err)` 함수를 호출하여 컨텍스트를 취소합니다. \n\n그럼 해당 컨텍스트에 에러가 등록되어 상향식으로 에러가 전파된 경로 외에 존재하는 `JobA2`, `JobB`, `JobB1`, `JobB2`가 `Done()`을 받은 후, `context.Cause`를 통해 에러를 확인할 수 있습니다.\n\n이렇게 에러를 전파하게 되면, 병렬적으로 수행되던 작업이 어떤 이유로 인해 중단되었는지를 파악하여 디버깅을 더욱 효율적으로 할 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/019_context_ex/", "created_date": "2023-05-26T18:47:21Z"}
{"title": "라이프타임에 대한 고찰", "content": "## 개요\n\n이전 포스팅과 동일한 프로젝트에 대한 회고를 내용으로 합니다. 이 포스트에선 비교적 안일하게 생각했던 라이프타임에 대해 생각했던 걸 적어보려합니다. 여기서 말하는 라이프타임은 객체의 라이프사이클, 캐시 수명, 요청의 타임아웃 등을 포함하는 어떤 것의 발생과 소멸까지의 시간을 의미합니다.\n\n### 뭐가 문제였지?\n\n대부분의 라이프타임 관리는 괜찮게 했습니다. 하지만 몇몇 경우에 대해서 시스템을 맹신한 나머지 안일하게 관리를 했고, 관련 부분에 대한 내용이 주가 될 것같습니다.\n\n## 본문\n\n해당 프로젝트가 Go로 구현된 만큼 `context` 패키지를 주로 활용하였고, `context` 패키지에 대해 처음 보신다면, 제가 이전에 작성한 [포스트](https://snowmerak.pages.dev/posts/018_context/)를 같이 읽으시면 좋아 보입니다.\n\n### 제대로 한 것들은?\n\n#### 캐시 TTL\n\n모든 캐시 데이터는 Redis를 통해 구성되었습니다. 그리고 기본적으로 프로젝트 내에서 데이터를 생산하는 서비스와 데이터를 소비하는 서비스가 분리되어 있습니다. 이 때문에 `producer` -> `redis` -> `consumer` 흐름을 서비스 간에 명확하게 구성할 수 있었습니다. 여기서 저는 레디스 클라이언트로 `rueidis`를 선택했고, Redis의 Server Assisted Client Side Cache를 적극적으로 활용할 수 있게 되었습니다. 제가 직접 `tracking`을 할 필요가 없어졌기 때문이죠.\n\n그래서 결과적으로 캐시 관련 TTL은 2가지가 발생합니다.\n\n1. Redis 키에 걸리는 TTL: 각 데이터의 특성에 맞게 적절한 TTL을 지정합니다. 주의할 점은 TTL 수치가 작업이 의도치 않게 종료된 후에 다음 작업 시작 가능 시점을 합리적으로 유추할 수 있는 값으로 해야합니다. 예를 들어, 분산 락을 구현한다고 가정합니다. 그러면 초기에 5분의 TTL을 설정하면, 중간에 작업을 수행 중인 시스템이 다운되어도 최대 5분 뒤에는 다른 시스템이 다시 수행할 수 있습니다. 하지만 TTL을 갱신하지 않으면 분산 락 설정 5분 후에 다른 시스템이 중복으로 작업을 수행할 수 있으니, TTL의 75~80% 쯔음 지나갔을 때 TTL을 갱신할 수 있도록 합니다.\n2. Client Side Cache에 걸리는 TTL: 해당 키가 Redis에서 할당받은 TTL보다 최대 10%까지 짧게 작성합니다. 자주 데이터가 갱신될 것같은지에 따라 TTL의 값을 조절합니다. 그래도 일반적으로 모든게 정상적으로 돌아간다면, Redis의 TTL이 만료되거나 데이터가 갱신되면, Client Side Cache가 만료되기 때문에 우선 순위가 높은 계산은 아닙니다.\n\n#### 객체 생명주기\n\n고 언어의 `context.Context`는 `func AfterFunc(ctx Context, f func()) (stop func() bool)` 함수가 존재합니다. 이 함수는 해당 컨텍스트가 만료 되었을때 동작합니다. 이 만료에는 명시적인 `cancel()` 함수 호출, timeout 도달 등이 포함됩니다. \n\n이 기능을 이용해서 거의 대부분의 객체는 생성될 때, 최소 하나의 `context.Context`를 받도록 합니다. 그리고 생성 후 `Close()`나 `Disconnect()` 등을 호출해야 하는 객체면 `AfterFunc` 함수를 호출하여 컨텍스트가 만료될 때, 객체가 소멸되도록 설정합니다. 개인적으로 `runtime.SetFinalizer` 함수를 큰 코스트 없이 대체할 수 있으며, 컨텍스트 단위로 논리적 스코프를 명시적으로 분리할 수 있다는 것에 큰 의의가 있다 생각합니다.\n\n#### API 호출에 대한 timeout\n\n1. http 요청에 대해선 `http.Client` 생성 시 timeout 설정을 이용합니다.\n2. gRPC 요청에 대해선 `grpc.WithTimeout` 옵션과 `context.WithTimeout` 함수를 사용합니다.\n3. 보편적으로 `context.WithTimeout`을 적극적으로 활용하여, 해당 논리적 스코프에 전역 타임아웃을 걸어 사용합니다.\n\n### 제대로 하지 못한 것들은?\n\n#### 스토리지에 대한 timeout\n\n이전에 작성한 포스트에 나왔던 파일 업로드 부분입니다. 단순하게 서술해서 제가 오브젝트 스토리지를 사용하는 중인데, '업로드가 제때 될 것이다'라는 안일함에 timeout과, 그에 관련된 로그를 전혀 남기지 않았었습니다. 이 부분에서 늦게 업로드 되는 케이스가 수시로 존재했고, 그로 인해 이슈가 되었던 적이 있습니다. 사실상 이로 인해 모든 요청 스코프에 대한 timeout 적용을 고민 했었습니다.\n\n### 그에 대한 대안은?\n\n사실상 모든 논리적 스코프에 `context.Context` 사용 강제와 어느정도의 timeout 수치 적용을 하면 해결할 수 있다고 생각합니다. 그리고 이를 받쳐주기 위해 서비스적으로 각 동작들이 얼마나 빠르게 동작해야하며, 최대 대기 시간이 얼마나 되는 지 등을 세세하게 파악해야할 필요성이 있습니다. 물론 설계 단계에서 이게 되지 않았다는 것에 회의를 느끼기도 합니다.\n\n## 결론\n\n모든 작업에 대한 스코프 적용은 사실 `context.Context`만 잘 사용하면 문제 없이 할 수 있습니다. 모두가 당연히 첫 인자로 `ctx context.Context`를 받고 그에 대한 처리만 빈틈없이 잘 해줄 수 있다면, 사실 문제 없어야 하는 게 맞다고 생각합니다. 하지만 이 글을 쓰는 저도 실수를 통해 더 엄격하게 해야겠다고 느껴 코드 스타일 상으로 강제할 수 있는 방법을 찾아야 했습니다.\n\n그 중 간단한 방법은 linter를 사용하는 것입니다. 하지만 이는 상황에 따라 내가 작성하지 않았거나, 체크하지 않는 게 더 나은 경우에 대해 예외 처리를 하는 게 번거로울 수 있으며, 이것 또한 새로운 스트레스가 될 수 있다는 가능성이 존재합니다. 그렇기에 저는 라이브러리 사용이라는 코드 상의 강제성을 부여할 수 있는 방법을 선택하는 게 어떤가 생각합니다.\n\n### 오랜만에..\n\n제가 작성한 [scope](https://github.com/snowmerak/scope)를 이용하면, 전통적인 `try ... catch ... finally`의 구조를 적용합니다. 어떠한 상태를 만들고, 해당 상태 중심으로 가공, 활용 및 재설정하는 일련의 흐름을 함수로써 작성하여 흐름대로 처리할 수 있게 합니다.\n\n#### example\n\n```go\n// riddle.go\npackage riddle\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/snowmerak/scope\"\n\t\"log\"\n\t\"sync/atomic\"\n)\n\ntype State struct {\n\tnumber atomic.Int64\n}\n\nfunc NewState() *State {\n\treturn &State{}\n}\n\nvar _ scope.Work[State] = AddOne\n\nfunc AddOne(ctx context.Context, state *State) error {\n\tstate.number.Add(1)\n\treturn nil\n}\n\nvar _ scope.Work[State] = AddTwo\n\nfunc AddTwo(ctx context.Context, state *State) error {\n\tstate.number.Add(2)\n\treturn nil\n}\n\nvar _ scope.Work[State] = SubOne\n\nfunc SubOne(ctx context.Context, state *State) error {\n\tstate.number.Add(-1)\n\treturn nil\n}\n\nvar _ scope.Work[State] = SetZero\n\nfunc SetZero(ctx context.Context, state *State) error {\n\tstate.number.Store(0)\n\treturn nil\n}\n\nvar _ scope.CleanUp[State] = ResetAndPrint\n\nfunc ResetAndPrint(ctx context.Context, state *State) {\n\tlog.Printf(\"State: %d\\n\", state.number.Load())\n\tstate.number.Store(0)\n}\n\nvar _ scope.CleanUp[State] = JustPrint\n\nfunc JustPrint(ctx context.Context, state *State) {\n\tfmt.Printf(\"State: %d\\n\", state.number.Load())\n}\n\nvar _ scope.Checker[State] = SimpleChecker\n\nfunc SimpleChecker(err error, state *State) {\n\tlog.Printf(\"SimpleChecker: %v\\n\", err)\n\tstate.number.Store(0)\n}\n```\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"github.com/snowmerak/scope\"\n\t\"github.com/snowmerak/scope/prac/riddle\"\n)\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tr := riddle.NewState()\n\n\tscope.Sequence(ctx, r, riddle.SimpleChecker, riddle.JustPrint, riddle.AddOne, riddle.AddTwo, riddle.SubOne)\n\n\tscope.Sequence(ctx, r, riddle.SimpleChecker, riddle.ResetAndPrint, riddle.AddOne, riddle.AddTwo, riddle.SubOne)\n\n\tscope.Sequence(ctx, r, riddle.SimpleChecker, riddle.JustPrint, riddle.AddOne, riddle.AddTwo, riddle.SubOne)\n}\n```\n\n```shell\nState: 2\n2024/07/28 16:17:28 State: 4\nState: 2\n```", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/033_work_timeout/", "created_date": "2024-07-27T23:59:19Z"}
{"title": "빠르게 메시지를 전파하고 싶어", "content": "## 배경\n\n### NATS는 정말 아름다워\n\nNATS는 Go 언어로 작성된 메시지큐입니다. 그리고 제가 개인적으로 좋아하는 오픈소스 프로젝트이기도 합니다. 한때 NATS에서 서브스크립션이 메시지를 받는 방식이 단순 채널이 아닌 독특한 자료구조가 따로 있는 걸 봤었던 적이 있습니다. 이 자료구조는 연결리스트를 통해 특정 서브스크립션에 전달될 메시지를 저장합니다. 그리고 `sync.Cond`를 이용하여 별도의 대기 중인 서브스크립션을 위한 고루틴에게 알려줍니다.\n\n1. 서브스크립션은 생성될 때, 별도 고루틴에서 `sync.Cond`의 `Wait()` 메서드를 통해 메시지가 추가되길 대기합니다.\n2. 커넥션은 외부 연결을 통해 서브스크립션에 대한 메시지를 받아옵니다.\n3. 커넥션은 해당 서브스크립션의 연결리스트에 메시지를 추가하고, `sync.Cond`의 `Signal()` 메서드로 해당 고루틴을 깨웁니다.\n4. 서브스크립션은 연결리스트를 순회해서 메시지를 읽어 처리합니다.\n\n### 쓰는 사람 따로, 읽는 사람 따로\n\n이 구조는 쓰는 주체가 가지는 상태와 읽는 주체가 가지는 상태가 별도로 존재한다는 것에 의의를 느꼈습니다. 물론 `channel`도 버퍼를 주게 되면 상태를 따로 가질 수 있습니다만, 기본적으로 내부에서 락을 가지는 구조이며 반쯤 동기식으로 동작하도록 코드가 작성됩니다. 그리고 무엇보다 채널은 팬아웃(fan out)에 적합한 구조가 아닙니다.\n\n이는 Go의 채널이 CSP를 따르기 때문에 발생한 어쩔 수 없는 구조라고도 생각합니다. 그래서 팬아웃을 빠르게 할 수 있는 패키지를 만들어 보고 싶었습니다. 물론 이 이유 뿐만 아니라, NATS가 가지는 어떠한 동작 때문에 비효율적일 수 있는 동작을 해소하기 위해 팬아웃을 지원하는 패키지가 필요합니다. \n\n## 구현\n\n### 아이디어\n\n그래서 제가 고안한 방법은 다음과 같았습니다.\n\n1. 버퍼는 연결리스트로 구성하고, 데이터가 추가될 때마다 꼬리에 추가하고, `sync.Cond`의 `Broadcast()` 메서드를 실행하여 대기 중인 구독자들을 깨웁니다.\n2. 깨어난 구독자 고루틴들은 연결리스트에서 데이터를 읽어서 함수를 실행합니다.\n3. 만약 실행할 때, 최대 원소 수를 넘으면 최대 원소 수에 도달할 때까지 앞 부분부터 삭제합니다.\n\n### 구현\n\n```go\ntype FakeLock struct{}\n\nfunc (fl FakeLock) Lock() {}\n\nfunc (fl FakeLock) Unlock() {}\n```\n\n코드는 완성된 걸 베이스로 작성하겠습니다. 먼저 이 코드에선 `FakeLock`이라는 실제로는 크리티컬 섹션을 관리하지 않는 가짜 라커를 사용합니다. 이 구조를 통해 `sync.Cond`의 `Wait()`을 호출할 때, 거의 즉시 호출하는 효과를 가져올 것입니다.\n\n```go\ntype Node[T any] struct {\n\tValue  T\n\tNext   *Node[T]\n\tClosed bool\n}\n\ntype AsyncLinkedList[T any] struct {\n\thead               atomic.Pointer[Node[T]]\n\ttail               atomic.Pointer[Node[T]]\n\tbufferLength       atomic.Int64\n\tmaxBuf             int64\n\tlock               sync.Locker\n\tcond               *sync.Cond\n\twheel              *timingwheel.TimingWheel\n\tlatestNotification time.Time\n}\n```\n\n그리고 `Node[T]`와 `AsyncLinkedList[T]`를 선언합니다.\n\n1. 노드의 `Next` 필드는 `atomic.Pointer`로 둘 수 있으나, 일반적으로 CPU가 연산하는 데에 있어, 포인터 하나 크기만큼은 atomic에 가깝게 비교할 거라는 고려가 있었습니다.\n2. 연결리스트의 `head`와 `tail`은 그럼에도 `atomic.Pointer`로 처리했습니다. 이는 추후 CAS를 통해 추가 삭제를 관리하기 위함입니다.\n\n```go\nfunc (all *AsyncLinkedList[T]) getNode() *Node[T] {\n\treturn new(Node[T])\n}\n\nfunc (all *AsyncLinkedList[T]) putNode(_ *Node[T]) {\n}\n```\n\n이는 중요한 코드는 아니나, 갑자기 등장하는 걸로 보일 수 있어 작성합니다.\n\n1. `getNode`는 새로운 노드를 만드는 메서드입니다.\n2. `putNode`는 노드를 반환하는 메서드입니다.\n\n이 두 메서드는 `sync.Pool`같은 오브젝트 풀을 사용할 수 있는 아이디어가 생기면, 적용하기 위해 작성해놓았습니다.\n\n```go\nfunc New[T any](maxBufferSize int64) *AsyncLinkedList[T] {\n\tl := FakeLock{}\n\ttw := timingwheel.NewTimingWheel(50*time.Millisecond, 60)\n\tall := &AsyncLinkedList[T]{\n\t\tmaxBuf: maxBufferSize,\n\t\tlock:   l,\n\t\tcond:   sync.NewCond(l),\n\t\twheel:  tw,\n\t}\n\ttw.ScheduleFunc(all, func() {\n\t\tif all.latestNotification.Add(10 * time.Millisecond).Before(time.Now()) {\n\t\t\tall.latestNotification = time.Now()\n\t\t\tall.cond.Broadcast()\n\t\t}\n\t})\n\ttw.Start()\n\treturn all\n}\n```\n\n새로운 연결리스트를 생성하는 `New` 함수는 한가지 특이점이 있습니다. `timingwheel` 패키지를 사용하여, 10ms마다 구독자 고루틴들을 일괄적으로 깨우고 있는 것입니다. 이에 대해선 설명에 필요한 더 많은 코드가 나왔을 때, 후술하도록 하겠습니다.\n\n```go\nfunc (all *AsyncLinkedList[T]) Push(value T) {\n\tall.push(value, false)\n}\n\nfunc (all *AsyncLinkedList[T]) Close() {\n\tall.push(*new(T), true)\n}\n\nfunc (all *AsyncLinkedList[T]) push(value T, closed bool) {\n\tnode := all.getNode()\n\tnode.Value = value\n\tnode.Closed = closed\n\nouter:\n\tfor {\n\t\ttail := all.tail.Load()\n\t\tif all.tail.CompareAndSwap(tail, node) {\n\t\t\tswitch tail {\n\t\t\tcase nil:\n\t\t\t\tall.head.Store(node)\n\t\t\tdefault:\n\t\t\t\ttail.Next = node\n\t\t\t}\n\t\t\tall.latestNotification = time.Now()\n\t\t\tall.cond.Broadcast()\n\t\t\tif all.bufferLength.Add(1) > all.maxBuf {\n\t\t\t\tall.bufferLength.Add(-1)\n\t\t\tinner:\n\t\t\t\tfor {\n\t\t\t\t\thead := all.head.Load()\n\t\t\t\t\tif all.head.CompareAndSwap(head, head.Next) {\n\t\t\t\t\t\tall.putNode(head)\n\t\t\t\t\t\tbreak inner\n\t\t\t\t\t}\n\t\t\t\t\tfmt.Println(\"failed to swap head\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak outer\n\t\t}\n\t}\n}\n```\n\n`push` 메서드는 `Push`와 `Close` 메서드로 확장됩니다. 이 중 베이스가 되는 `push`는 단순한 형태의 CAS 기반의 연결리스트로 구현됩니다. 한가지 특이한 것은 수가 넘어가서 `head`를 제거하는 코드가 `push`에서 syncronize하게 수행되는 점입니다. 매우 많은 입력이 동시에 일어나면 부하가 될 가능성이 있어 보이지만, 실제 유스케이스에서는 그렇게 동작될 가능성은 낮아보입니다.\n\n```go\nfunc (all *AsyncLinkedList[T]) Subscribe(callback func(value T, closed bool)) <-chan struct{} {\n\tcursor := all.head.Load()\n\tlast := (*Node[T])(nil)\n\tclosed := make(chan struct{}, 1)\n\n\tgo func() {\n\t\tfor {\n\t\t\tall.lock.Lock()\n\t\t\tall.cond.Wait()\n\t\t\tall.lock.Unlock()\n\n\t\t\tswitch last {\n\t\t\tcase nil:\n\t\t\t\tcursor = all.head.Load()\n\t\t\tdefault:\n\t\t\t\tcursor = last.Next\n\t\t\t}\n\n\t\t\tfor cursor != nil {\n\t\t\t\tlast = cursor\n\t\t\t\tcallback(cursor.Value, cursor.Closed)\n\t\t\t\tswitch cursor.Closed {\n\t\t\t\tcase true:\n\t\t\t\t\tclose(closed)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tcursor = cursor.Next\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn closed\n}\n```\n\n마지막으로 `Subscribe` 메서드입니다. 해당 메서드는 콜백 함수를 받아서, 연결리스트가 갱신될 때마다 실행합니다.\n\n1. `cursor`는 한 사이클에서 임시로 쓰이는 임시 변수입니다. 가장 마지막으로 읽었던 노드의 다음 노드, 만약 처음 읽는다면 `head` 노드부터 읽기 시작합니다.\n2. 순회를 하다가 `cursor`가 `nil`을 맞이하기 전 가장 마지막으로 실행된 노드를 `last`에 저장합니다.\n3. 마지막으로 사이클을 시작하기 전, `sync.Cond`를 통해 연결리스트에 노드가 추가되는 것을 대기합니다.\n\n이 메서드 흐름에는 한가지 공백이 존재합니다. 연결리스트에 데이터가 추가될 때, 만약 구독자 고루틴은 사이클을 실행하고 있다면 상황은 확률적으로 변합니다.\n\n1. for loop를 돌고 있다면, 높은 확률로 새로 추가된 노드도 읽어서 수행할 것입니다. 낮은 확률로 아래 케이스와 합쳐집니다.\n2. for loop을 돌고 `sync.Cond`가 `Wait()` 메서드를 호출하기 전의 짧은 구간이라면, 새로 추가된 노드를 인지하지 못합니다.\n\n이 가능성에 의해 저는 앞서 `timingwheel`을 통해 10ms마다 구독자 고루틴을 깨워서 새로운 노드의 추가 여부를 인지시킵니다.  \n이 부분은 분명한 알고리즘 상 헛점이고 하드웨어 리소스를 낭비하는 요소가 됩니다. 차후, 분명히 인지시킬 수 있는 방법을 고안하여 적용해야할 것입니다.\n\n### 벤치마크\n\n이 패키지는 여러 채널을 이용하여 팬아웃하는 구조에 대응하여 만들어졌습니다. 그에 따라 벤치마크를 위한 `_test` 패키지를 이렇게 구성했습니다.\n\n```go\npackage all_test\n\nimport (\n\t\"sync\"\n\t\"testing\"\n\n\t\"github.com/snowmerak/all\"\n)\n\nconst (\n\tSubscribers = 1000\n\tMessages    = 10000\n\tBuffer      = 100\n)\n\nfunc BenchmarkAllLinkedList(b *testing.B) {\n\tll := all.New[[]byte](Buffer)\n\n\twg := sync.WaitGroup{}\n\tfor i := 0; i < Subscribers; i++ {\n\t\twg.Add(1)\n\t\tll.Subscribe(func(value []byte, closed bool) {\n\t\t\tif closed {\n\t\t\t\twg.Done()\n\t\t\t}\n\n\t\t\tmesg := value\n\t\t\t_ = mesg\n\t\t})\n\t}\n\n\tfor i := 0; i < Messages; i++ {\n\t\tll.Push([]byte(\"hello\"))\n\t}\n\tll.Close()\n\n\twg.Wait()\n}\n\nfunc BenchmarkChannel(b *testing.B) {\n\tchList := make([]chan []byte, Subscribers)\n\n\twg := sync.WaitGroup{}\n\tfor i := 0; i < Subscribers; i++ {\n\t\twg.Add(1)\n\t\tchList[i] = make(chan []byte, Buffer)\n\t\tgo func(ch chan []byte) {\n\t\t\tdefer wg.Done()\n\t\t\tfor mesg := range ch {\n\t\t\t\t_ = mesg\n\t\t\t}\n\t\t}(chList[i])\n\t}\n\n\tfor i := 0; i < Messages; i++ {\n\t\tfor j := 0; j < Subscribers; j++ {\n\t\t\tchList[j] <- []byte(\"hello\")\n\t\t}\n\t}\n\tfor i := 0; i < Subscribers; i++ {\n\t\tclose(chList[i])\n\t}\n\n\twg.Wait()\n}\n```\n\n간단하게 1000개의 구독자에게 10000개의 데이터를 전송하는 코드입니다. 각 버퍼(연결리스트 최대 길이, 각 구독자의 채널 버퍼 크기)는 100입니다.\n\n1. `BenchmarkAllLinkedList`는 제가 기 작성한 `AsyncLinkedList`로 전파하는 벤치마크 코드입니다.\n2. `BenchmarkChannel`은 각 구독자마다 생성된 채널을 통해 데이터를 전파하는 벤치마크 코드입니다.\n\n이 코드는 제가 가진 기기 중 라데온과 M1 맥에서 실행했습니다.\n\n```sh\ngoos: darwin\ngoarch: arm64\npkg: github.com/snowmerak/all\ncpu: Apple M1\nBenchmarkAllLinkedList-8   \t1000000000\t         0.04875 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkChannel-8         \t       1\t2700985250 ns/op\t56393016 B/op\t10007084 allocs/op\nPASS\nok  \tgithub.com/snowmerak/all\t3.843s\n```\n\n```sh\ngoos: windows\ngoarch: amd64\npkg: github.com/snowmerak/all\ncpu: AMD Ryzen 7 8845HS w/ Radeon 780M Graphics\nBenchmarkAllLinkedList-16       1000000000            0.05583 ns/op          0 B/op          0 allocs/op\nBenchmarkChannel-16                    1   1288117300 ns/op   56304016 B/op   10005694 allocs/op\nPASS\nok     github.com/snowmerak/all   1.913s\n```\n\n물론 구독자 수와 메시지 수가 늘어나면 연결리스트 방식도 비효율적인 벤치마크 결과가 나오게 됩니다. 앞으로 더 효율적으로 데이터 추가를 인지시키는 방식이나, 메모리 재활용 방법을 연구해야겠습니다.\n\n## 외부 링크\n\n- [snowmerak/all](https://github.com/snowmerak/all)\n- [nats-io/nats-server](https://github.com/nats-io/nats-server)", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/037_fan_out/", "created_date": "2024-09-27T23:20:37Z"}
{"title": "로그 정책에 대한 고찰", "content": "## 개요\n\n최근 그래도 규모가 조금 있는 프로젝트를 처음부터 설계하고, 구현 및 운영한 경험이 생겼습니다. 상당히 많은 부분에서 하고 싶은 걸 했고, 부족한 점을 느끼기고 했고, 발전했다고 생각합니다. 하지만 안타깝게도, 기반 시스템(혹은 라이브러리)에 대해서 조직 내에서도 명확히 어떻게 해왔다는 게 없었어서 상당히 아쉬운 결과를 낳게 되었습니다. 지금에 와서 매우 많은 부분에 스며들어 있어서 일일이 교체하기에도 비용이 큰 작업이 되었구요. 그 중 하나가 로그입니다.\n\n### 뭐가 문제?\n\n해당 프로젝트의 로그 라이브러리는 [`zerolog`](https://github.com/rs/zerolog)를 사용하고 있습니다. 그리고 필요한 곳에서, 물론 실제 코드와는 다르지만, 단순하게 `log.Error().Str(\"error\", err).Str(\"ip\", ip).Int(\"port\", port).Msg(\"not expected connection closed\")`처럼 그 자리에서 필요한 필드를 넣고 로그를 남기는 형태입니다.  \n그리고 이벤트 로그에 대한 개념도 필요했습니다. 저에게 이벤트 로그에 대한 개념이 부족 했던 것도 있지만, 과연 '모든 행동에 로그를 찍는 것이, 이벤트 로그를 작성하는 바람직한 방식인가'에 대해서 고찰의 시간이 필요했습니다.\n\n### 그래서 어떻게?\n\n단순하게 요즘 핫한 생성형 AI 중 하나인 google gemini와 대화를 좀 했습니다. 제가 마침 google gemini 구독 중이라..\n\n## 로그란 무엇일까\n\n### 무엇을 남겨야 하는가?\n\n- 필수 정보\n  1. timestamp: 해당 로그가 언제 발생 했는 지, 다른 로그와의 상관 관계는 어떤지 등을 추적하기 위해 필요합니다.\n  2. level: 해당 로그의 심각도 및 성격을 나타냅니다. 주로 다음 몇가지 레벨을 사용합니다.\n     1. debug: 디버그 과정에서 필요한 로그입니다.\n     2. info: 사용자 행동, 요청 등을 추적하기 위해 사용합니다.\n     3. warning: 지금 당장 문제는 되지 않으나, error로 발전할 수 있는 정보를 남길 때 사용합니다.\n     4. error: 일부 함수나 서비스, 혹은 파이프라인이 정상적으로 동작할 수 없는 상황에 사용합니다.\n     5. fatal(혹은 panic): 프로그램이 더 이상 정상적으로 동작할 수 없을 때 사용합니다.\n  3. message: 이 로그가 왜 남아야 하는 지 서술합니다.\n  4. stack trace(혹은 call stack): 함수 호출 순서를 기록해서 어떤 흐름으로 발생했는지 파악할 수 있게 합니다.\n  5. line number: 어떤 파일의 몇번 라인에서 발생했는지 작성하여, 문제 해결을 위한 실마리를 제공합니다.\n\n- 주의 사항\n  1. 개인 정보 보호: 로그에 개인 정보가 남지 않도록 가리거나, 필터링 합니다.\n  2. 출력 레벨 설정: 적절한 레벨을 지정해서 추적이나 통계, 이슈 파악을 하기 용이하게 합니다.\n  3. 저장 용량 관리: 너무 큰 로그를 가지지 않도록, 적절히 압축이나 삭제를 하도록 합니다.\n\n### 언제 남겨야 하는가?\n\n대략적인 예시로, 다음과 같은 상황에 로그를 남길 수 있습니다.\n\n- 예외 발생 시\n  - 프로그램 실행 중 예키지 못한 예외가 발생했을 때\n  - 외부 시스템으로부터 에러 코드를 받았을 때\n  - 프로그램이 비정상적으로 종료되었을 때\n- 중요 이벤트 발생 시\n  - 시스템 시작/종료되었을 때\n  - 설정이 변경되었을 때\n  - 외부 시스템과의 상호작용 및 상태 변화가 발생했을 때\n- 성능 문제 발생 시\n  - 응답이 지연 되었을 때\n  - CPU 사용량이 높을 때\n  - 가용 메모리가 부족할 때\n- 보안 관련 이벤트 발생 시\n  - 인증에 실패했을 때\n  - 중요 데이터가 변경되었을 때\n\n### 왜 남겨야 하나?\n\n- 서비스 운영 중 발생하는 이슈에 대한 알림과 원인 파악 및 빠른 해결에 도움\n- 시스템 안정성 및 성능 향상을 위한 데이터 분석\n- 사고 발생 시 증거 자료 확보 및 추적\n- 시스템 운영 및 관리에 대한 책임 소재 명확화\n\n## 로그 설계\n\n### 이론\n\n이제 로그를 분리할 시간입니다. 저는 나름대로 에러를 2가지로 분류했습니다.\n\n- 이벤트 로그: 시스템 상태 변화, 사용자 행동, 시스템 운영 등 서비스 운영과 관련된 모든 중요 이벤트\n- 시스템 (오류) 로그: 예외 상황(Exception), 에러 코드, 비정상 종료 등 시스템 오류와 관련된 모든 정보\n\n다만 이 로그들에 대해서 몇가지 의문 사항을 제미나이에게 제기했었습니다.  \n모든 이벤트(요청, 상태 변화 성공 및 실패)에 로그를 남기는 건 공간에 비효율적이라 보입니다.\n\n예를 들어, 큰 파일을 사용자가 업로드하고 빠른 시간 안에 제공해야하는 기능이 있다고 가정합시다. 그럼 우리는 어느정도 시간 안에는 올라갈 것이라는 timeout을 잡게 될 것입니다. 그 시간을 넘어가면 실패로 간주하고, 재시도를 요청할 것이구요.\n\n이 과정에서 저희는 아래 2가지 로그를 남기게 될 전망이라 가정합시다.\n\n1. 유저 업로드 요청에 대한 info 로그\n2. timeout이 발생한 error 로그\n\n하지만 timeout에만 남기게 되면, 잠재적으로 사용량이 점점 늘어나 임계치에 가까워져 서서히 성능이 줄어드는 경고를 알아채지 못할 수 있습니다. 그래서 임계치 기반의 로그 두가지를 추가할 수 있습니다.\n\n1. 유저 업로드 요청에 대한 info 로그\n2. timeout이 발생한 error 로그\n3. timeout의 75%만큼 시간이 걸린 업로드에 대한 warn 로그\n4. timeout의 50%만큼 시간이 걸린 업로드에 대한 warn 로그\n\n이 모든 로그를 남길 수도 있습니다만, 몇가지 제안을 추가로 할 수 있습니다.\n\n1. 중요하지 않다면 업로드 요청 데이터와 시간을 기록한 채로 timeout이 발생 했을 때만 error 로그에 포함하여 로깅\n2. 3번과 4번 로그에 대해 집계 로그 형태로 분당, 혹은 시간당 얼마나 발생했는 지와 평균 파일 크기 등을 로깅\n\n만약 전체 요청 수를 info 로그로 확인하고 있었다면, 데이터베이스나 스토리지 등을 활용하여 전체 수를 지정하는 것도 좋은 방법으로 고려됩니다.\n\n### 구성\n\n로그의 종류는 크게 2가지, event log와 system log로 나뉩니다.  \n`kind` 필드를 추가해서 `event`와 `system`을 가지도록 합니다.\n\n```go\ntype Kind string\n\nconst (\n    KindEvent Kind = \"event\"\n    KindSystem Kind = \"system\"\n)\n```\n\n그리고 `type` 필드를 추가해서 어떤 로그인지 구체화 합니다. 방금 나왔던 파일 업로드를 예로 들면 `user_content_upload`같은 형식으로 쓸 수 있어 보입니다. 제미나이는 아래와 같은 타입들을 예제로 만들었습니다.\n\n- user_login\n- user_logout\n- file_upload_success\n- file_upload_failure\n- file_download\n- search_query\n- payment_success\n- payment_failure\n- system_start\n- system_shutdown\n\n### 구현\n\n이 부분 또한 이전 프로젝트를 진행하며 아쉬웠던 부분 중 하나입니다. 좀 더 정규화된 로그 출력 함수나 메서드를 만들어 관리하지 못한 것이 매우 아쉽습니다.\n\n```go\nimport \"github.com/rs/zerolog/log\"\n\nfunc Write(ev *zerolog.Event, timestamp time.Time, kind Kind, typ string) {\n    ev.Time(\"timestamp\", timestmap).Str(\"kind\", string(kind)).Str(\"type\", typ).Send()\n}\n```\n\n먼저 zerolog의 `*zerolog.Event`를 받아서 제공할 함수를 작성합니다.\n\n```go\nfunc WriteFileUploadTimeout(ev *zerolog.Event, requestTime time.Time, requestUser string, requestSession string, duration time.Duration) {\n\tWrite(ev.Time(\"request_time\", requestTime).\n        Str(\"request_user\", requestUser).\n        Str(\"request_session\", requestSession).\n        Dur(\"duration\", duration), time.Now(), log.KindEvent, \"file_upload_timeout\")\n}\n```\n\n그리고 필요한 패러미터를 받아 `*zerolog.Event`에 추가하며, 정해진 값으로 미리정의한 `Write` 함수를 호출하도록 합니다. 그리고 실제 에러를 남길 때는 다음과 같이 호출합니다.\n\n```go\nfunc main() {\n    WriteFileUploadTimeout(log.Error(), time.Now(), \"user\", \"session\", time.Second)\n}\n```\n\n로그 레벨은 최종적으로 호출할 때 설정합니다. 필요하다면 첫번째 인자인 `*zerolog.Event`를 활용하여 함수를 중첩해 나가는 방식으로 로그를 확장할 수 있습니다. 필요하다면, 구조체나 함수형 패러미터를 이용하는 것도 좋은 방법이라 생각합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/032_error_policy/", "created_date": "2024-07-26T18:24:08Z"}
{"title": "전략 패턴에 대해서", "content": "> 행위 패턴의 하나로, `어떤 문제`를 해결함에 `어떤 방법`을 사용하는 지 적절히 선택, 혹은 작성할 수 있게 해주는 패턴\n\n---\n\n## 코틀린으로 평균을 구하는 계산기 만들기\n\n이 계산기의 평균을 구하는 방법은 총 3가지가 있습니다.\n\n1. 산술 평균 : (a + b) / 2\n  \n2. 기하 평균 : root2(a * b)\n  \n3. 조화 평균 : 2ab / (a + b)\n  \n\n이 3가지를 각각 따로 입력 받아 실행하는 `평균만 구하는 계산기`의 코틀린 코드입니다.\n\n```kotlin\nimport kotlin.math.sqrt\n\nfun main(args: Array<String>) {\n    val (a, b) = Pair(30, 80)\n    val arithmeticCalculator = Calculator(ArithmeticMean())\n    println(arithmeticCalculator.average.calculate(a, b))\n\n    val geometricCalculator = Calculator(GeometricMean())\n    println(geometricCalculator.average.calculate(a, b))\n\n    val harmonicCalculator = Calculator(HarmonicMean())\n    println(harmonicCalculator.average.calculate(a, b))\n}\n\nclass Calculator(val average: Average)\n\ninterface Average {\n    fun calculate(a: Int, b: Int): Int\n}\n\nclass ArithmeticMean: Average {\n    override fun calculate(a: Int, b: Int): Int {\n        return (a + b) / 2\n    }\n}\n\nclass GeometricMean: Average {\n    override fun calculate(a: Int, b: Int): Int {\n        return sqrt(a.toDouble() * b.toDouble()).toInt()\n    }\n}\n\nclass HarmonicMean: Average {\n    override fun calculate(a: Int, b: Int): Int {\n        return (2*a*b) / (a + b)\n    }\n}\n```\n\n`ArithmeticMean`, `GeometricMean`, `HarmonicMean` 클래스는 `Average` 인터페이스를 상속 받아 평균을 구하는 메서드를 오버라이드하고 있습니다.\n\n`Calculator` 클래스는 이 `Average` 인터페이스를 입력 받아 평균을 구하는 기능을 가진 계산기를 생성합니다.\n\n메인 함수를 보면 각 계산기 클래스 구현체에서 `average.calculate` 메서드를 호출하여 평균을 계산했을 때 산술, 기하, 조화 평균에 맞는 값을 출력하는 걸 확인할 수 있습니다.\n\n```kotlin\n55\n48\n43\n```\n\n이 예시에서는 코틀린의 인터페이스를 활용하여 각기 다른 메서드 구현을 가지는 클래스를 매개로 전략 패턴을 구현했습니다.\n\n---\n\n## 자바에서 AES 암호화하기\n\n```java\nimport javax.crypto.*;\nimport javax.crypto.spec.IvParameterSpec;\nimport javax.crypto.spec.SecretKeySpec;\nimport java.nio.charset.StandardCharsets;\nimport java.security.InvalidAlgorithmParameterException;\nimport java.security.InvalidKeyException;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\n\nclass Hex {\n    static public String from(byte[] a) {\n        return new java.math.BigInteger(a).toString(16);\n    }\n\n    static public byte[] toBytes(String a) {\n        return new java.math.BigInteger(a, 16).toByteArray();\n    }\n}\n\nclass SHA256 {\n    static public byte[] hash(String source) throws NoSuchAlgorithmException {\n        MessageDigest messageDigest = MessageDigest.getInstance(\"SHA-256\");\n        return messageDigest.digest(source.getBytes(StandardCharsets.UTF_8));\n    }\n}\n```\n\n`Hex` 클래스와 `SHA256` 클래스를 먼저 만듭니다. 전자는 바이트 배열을 헥스 문자열로 바꾸거나 헥스 문자열을 바이트 배열로 바꾸는 역할을 합니다. 후자는 입력받은 문자열을 sha256 알고리즘으로 해싱합니다.\n\n```java\nclass AES {\n    private final Cipher cipher;\n    private final SecretKey key;\n    private final byte[] iv;\n\n    public AES(String algorithm, String password) throws NoSuchAlgorithmException, NoSuchPaddingException {\n        byte[] passwordBytes = SHA256.hash(password);\n\n        KeyGenerator keygen = KeyGenerator.getInstance(\"AES\");\n        keygen.init(256);\n\n        key = new SecretKeySpec(passwordBytes, \"AES\");\n        iv = new byte[16];\n        System.arraycopy(passwordBytes, 0, iv, 0, 16);\n\n        cipher = Cipher.getInstance(algorithm);\n    }\n\n    public String encrypt(String plainText) throws IllegalBlockSizeException, BadPaddingException, InvalidAlgorithmParameterException, InvalidKeyException {\n        cipher.init(Cipher.ENCRYPT_MODE, key, new IvParameterSpec(iv));\n\n        byte[] ciphertext = cipher.doFinal(plainText.getBytes(StandardCharsets.UTF_8));\n\n        return Hex.from(ciphertext);\n    }\n\n    public String decrypt(String cipherText) throws InvalidAlgorithmParameterException, InvalidKeyException, IllegalBlockSizeException, BadPaddingException {\n        cipher.init(Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));\n\n        byte[] decodedText = Hex.toBytes(cipherText);\n        byte[] plainText = cipher.doFinal(decodedText);\n\n        return new String(plainText, StandardCharsets.UTF_8);\n    }\n}\n```\n\n`AES` 클래스는 aes 암호화 알고리즘을 사용하기 위한 간단한 래핑 클래스입니다. 생성자에서 사용할 알고리즘과 비밀번호를 입력받습니다. 입력받은 비밀번호를 기반으로 암호화에 필요한 키 스펙을 생성하고 입력받은 알고리즘을 기반으로 `Cipher` 클래스 구현체를 생성합니다.\n\n이 `Cipher` 클래스 구현체에 따라 서로 다른 방식의 aes 암호화를 진행합니다.\n\n```java\npublic class App {\n    public static void main(String[] args) throws NoSuchPaddingException, NoSuchAlgorithmException, InvalidAlgorithmParameterException, IllegalBlockSizeException, BadPaddingException, InvalidKeyException {\n        var plainText = \"Hello, World!\";\n        System.out.println(\"Plain text: \" + plainText);\n\n        var cbc = new AES(\"AES/CBC/PKCS5PADDING\", \"1q2w3e4r\");\n        var cbcEncrypted = cbc.encrypt(plainText);\n        var cbcDecrypted = cbc.decrypt(cbcEncrypted);\n        System.out.println(\"CBC encrypted: \" + cbcEncrypted);\n        System.out.println(\"CBC decrypted: \" + cbcDecrypted);\n\n        var ctr = new AES(\"AES/CTR/NoPadding\", \"1q2w3e4r\");\n        var ctrEncrypted = ctr.encrypt(plainText);\n        var ctrDecrypted = ctr.decrypt(ctrEncrypted);\n        System.out.println(\"CTR encrypted: \" + ctrEncrypted);\n        System.out.println(\"CTR decrypted: \" + ctrDecrypted);\n    }\n}\n```\n\n메인 메서드에서는 평문인 `Hello, World!`를 각각 aes-cbc와 aes-ctr 방식으로 암호화 및 복호화합니다. `AES` 클래스의 생성자에 입력한 알고리즘이 CBC인지, CTR인지에 따라 서로 다른 방식의 암호화 방식을 취하고 있습니다.\n\n```bash\nPlain text: Hello, World!\nCBC encrypted: -2f0c50ee15e575610bfe998bef09f085\nCBC decrypted: Hello, World!\nCTR encrypted: -24b40ce10f09034538f4cff9f0\nCTR decrypted: Hello, World!\n```\n\n대칭키 암호화 방식인 aes에 같은 비밀번호를 사용했음에도 서로 다른 암호문이 나온걸 확인할 수 있습니다.\n\n팩토리 패턴에도 상충하는 부분이 존재합니다.\n\n---\n\n## 결제 수단 선택하기\n\n```dart\nimport 'dart:io';\n\nabstract class Payment {\n  void pay(int amount);\n}\n\nclass CheckCard implements Payment {\n  late int amount;\n\n  CheckCard(this.amount);\n\n  @override\n  void pay(int amount) {\n    if (amount <= this.amount) {\n      print('Paying $amount using check card');\n      this.amount -= amount;\n      return;\n    } \n    print('Not enough money');\n  }\n}\n\nclass CreditCard implements Payment {\n  @override\n  void pay(int amount) {\n    print('Paying $amount using credit card');\n  }\n}\n\nclass DartPay implements Payment {\n  int points = 0;\n\n  @override\n  void pay(int amount) {\n    if (points > 0) {\n      var paid = points % (amount+1);\n      points -= paid;\n      print('Paying $paid using dart pay\\'s points');\n      amount -= paid;\n    }\n    print('Paying $amount using dart pay');\n    points += (amount * 0.15).toInt();\n    print('You have $points points');\n  }\n}\n```\n\n1. 체크카드: 은행 잔고만큼 결제할 수 있는 카드입니다.\n2. 신용카드: 신용을 담보로 한도까지 결제할 수 있는 카드입니다.\n3. 다트페이: 높은 포인트 적립율을 보여주는 페이 서비스입니다.\n\n```dart\nclass Consumer {\n  late String name;\n  late Payment payment;\n\n  Consumer(this.name, this.payment);\n\n  void changePayment(Payment payment) {\n    this.payment = payment;\n  }\n\n  void pay(int amount) {\n    stdout.write(\"$name: \");\n    payment.pay(amount);\n  }\n}\n```\n\n소비자 클래스는 이름과 결제수단을 가지고 있습니다.\n\n```dart\nvoid main(List<String> args) {\n  var consumer = Consumer(\"merak\", CheckCard(10000));\n\n  consumer.pay(8000); // 2000\n  consumer.pay(8000); // -6000 ?\n  print(\"------------------\");\n\n  consumer.changePayment(CreditCard());\n  consumer.pay(10000); // 마음의 빚\n  print(\"------------------\");\n\n  consumer.changePayment(DartPay());\n  consumer.pay(10000);\n  consumer.pay(2000);\n}\n```\n\n결제 수단을 바꿔가며 결제합니다.\n\n```bash\nmerak: Paying 8000 using check card\nmerak: Not enough money\n------------------\nmerak: Paying 10000 using credit card\n------------------\nmerak: Paying 10000 using dart pay\nYou have 1500 points\nmerak: Paying 1500 using dart pay's points\nPaying 500 using dart pay\nYou have 75 points\n```\n\n선택한 수단에 따라 적절한 결과를 출력합니다.\n\n---\n\n## 특징\n\n1. 서로 다른 알고리즘을 `런타임`에 변경하고 적용할 때 사용합니다.\n  위 3가지 예시 모두 런타임에 어떤 계산 방식, 어떤 암호화 방식, 어떤 버퍼를 쓸지 정할 수 있습니다.\n  \n2. 같은 레이어의 서로 다른 구현체는 철저히 격리되어 있지만 동일한 입출력 타입을 보장합니다.\n  동일한 인터페이스를 구현하여 동일한 동작을 보장합니다.\n  \n3. 각 인터페이스를 구현하는 구현체는 각자의 비즈니스 로직을 가집니다.\n  입출력 타입은 동일하지만 내부 동작이 다르므로 동일한 입력에도 서로 다른 출력을 가집니다.\n  \n4. 느슨한 결합으로 구성되어 새로운 구현부를 추가하기 용이합니다.\n  \n\n---\n\n## 단점\n\n1. 적은 수의 알고리즘만 존재하고 거의 수정되지 않으면 불필요하게 복잡해질 수 있습니다.\n  \n2. 이 패턴으로 만들어진 객체를 이용하는 사람은 알고리즘 간의 차이를 알아야 합니다.\n  어떤 알고리즘을 선택할지는 전적으로 객체를 이용하는 사람이 결정합니다.\n  \n\n---\n\n## FP\n\n프로그래밍 환경이 과거와 달리 OOP로만 작업을 하는 케이스는 거의 없고 선언형, 함수형 프로그래밍의 개념이 일반적인 프로그래밍 환경에도 많이 녹아들었기에 디자인 패턴의 변형도 이제는 생각해야할 시기라고 생각합니다.\n\nOOP의 전략 패턴은 객체를 중심으로 생각하기에 각 알고리즘의 명세를 인터페이스로 정의하고 클래스(혹은 구조체)로 만들어 객체를 생성하여 넘기는게 일반적입니다.\n\n하지만 `서로 다른 구현부를 런타임에 자유롭게 교체하여 사용할 수 있는` 전략 패턴의 의의를 가져온다면 굳이 객체만 넘겨야할 필요는 없다고 생각합니다.\n\n### 해시 알고리즘 선택하기\n\n```go\npackage main\n\nimport (\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\t\"crypto/rand\"\n\t\"crypto/sha256\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"math/big\"\n\n\t\"golang.org/x/crypto/blake2b\"\n\t\"golang.org/x/crypto/blake2s\"\n)\n\ntype Secp521r1 struct {\n\thasher     func([]byte) [32]byte\n\tprivateKey *ecdsa.PrivateKey\n\tpublicKey  *ecdsa.PublicKey\n}\n\nfunc NewSecp521r1(hasher func([]byte) [32]byte) *Secp521r1 {\n\treturn &Secp521r1{\n\t\thasher:     hasher,\n\t\tprivateKey: nil,\n\t\tpublicKey:  nil,\n\t}\n}\n\nfunc (s *Secp521r1) GenerateKey() error {\n\tprivateKey, err := ecdsa.GenerateKey(elliptic.P521(), rand.Reader)\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.privateKey = privateKey\n\ts.publicKey = &privateKey.PublicKey\n\treturn nil\n}\n\nfunc (e *Secp521r1) Sign(key []byte) (*big.Int, *big.Int, error) {\n\tif e.privateKey == nil {\n\t\treturn nil, nil, fmt.Errorf(\"private key is not set\")\n\t}\n\tpass := e.hasher(key)\n\treturn ecdsa.Sign(rand.Reader, e.privateKey, pass[:])\n}\n\nfunc (e *Secp521r1) Verfy(key, data []byte, r, s *big.Int) bool {\n\tif e.publicKey == nil {\n\t\treturn false\n\t}\n\tpass := e.hasher(key)\n\treturn ecdsa.Verify(e.publicKey, pass[:], r, s)\n}\n\nfunc (e *Secp521r1) ExportPrivateKey() ([]byte, error) {\n\treturn x509.MarshalECPrivateKey(e.privateKey)\n}\n\nfunc (e *Secp521r1) ImportPrivateKey(key []byte) error {\n\tpriv, err := x509.ParseECPrivateKey(key)\n\tif err != nil {\n\t\treturn err\n\t}\n\te.privateKey = priv\n\treturn nil\n}\n\nfunc (e *Secp521r1) ExportPublicKey() ([]byte, error) {\n\tif e.publicKey == nil {\n\t\treturn nil, fmt.Errorf(\"public key is not set\")\n\t}\n\treturn elliptic.Marshal(elliptic.P521(), e.publicKey.X, e.publicKey.Y), nil\n}\n\nfunc (e *Secp521r1) ImportPublicKey(key []byte) error {\n\tx, y := elliptic.Unmarshal(elliptic.P521(), key)\n\tif x == nil || y == nil {\n\t\treturn fmt.Errorf(\"invalid public key\")\n\t}\n\tpublicKey := &ecdsa.PublicKey{\n\t\tCurve: elliptic.P521(),\n\t\tX:     x,\n\t\tY:     y,\n\t}\n\te.publicKey = publicKey\n\treturn nil\n}\n\nfunc main() {\n\tpassword := []byte(\"password\")\n\n\tsecp521r1s := []*Secp521r1{NewSecp521r1(sha256.Sum256), NewSecp521r1(blake2b.Sum256), NewSecp521r1(blake2s.Sum256)}\n\tfor _, secp521r1 := range secp521r1s {\n\t\tif err := secp521r1.GenerateKey(); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tkey, err := secp521r1.ExportPrivateKey()\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tfmt.Printf(\"private key: %x\\n\", key)\n\t\tr, s, err := secp521r1.Sign(password)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tfmt.Printf(\"signature: %x %x\\n\", r, s)\n\t\tpub, err := secp521r1.ExportPublicKey()\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tfmt.Printf(\"public key: %x\\n\", pub)\n\t\tif !secp521r1.Verfy(password, []byte(\"hello world\"), r, s) {\n\t\t\tpanic(\"verify failed\")\n\t\t}\n\t\tfmt.Println(\"verified\")\n\t}\n}\n```\n\n```bash\nprivate key: 3081dc0201010442005af683aaa7b3103cc5a893a2d5edcd12c94d6c25c603938fb8db3605d4018d742ee8a83c5b9fd8ceb15f25b2aa57f1c8b78b829bfe8c2a8e3b407027828d384834a00706052b81040023a1818903818600040041a0d4653a1b88f9ad0554c2ec7a549e0ccd2a6c5fb91b420a8ee18d12eafd2e14b34fc4c73af89b49800254e3e90137fc1ac193d0fbfbbb9873a31a533a55a3180147f9c4a35502adc5f9a82fffc6f3ec2738bb54a80407218e6c83af18bc730a5cc4bd3c6db7c3abec90e784d92970ec6693e10c453fd28b6a83295a077918572699\nsignature: 1ca9495865d94e57edcd971c1aaf0cb20009e1f6bad62b304e4eb7267b1fddae24a429a865a52471c095cc4978d8161667a933e6f80b73b24dcd921eed58fca30b1 1dbf3033500a1544d89b7f9669d51d6ff5be404cd2f496fe661ce931bb4e17547b336cb57b0404d9b40aea7d63febb68d6f899105b32b4668a4dc35ec2b522df08\npublic key: 040041a0d4653a1b88f9ad0554c2ec7a549e0ccd2a6c5fb91b420a8ee18d12eafd2e14b34fc4c73af89b49800254e3e90137fc1ac193d0fbfbbb9873a31a533a55a3180147f9c4a35502adc5f9a82fffc6f3ec2738bb54a80407218e6c83af18bc730a5cc4bd3c6db7c3abec90e784d92970ec6693e10c453fd28b6a83295a077918572699\nverified\nprivate key: 3081dc0201010442013f9bd30fb3e9c3b7938b1460afb5dd2b387276a2ad3972d02b207cc17c1a2c936d30def3ff553a82705d1ff6707ca92faa05ebf29151e7cd397e688def7eb986dea00706052b81040023a18189038186000401926686d54c17305d36ce85781afb4b37af3d0c6600c1c2f86966fdf9141fe1bb764abb797d2dc5587756f3bd3b59f24218007d7b26c5c21912d74900578f2803ff01cfb76b3433c47bcacbec67cb1e604ca39df3ac1b71a504b0587f232389d2612a3372296d8f9ddb0ff1fa178eaeb9ac775cc2c21d09efebe5fa28eca6cd81e1af25\nsignature: 47181b97a7593ff886bdb048e28aad3aff6b070c0f092de442566b10f84e3ca44a4f489bb1715e8fa2db8afb47b892173bde915153a1b97bcb9aa24fc8877f4e12 612fbb3ea9eef240ae28763448efa8ccc796d11c235448be14da6a28969d58597aea2f95fcbc6fcef873b7e8f55d7ed23f482fd1a25ed7974a23d79e22a32bcdef\npublic key: 0401926686d54c17305d36ce85781afb4b37af3d0c6600c1c2f86966fdf9141fe1bb764abb797d2dc5587756f3bd3b59f24218007d7b26c5c21912d74900578f2803ff01cfb76b3433c47bcacbec67cb1e604ca39df3ac1b71a504b0587f232389d2612a3372296d8f9ddb0ff1fa178eaeb9ac775cc2c21d09efebe5fa28eca6cd81e1af25\nverified\nprivate key: 3081dc02010104420038c726ef8a57cce983c52f16c9ff9e726567b1b3a7f65b8e18bc4d9330c4556d20fb450f1d1f1ebe4792a38946d90e4d3c6e51930deaf4934f820dc1532c028d58a00706052b81040023a181890381860004001005cdde06ca3f8552237c5a9e120a2e5ee58a1b44d8f4db46e311bd61a67bf00b38912e5daf4ebadfe71959044feb6e6809caffcc125eba28c0bd4fc9c356db93011128764b2fb69cffab80a6970af7c2c4b3f13390905843600e6d722d5089dc0fc4b3408a230d59cf261f3ccc213d034bb043744e33d1eed9cbf41bc5f04afaded2\nsignature: 14531b9b3bd2fe9f41c1cd69f6d2fa29bc59d3a4e91bff256b231af34d910786c094503a726f53174a228cfaaba8b93655014205baefd09307f53c2440a7fc8b2b7 1987d0e1cc57416b8c9ce7ced1e0b760b1f586f9ccc1e3cbc3271cb2615911caef1aa56b40702abb11b6207acead199cd995c4a4debbb6fd838a689f07abece349a\npublic key: 04001005cdde06ca3f8552237c5a9e120a2e5ee58a1b44d8f4db46e311bd61a67bf00b38912e5daf4ebadfe71959044feb6e6809caffcc125eba28c0bd4fc9c356db93011128764b2fb69cffab80a6970af7c2c4b3f13390905843600e6d722d5089dc0fc4b3408a230d59cf261f3ccc213d034bb043744e33d1eed9cbf41bc5f04afaded2\nverified\n```\n\n객체 자체를 넘기기 보다 특정 동작을 하는 함수를 기반으로 전략 패턴을 구현한 케이스입니다.\n\n어떠한 해시 함수(알고리즘 구현부)와 일급 함수 변수가 느슨하게 결합되어 있으며 런타임에 유연하게 교체하여 적용할 수 있습니다. 또한 동일 레이어, 각 해시 함수들은 서로에게 영향을 주지 않고 각자의 로직을 함수 정의에 포함시켰습니다. 확장 또한 새로운 함수를 구현하면 되기에 쉽습니다.\n\n### 단점?\n\n책의 저자는 전략 패턴의 마지막 단점으로 이러한 함수형의 특징 때문에 굳이 OOP의 전략 패턴을 사용하기보다 함수형의 함수 시그니처를 활용하면 클래스 정의와 인터페이스 정의, 패키지 분리 등을 통해 코드가 부풀려지는 걸 방지할 수 있다고 작성했습니다.\n\n하지만 위에서도 썼다시피 OOP의 관점이 아니라 다양한 패러다임에 확장하여 생각해보면 그것 또한 전략 패턴이고 전략 패턴의 특징이라고 생각합니다.\n\n---\n\n## 다른 디자인 패턴과의 관계\n\n### 상태(State)\n\n상태 패턴은 가지고 있는 상태 값의 변경에 의해 다른 행동을 수행하는 패턴입니다. 이러한 특징을 저는 두 가지 이상의 상태에 각각 하나의 전략을 배정하여 특정 상태에 적절한 전략을 사용하는 패턴을 생각했습니다.\n\n```rust\nfn main() {\n    let mut a = Switch::new(Box::new(OnPrinter{}), Box::new(OffPrinter{}));\n    a.click();\n    a = a.inverse();\n    a.click();\n}\n\ntrait Printable {\n    fn print(&self);\n}\n\nstruct OnPrinter;\n\nimpl Printable for OnPrinter {\n    fn print(&self) {\n        println!(\"on!\");\n    }\n}\n\nstruct OffPrinter;\n\nimpl Printable for OffPrinter {\n    fn print(&self) {\n        println!(\"off!\");\n    }\n}\n\nstruct Switch {\n    positive_printable: Box<dyn Printable>,\n    negative_printable: Box<dyn Printable>,\n    state: bool,\n}\n\nimpl Switch {\n    pub fn new(positive_action: Box<dyn Printable>, negative_action: Box<dyn Printable>) -> Switch {\n        Switch {\n            positive_printable: positive_action,\n            negative_printable: negative_action,\n            state: false,\n        }\n    }\n\n    pub fn inverse(self) -> Self {\n        Self {\n            negative_printable: self.negative_printable,\n            positive_printable: self.positive_printable,\n            state: !self.state,\n        }\n    }\n\n    pub fn click(&self) {\n        if self.state {\n            self.positive_printable.print();\n        } else {\n            self.negative_printable.print();\n        }\n    }\n}\n```\n\n간단한 On, Off만 있는 `Switch` 구조체를 만들었습니다.\n\n해당 구조체는 스위치가 On일 때 행동, Off일 때 `Printable` 구현체가 각각 설정되어 있으며 각 상태에 맞춰서 해당 구현체의 메서드를 수행할 것입니다.\n\n```bash\noff!\non!\n```\n\n---\n\n### 커맨드(Command)\n\n```rust\npub trait Command {\n    fn excute(&self);\n}\n\npub struct SaveCommand {\n    name: String,\n}\n\nimpl SaveCommand {\n    pub fn new(name: &str) -> Self {\n        Self {\n            name: name.to_string(),\n        }\n    }\n}\n\nimpl Command for SaveCommand {\n    fn excute(&self) {\n        println!(\"{} save command excuted!\", self.name);\n    }\n}\n\npub struct ExitCommand {\n    name: String,\n}\n\nimpl ExitCommand {\n    pub fn new(name: &str) -> Self {\n        Self {\n            name: name.to_string(),\n        }\n    }\n}\n\nimpl Command for ExitCommand {\n    fn excute(&self) {\n        println!(\"{} exit command excuted!\", self.name);\n    }\n}\n\npub struct OpenCommand {\n    name: String,\n}\n\nimpl OpenCommand {\n    pub fn new(name: &str) -> Self {\n        Self {\n            name: name.to_string(),\n        }\n    }\n}\n\nimpl Command for OpenCommand {\n    fn excute(&self) {\n        println!(\"{} open command excuted!\", self.name);\n    }\n}\n\npub struct Button<'a> {\n    command: Box<&'a dyn Command>,\n}\n\nimpl<'a> Button<'a> {\n    pub fn new(command: Box<&'a dyn Command>) -> Self {\n        Self {\n            command,\n        }\n    }\n\n    pub fn press(&self) {\n        self.command.excute();\n    }\n}\n\npub fn main() {\n    let save_command = SaveCommand::new(\"app\");\n    let exit_command = ExitCommand::new(\"app\");\n    let open_command = OpenCommand::new(\"app\");\n\n    let save_button = Button::new(Box::new(&save_command));\n    let exit_button = Button::new(Box::new(&exit_command));\n    let open_button = Button::new(Box::new(&open_command));\n\n    save_button.press();\n    exit_button.press();\n    open_button.press();\n\n    let save_touch = Button::new(Box::new(&save_command));\n    let exit_touch = Button::new(Box::new(&exit_command));\n    let open_touch = Button::new(Box::new(&open_command));\n\n    save_touch.press();\n    exit_touch.press();\n    open_touch.press();\n}\n```\n\n전략 패턴은 한가지 문제 해결에 대해서 여러 전략을 선택할 수 있는 여러 전략이 하나의 기능에 대응하는 형태라면 커맨드 패턴은 여러 기능에 대해 하나 이상의 전략이 대응되는 형태입니다. \n\n이를 두고 책에서는 커맨드 패턴과 비교한 전략 패턴의 차이점을 ‘letting you swap these algo­rithms with­in a sin­gle con­text class’, 단일 문맥에 대한 알고리즘들의 교체로 표현하였습니다.\n\n---\n\n### 데코레이터(Decorator)\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"crypto/rand\"\n\t\"fmt\"\n\n\t\"github.com/andybalholm/brotli\"\n\t\"github.com/golang/snappy\"\n\t\"github.com/itchio/lzma\"\n)\n\nfunc main() {\n\t{\n\t\tbuffer := bytes.NewBuffer(nil)\n\t\tbrotliWriter := brotli.NewWriter(buffer)\n\t\tsnappyWriter := snappy.NewBufferedWriter(brotliWriter)\n\t\tlzmaWriter := lzma.NewWriter(snappyWriter)\n\n\t\tdata := [512]byte{}\n\t\trand.Read(data[:])\n\t\tlzmaWriter.Write(data[:])\n\n\t\tlzmaWriter.Close()\n\t\tsnappyWriter.Close()\n\t\tbrotliWriter.Close()\n\n\t\tfmt.Println(buffer.Bytes())\n\t}\n\n\t{\n\t\tbuffer := bytes.NewBuffer(nil)\n\t\tlzmaWriter := lzma.NewWriter(buffer)\n\t\tsnappyWriter := snappy.NewBufferedWriter(lzmaWriter)\n\t\tbrotliWriter := brotli.NewWriter(snappyWriter)\n\n\t\tdata := [512]byte{}\n\t\trand.Read(data[:])\n\t\tlzmaWriter.Write(data[:])\n\n\t\tbrotliWriter.Close()\n\t\tsnappyWriter.Close()\n\t\tlzmaWriter.Close()\n\n\t\tfmt.Println(buffer.Bytes())\n\t}\n}\n```\n\n```bash\n[11 28 129 255 6 0 0 115 78 97 80 112 89 1 43 2 0 228 245 231 207 93 0 0 128 0 255 255 255 255 255 255 255 255 0 16 42 90 225 96 215 49 150 191 231 81 102 136 62 95 77 104 92 78 75 16 204 0 91 238 30 145 117 113 242 30 249 138 121 187 208 255 110 204 123 92 139 9 184 87 109 246 142 142 64 250 231 128 158 65 192 100 179 48 32 61 117 150 229 36 110 141 9 125 96 158 5 47 105 93 232 198 47 240 40 123 12 177 172 44 197 38 236 247 246 18 136 238 55 128 131 124 96 125 246 134 126 235 152 114 229 37 169 53 176 129 105 177 57 106 70 6 92 174 203 182 84 237 143 84 150 57 79 64 213 25 26 68 240 131 177 129 101 195 128 69 26 15 191 166 62 51 131 51 114 193 191 70 232 222 139 174 146 210 179 91 213 188 122 46 135 189 31 43 231 105 224 119 213 25 109 75 94 120 32 114 102 220 103 90 211 172 173 57 204 26 171 160 37 223 59 115 18 250 105 171 103 37 81 100 136 178 53 119 160 72 166 232 253 48 44 30 167 148 253 215 171 250 110 207 119 207 201 186 132 197 52 171 81 224 46 4 247 12 73 149 65 217 49 123 177 50 25 92 46 70 37 173 171 195 135 120 126 149 156 126 124 22 225 37 195 43 58 97 16 121 167 1 218 48 129 232 244 104 158 120 164 141 64 130 135 114 188 148 91 206 118 206 230 88 98 108 143 23 180 148 153 12 95 168 70 177 68 143 95 21 82 185 162 15 189 218 12 58 244 103 221 60 94 9 66 143 75 126 82 47 124 206 65 205 146 106 204 119 149 195 214 19 45 114 37 167 11 234 100 183 203 134 219 69 182 201 85 156 131 165 195 135 2 50 146 76 37 39 241 5 51 23 197 36 137 66 189 79 161 42 228 40 228 205 26 88 161 197 38 83 248 152 104 203 6 25 220 250 245 46 134 221 215 72 76 165 92 164 112 89 88 150 69 18 140 129 140 93 111 251 113 228 85 107 67 242 26 39 181 81 60 64 114 145 201 95 52 26 156 1 141 238 129 213 241 254 62 67 2 187 92 209 178 166 117 21 95 30 208 213 10 75 22 68 243 63 192 211 181 244 185 97 64 64 49 174 120 142 230 53 56 236 255 173 170 85 249 202 56 112 212 133 220 38 231 104 220 99 96 58 131 195 128 47 28 227 233 29 103 207 54 236 45 227 128 74 148 160 187 214 27 255 189 34 57 209 163 130 105 170 255 255 245 237 64 0 3]\n[93 0 0 128 0 255 255 255 255 255 255 255 255 0 122 24 25 194 85 84 33 139 74 229 139 76 80 79 119 247 20 8 180 11 199 251 57 222 145 219 179 232 15 99 78 167 85 109 155 143 3 112 15 46 200 161 250 235 38 83 203 221 91 21 123 255 195 115 53 232 201 174 65 204 178 84 213 46 219 6 83 215 195 107 148 132 55 56 158 202 122 122 146 146 223 196 40 207 7 232 218 43 161 11 73 101 163 87 239 20 19 42 43 172 10 35 106 234 153 225 23 183 188 193 192 131 134 50 21 83 175 32 184 76 108 229 232 108 125 3 5 233 157 20 205 10 60 32 176 41 116 13 196 133 51 99 157 158 93 93 170 20 223 84 164 188 113 227 2 73 109 159 67 174 247 178 57 133 164 115 16 12 93 69 138 74 227 94 118 64 51 90 172 170 0 37 82 86 235 237 36 234 56 116 50 57 192 83 78 95 251 54 201 67 79 64 15 35 24 175 138 139 134 197 60 248 169 85 231 89 52 70 60 71 170 179 123 183 237 156 14 161 140 40 106 133 246 252 2 187 217 39 47 50 2 82 141 81 116 81 28 13 173 187 210 148 199 125 124 167 173 133 50 218 157 97 85 55 17 179 33 52 120 96 12 125 96 87 109 43 70 95 57 135 254 20 143 230 12 17 216 105 82 97 91 159 90 188 216 149 170 118 196 160 77 61 75 18 71 247 176 204 136 10 53 5 10 115 138 167 172 238 176 152 229 99 14 45 37 105 37 254 87 195 87 131 114 209 83 33 21 125 48 42 236 109 93 68 179 15 183 123 251 71 130 255 222 121 247 46 60 8 119 201 102 241 201 166 189 187 32 178 18 67 39 224 48 18 243 213 102 174 31 7 233 64 131 46 251 82 95 65 95 200 245 191 206 208 160 177 149 204 212 151 138 144 240 31 235 249 2 215 154 70 208 55 110 32 102 129 227 29 146 169 117 158 103 232 59 161 223 115 38 146 119 194 54 111 41 154 170 191 159 124 216 56 77 56 120 139 124 54 127 144 19 36 40 170 228 168 121 76 26 108 17 232 3 141 158 240 20 127 55 126 87 161 143 246 10 254 249 206 118 243 86 73 14 12 29 231 108 156 139 253 223 10 38 47 15 204 142 174 194 20 175 116 114 227 74 141 110 237 129 30 57 194 48 7 133 234 17 45 98 38 123 123 189 128 3 221 61 2 208 34 175 173 221 150 147 90 135 6 153 52 38 66 21 228 209 48 58 65 22 56 255 255 183 120 0 0]\n```\n\n데코레이터 패턴의 예는 편의를 위해 외부 라이브러리를 사용하였습니다. 각각 `brotli`, `snappy`, `lzma`의 `writer`를 순차적으로 데코레이팅 했습니다. 필요에 따라 서로 다른 순서로 맞추게 되면 서로 다른 목적에 따른 압축 결과물이 나오게 됩니다. \n\n`http server`의 미들웨어와 비슷하다고 봅니다. 여기서는 단순 압축이었지만 어떤 단어를 필터링한다거나 어떤 문장을 말미에 추가한다는 식의 기능도 선택하여 추가할 수 있을 것입니다. \n\n그래서 데코레이터와 전략 패턴의 관계는 여러 전략을 여러 층으로 데코레이팅하는 일종의 상호 보완적 관계에 있다고 생각합니다. \n\n---\n\n### 템플릿 메서드(Template Method)\n\n책에서는 템플릿 메서드 패턴은 클래스 단위에서 상속을 통해 처리하기에 컴파일 타임에 기능이 지정되어 런타임에 안정적(stable)이고, 전략 패턴은 런타임에 전략이 정해지기에 안정적(stable)이지 않다고 표현합니다. 단순히 생각하면 정적 타입 언어와 동적 타입 언어를 생각하면 편리합니다.\n\n```java\n// BaseCalculator.java\npublic abstract class BaseCalculator {\n    public abstract int method(int a, int b);\n\n    final int calculate(int a, int b) {\n        return method(a, b);\n    }\n}\n\n// AddCalculator.java\npublic class AddCalculator extends BaseCalculator {\n    @Override\n    public int method(int a, int b) {\n        return a + b;\n    }\n}\n\n// SubCalculator.java\npublic class SubCalculator extends BaseCalculator {\n    @Override\n    public int method(int a, int b) {\n        return a - b;\n    }\n}\n\n// App.java\npublic class App {\n    public static void main(String[] args) {\n        var addCalculator = new AddCalculator();\n        System.out.println(addCalculator.calculate(6, 1));\n\t\t\t\t// 7\n\n        var subCalculator = new SubCalculator();\n        System.out.println(subCalculator.calculate(10, 3));\n\t\t\t\t// 7\n    }\n}\n```\n\n템플릿 메서드로 간단한 하나의 계산 방법을 가지는 계산기를 만들었습니다. `BaseCalculator` 추상 클래스의 `method` 메서드를 구현함으로 각기 다른 계산기를 구현합니다. 이걸 고도화하는 건 제 파트가 아니니 넘어가겠습니다.\n\n```java\n// BaseCalculator.java\npublic class BaseCalculator {\n    public interface Method {\n        int calculate(int a, int b);\n    }\n\n    private Method method;\n\n    public BaseCalculator(Method method) {\n        this.method = method;\n    }\n\n    final int calculate(int a, int b) {\n        return method.calculate(a, b);\n    }\n}\n\n// App.java\npublic class App {\n    public static void main(String[] args) {\n        var addCalculator = new BaseCalculator((a, b) -> a + b);\n        System.out.println(addCalculator.calculate(1, 2));\n\t\t\t\t// 3\n\n        var subCalculator = new BaseCalculator((a, b) -> a - b);\n        System.out.println(subCalculator.calculate(1, 2));\n\t\t\t\t// -1\n    }\n}\n```\n\n전략 패턴으로 작성하면 단일 계산 방법의 명세를 정의하는 `Method` 인터페이스를 정의하고 `Method` 인터페이스의 값을 `BaseCalculator`가 가집니다. 그리고 `BaseCalculator`의 `calculate`는 `method`의 `calculate`를 호출하여 값을 반환합니다. \n\n`App.java` 파일에서는 자바의 람다식을 이용하여 간편하게 `Method` 인터페이스의 구현체를 만들어 생성자에 넘겨줍니다.  동작은 완전히 동일함을 확인할 수 있습니다. \n\n두 방식의 큰 차이는 역시 템플릿 메서드는 미리 컴파일 때 구현된 클래스가 메서드 영역에 포함되어 실행되어 안정적으로 호출할 수 있지만 전략 패턴은 힙 영역에 올라가서 비교적 덜 안정적입니다.\n\n---\n\n### 브릿지(Bridge)\n\n브릿지 패턴은 관점의 차이로 어떤 전략을 선택하느냐가 아니라 어떤 전략을 특정 객체에 붙이기 위해 중간 인터페이스를 두는 방식을 취한 것이기에 결과 자체는 전략 패턴과 비슷하지만, 전략 패턴이 가지는 여러 전략을 선택하여 사용할 수 있게 하는 의의에 있어 두 패턴은 차이가 있습니다.\n\n---\n\n### 어댑터(Adapter)\n\n어댑터 패턴 또한 어떤 기능을 특정 객체에 붙이기 위한 중간 다리를 제공하기 위해 나온 패턴이라 전략 패턴과 비슷하게 구성될 수는 있지만 두 패턴이 한번에 가지는 뎁스의 차이와 브릿지 패턴과 마찬가지로 의의에 차이가 있다고 생각합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/008_strategy/", "created_date": "2022-03-04T15:57:12Z"}
{"title": "고랭과 아레나", "content": "## Arena?\n\n고랭은 가비지 컬렉터를 쓰는 언어이고, 덕분에 사용자는 메모리를 관리하는 데에 크게 신경을 쓸 필요가 없습니다. 하지만 프로젝트 크기가 커지고, 사용해야할 힙 메모리가 커질수록 더욱 빈번하게, 그리고 한번에 많은 양의 메모리를 수집하여 처리하게 됩니다. 하나의 기능을 수행하고 난 후에는 물론이고, 수행하는 도중에도 GC가 동작하여 응답이 늦어지는 상황이 생겨날 확률이 늘어납니다.\n\n그런 상황에서 유용하게 쓸 수 있는 아레나(`arena`)라는 개념이 등장했습니다. 아레나는 미리 사용할 힙 메모리를 할당 받아서 사용합니다. 이 때 할당받는 큰 힙 메모리 덩어리 하나를 페이지라고 합니다. 일반적인 경우는 IO 버퍼 사이즈로, 4096(4K) 혹은 8192(8K), 16384(16K) byte 중 하나입니다. 이 페이지들이 이중 연결리스트 형태로 이어지고, 필요한 메모리를 기존 페이지에서 충당할 수 없을 때마다 새로운 페이지를 추가합니다. 그리고 아레나를 활용한 모든 작업이 끝나면 마지막에 최종적으로 아레나 전체를 반환합니다.\n\n아레나를 고랭에 구현하기 위해, 먼저 GC에 영향을 받지 않는 동적 할당을 할 수 있을 필요가 있습니다. 이 부분에 대해 [lemon-mint](https://github.com/lemon-mint)님은 [umem](https://github.com/unsafe-risk/umem)이란 레포에서 `runtime` 패키지의 `sysAlloc`과 `sysFree`를 링크하여 사용하는 것으로 아레나를 구현했고, 저는 마이크로소프트가 만든 [`mimalloc`](https://github.com/microsoft/mimalloc)을 `cgo`로 고랭에 붙여서 동적할당을 구현하였습니다. 해당 구현체는 [mi](https://github.com/unsafe-risk/mi) 레포에 아레나와 함께 작성되어 있습니다. \n\n어느 쪽이든 아레나를 사용하게 되면 필요에 따라 힙에 데이터를 할당할 수 있고, 이 힙 메모리는 GC에 영향을 받지 않기 때문에 GC 부담을 줄일 수 있습니다. 또한 한번에 힙 메모리를 할당 받아 사용하기에 일종의 메모리 풀 역할 또한 수행할 수 있습니다. 단점은 어느 쪽이든 고랭에서 정상적으로 지원하는 방법이 아니기에, 불안함이 존재할 수밖에 없다는 것과 페이지 내에 충분한 양의 메모리가 존재하지 않을 경우 새로운 페이지를 할당 받게 되는데, 이 때 메모리 파편화가 발생하여 메모리를 낭비할 수 있습니다.\n\n## mi 아레나 코드\n\n> `현재는 이 코드로 되어 있지 않습니다` from 2022 06 11\n\n아레나 코드는 제가 `umem`에서 가져와서 수정한 `mi`의 아레나 코드를 보여드리겠습니다. 당연하게도 99% lemon-mint님 코드이고 `mimalloc`을 사용하게 바꾼 부분만 제가 작성하였습니다. \n\n```go\npackage arena\n\nimport (\n\t\"reflect\"\n\t\"runtime\"\n\t\"syscall\"\n\t\"unsafe\"\n\n\t\"github.com/unsafe-risk/mi/mimalloc\"\n)\n\n// This code is implemented by lemon-mint.\n// I brought the code from unsafe-risk/umem.\n\n// This Implementation is based on the proposal in the following url: https://github.com/golang/go/issues/51317\n\n// Thread-unsafe Allocation Arena.\ntype Arena struct {\n\t// The start address of the region.\n\thead uintptr\n\t// Tail of the region.\n\ttail uintptr\n}\n\nfunc NewFinalizer() *Arena {\n\ta := &Arena{}\n\truntime.SetFinalizer(a, arenaFinalizer)\n\treturn a\n}\n\nfunc New() *Arena {\n\ta := &Arena{}\n\treturn a\n}\n\nfunc arenaFinalizer(a *Arena) {\n\ta.Free()\n}\n\n// Page Structure\n/*\n\t|  0  |  1  |  2  |  3  |  4  |  5  |  6  |  7  |\n\t|-----|-----|-----|-----|-----|-----|-----|-----|\n\t|  page size            |  page head            |\n\t|-----|-----|-----|-----|-----|-----|-----|-----|\n\t|  Next Page Ptr                                |\n\t|-----|-----|-----|-----|-----|-----|-----|-----|\n\t|                                               |\n\t|                                               |\n\t|                                               |\n\t|                      Data                     |\n\t|                                               |\n\t|                                               |\n\t|                                               |\n\t|-----|-----|-----|-----|-----|-----|-----|-----|\n*/\n```\n\n아레나는 연결리스트로 다음 노드의 포인터를 가지고 있습니다. 이 모양은 lemon-mint님이 그리신 `page structure`에 잘 표현되어 있습니다. 그리고 아레나를 참조하고 있는 변수가 드랍될 때 자동으로 해제 되게끔 파이널라이저가 설정되어 있습니다. \n\n```go\nfunc (r *Arena) newPage(size int) {\n\t// println(\"Allocating new page\", size)\n\tsptr := mimalloc.Malloc(size + 16)\n\tpagesize := (*uint32)(unsafe.Pointer(sptr))\n\tpagehead := (*uint32)(unsafe.Pointer(uintptr(sptr) + 4))\n\tnextpage := (*uint64)(unsafe.Pointer(uintptr(sptr) + 8))\n\n\t*pagesize = uint32(size)\n\t*pagehead = 0\n\t*nextpage = 0\n\n\tif r.tail != 0 {\n\t\t// Add to the tail of the region.\n\t\ttailNextPage := (*uint64)(unsafe.Pointer(r.tail + 8))\n\t\tif *tailNextPage != 0 {\n\t\t\t*nextpage = *tailNextPage\n\t\t}\n\t\t*tailNextPage = uint64(uintptr(sptr))\n\t}\n\tr.tail = uintptr(sptr)\n\tif r.head == 0 {\n\t\tr.head = uintptr(sptr)\n\t}\n\t// println(\"New page allocated\", size, sptr)\n}\n\nvar defaultPageSize = syscall.Getpagesize()*4 - 16\n\nfunc (r *Arena) allocate(size int) uintptr {\nretry:\n\tif r.tail == 0 {\n\t\t// println(\"tail is 0, allocating new page\")\n\t\tif size > defaultPageSize {\n\t\t\tr.newPage(size)\n\t\t} else {\n\t\t\tr.newPage(defaultPageSize)\n\t\t}\n\t}\n\n\tpagesize := (*uint32)(unsafe.Pointer(r.tail))\n\tpagehead := (*uint32)(unsafe.Pointer(r.tail + 4))\n\tnextpage := (*uint64)(unsafe.Pointer(r.tail + 8))\n\tif *pagesize-*pagehead < uint32(size) {\n\t\tif *nextpage != 0 {\n\t\t\tr.tail = uintptr(*nextpage)\n\t\t\tgoto retry\n\t\t}\n\t\tif size > defaultPageSize {\n\t\t\tr.newPage(size)\n\t\t} else {\n\t\t\tr.newPage(defaultPageSize)\n\t\t}\n\t\tpagesize = (*uint32)(unsafe.Pointer(r.tail))\n\t\tpagehead = (*uint32)(unsafe.Pointer(r.tail + 4))\n\t\tnextpage = (*uint64)(unsafe.Pointer(r.tail + 8))\n\t}\n\n\tdata := r.tail + 16 + uintptr(*pagehead)\n\t*pagehead += uint32(size)\n\treturn data\n}\n```\n\n`newPage` 함수는 입력받은 크기로 새로운 페이지를 만듭니다. 여기에 입력받는 크기는 상황에 따라 변하게 되는 데 이 부분은 `allocate` 함수의 실행 과정에 따릅니다. 일반적으로는 `defaultPageSize` 변수의 값을 사용하여 하드웨어 페이지 사이즈에 따라 자동으로 지정됩니다. `mi`의 아레나는 하드웨어에서 지원하는 페이지의 4배 크기를 사용합니다. 이는 `umem`도 비슷합니다. 만약 만들어야 할 페이지의 크기가 `defaultPageSize` 보다 클 경우, 해당 사이즈를 그대로 가져가서 페이지를 만듭니다. \n\n```go\nfunc (r *Arena) Free() {\n\tfor r.head != 0 {\n\t\t_ = (*uint32)(unsafe.Pointer(r.head))\n\t\tnextpage := (*uint64)(unsafe.Pointer(r.head + 8))\n\t\tnexthead := uintptr(*nextpage)\n\t\tmimalloc.Free(unsafe.Pointer(r.head))\n\t\tr.head = nexthead\n\t}\n\tr.tail = 0\n}\n```\n\n마지막으로 `Free` 함수는 모든 아레나를 돌면서 할당된 페이지를 반환합니다. 이 함수가 호출되면 해당 아레나에서 할당받은 모든 포인터는 접근 불가가 됩니다. 하지만 `mimalloc`을 기반으로 하고 있는 `mi`는 메모리를 풀에 캐싱하기 때문에, 접근 에러를 바로 띄우지는 않습니다.\n\n## umem 아레나 벤치마크\n\n`umem` 패키지의 아레나의 벤치마크 코드는 [lemon-mint](https://github.com/lemon-mint)님이 작성하였습니다. 벤치마크 분야는 총 3가지로, 아레나를 통해서 타입 없이 메모리 크기만 받고 동적할당 받을 때, 타입을 가지고 동적할당 받을 때, 그리고 고랭에서 `new`로 동적할당 받을 때로 구성되어 있습니다. \n\n```go\npackage arena\n\nimport (\n\t\"testing\"\n)\n\ntype Person struct {\n\tName    string\n\tAge     int\n\tAddress string\n\tnumber  int\n\tuuid    string\n}\n\nconst nAlloc = 1000000\n\nfunc BenchmarkAllocateUmemUninitializedPerson(b *testing.B) {\n\tr := New()\n\tfor i := 0; i < b.N; i++ {\n\t\tfor j := 0; j < nAlloc; j++ {\n\t\t\tp := NewOfUninitialized[Person](r)\n\t\t\tp.Name = \"John\"\n\t\t\tp.Age = 42\n\t\t\tp.Address = \"London\"\n\t\t\tp.number = i\n\t\t\tp.uuid = \"12345\"\n\t\t}\n\t}\n\tr.Free()\n}\n\nfunc BenchmarkAllocateUmemPerson(b *testing.B) {\n\tr := New()\n\tfor i := 0; i < b.N; i++ {\n\t\tfor j := 0; j < nAlloc; j++ {\n\t\t\tp := NewOf[Person](r)\n\t\t\tp.Name = \"John\"\n\t\t\tp.Age = 42\n\t\t\tp.Address = \"London\"\n\t\t\tp.number = i\n\t\t\tp.uuid = \"12345\"\n\t\t}\n\t}\n\tr.Free()\n}\n\n//go:noinline\nfunc StdNewPerson() *Person {\n\tp := new(Person)\n\treturn p\n}\n\nfunc BenchmarkAllocateStdNew(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tfor j := 0; j < nAlloc; j++ {\n\t\t\tp := StdNewPerson()\n\t\t\tp.Name = \"John\"\n\t\t\tp.Age = 42\n\t\t\tp.Address = \"London\"\n\t\t\tp.number = i\n\t\t\tp.uuid = \"12345\"\n\t\t}\n\t}\n}\n```\n\n### 맥\n\n```bash\ngoos: darwin\ngoarch: amd64\npkg: github.com/unsafe-risk/umem/arena\ncpu: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\nBenchmarkAllocateUmemUninitializedPerson-16    \t      27\t  47605454 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateUmemPerson-16                 \t      28\t  42615860 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateStdNew-16                     \t      33\t  35083543 ns/op\t64000142 B/op\t 1000001 allocs/op\n```\n\n맥의 경우 어느쪽이든 아레나가 `new` 힙 얼록보다 느립니다.\n\n### 리눅스\n\n```bash\ngoos: linux\ngoarch: amd64\npkg: github.com/unsafe-risk/umem/arena\ncpu: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz\nBenchmarkAllocateUmemUninitializedPerson-4   \t      26\t  42833816 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateUmemPerson-4                \t      24\t  45659802 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateStdNew-4                    \t      25\t  48400510 ns/op\t64000235 B/op\t 1000001 allocs/op\n```\n\n```bash\ngoos: linux\ngoarch: amd64\npkg: github.com/unsafe-risk/umem/arena\ncpu: AMD Ryzen 7 4800H with Radeon Graphics         \nBenchmarkAllocateUmemUninitializedPerson-16    \t      45\t  24664832 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateUmemPerson-16                 \t      43\t  26992343 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateStdNew-16                     \t       9\t 118905567 ns/op\t64000272 B/op\t 1000001 allocs/op\n```\n\n```bash\ngoos: linux\ngoarch: arm64\npkg: github.com/unsafe-risk/umem/arena\nBenchmarkAllocateUmemUninitializedPerson-4   \t      22\t  51751258 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateUmemPerson-4                \t      21\t  52538087 ns/op\t       0 B/op\t       0 allocs/op\nBenchmarkAllocateStdNew-4                    \t      20\t  57569415 ns/op\t64000338 B/op\t 1000002 allocs/op\n```\n\n리눅스에서는 상황이 조금 달랐습니다. 인텔 x64와 arm64에서는 큰 차이를 보이지 않은 상태에서 아레나가 조금 앞섰지만, AMD 위에서는 아레나가 압도적으로 빨랐습니다. 아직까진 이 부분에 대해서 지식이 부족하여 설명할 수는 없지만, 리눅스 환경에서는 충분히 경쟁력 있다고 보입니다.\n\n## mi 아레나 벤치마크\n\n`mi` 패키지의 벤치마크는 `umem` 패키지의 벤치마크에서 패러렐 부분을 추가했습니다. 그래서 한번에 여러개의 벤치마크가 동시에 수행하게 됩니다. 이는 `mimalloc`에게 불리한 부분입니다. 여러번의 테스트로 `mimalloc`은 할당 해제를 하더라도, C 메모리 영역에서 할당받은 메모리를 가지고 있다가 재활용을 하게 되는데, 이 벤치마크 코드에서는 아쉽게도 이 부분을 적극적으로 활용할 수는 없을 것같습니다.\n\n```go\npackage arena_test\n\nimport (\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/unsafe-risk/mi/arena\"\n)\n\ntype Person struct {\n\tName string\n\tAge  int\n\tAddr string\n\tZip  int\n}\n\nconst MAX = 2000000\n\nfunc BenchmarkMiArenaPerson(b *testing.B) {\n\tb.RunParallel(func(p *testing.PB) {\n\t\tfor p.Next() {\n\t\t\ta := arena.New()\n\t\t\tfor i := 0; i < MAX; i++ {\n\t\t\t\tp := arena.NewOf[Person](a)\n\t\t\t\tp.Name = \"John\"\n\t\t\t\tp.Age = 32\n\t\t\t\tp.Addr = \"Istanbul\"\n\t\t\t\tp.Zip = 397\n\t\t\t}\n\t\t\ta.Free()\n\t\t}\n\t})\n}\n\n//go:noinline\nfunc StdNewPerson() *Person {\n\tp := new(Person)\n\treturn p\n}\n\nfunc BenchmarkStdNew(b *testing.B) {\n\tb.RunParallel(func(p *testing.PB) {\n\t\tfor p.Next() {\n\t\t\tfor i := 0; i < MAX; i++ {\n\t\t\t\tp := StdNewPerson()\n\t\t\t\tp.Name = \"John\"\n\t\t\t\tp.Age = 32\n\t\t\t\tp.Addr = \"London\"\n\t\t\t\tp.Zip = 1111\n\t\t\t}\n\t\t\truntime.GC()\n\t\t}\n\t})\n}\n```\n\n### 맥\n\n```bash\ngoos: darwin\ngoarch: amd64\npkg: github.com/unsafe-risk/mi/arena\ncpu: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\nBenchmarkMiArenaPerson-16    \t      33\t  33580864 ns/op\t      42 B/op\t       1 allocs/op\nBenchmarkStdNew-16           \t      39\t  30365484 ns/op\t96002062 B/op\t 2000022 allocs/op\n```\n\n`umem` 아레나와 마찬가지로 인텔 맥에서는 `new` 할당에 비해 조금 밀리는 성능을 보입니다.\n\n### 리눅스\n\n```bash\ngoos: linux\ngoarch: amd64\npkg: github.com/unsafe-risk/mi/arena\ncpu: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz\nBenchmarkMiArenaPerson-4   \t      19\t  59619689 ns/op\t     195 B/op\t       0 allocs/op\nBenchmarkStdNew-4          \t      20\t  54869378 ns/op\t96000574 B/op\t 2000006 allocs/op\n```\n\n```bash\ngoos: linux\ngoarch: amd64\npkg: github.com/unsafe-risk/mi/arena\ncpu: AMD Ryzen 7 4800H with Radeon Graphics         \nBenchmarkMiArenaPerson-16    \t      32\t  49734124 ns/op\t     861 B/op\t       2 allocs/op\nBenchmarkStdNew-16           \t      52\t  20565911 ns/op\t96001187 B/op\t 2000011 allocs/op\n```\n\n```bash\ngoos: linux\ngoarch: arm64\npkg: github.com/unsafe-risk/mi/arena\nBenchmarkMiArenaPerson-4   \t      85\t  11788425 ns/op\t      10 B/op\t       0 allocs/op\nBenchmarkStdNew-4          \t      26\t  44319635 ns/op\t96001133 B/op\t 2000011 allocs/op\n```\n\n리눅스의 경우 인텔과 AMD 위에서는 `new` 할당에 비해 매우 떨어지는 모습을 보이지만, ARM64에서는 예상보다 큰 차이를 보였습니다.\n\n## 고랭의 아레나\n\n고랭에서 아레나 이야기가 크게 나온건 비교적 최근인 2022년 2월 22일에 고랭 이슈에 올라온 하나의 [프로포절](https://github.com/golang/go/issues/51317)입니다. 해당 프로포절은 아레나의 기본적인 내용과 이점에 대해 작성하고 있으며, 고랭에 어떤 식으로 아레나를 들여올 건지 쓰여 있고, 고퍼들의 토론이 포함되어 있습니다. 현재는 프로포절로만 올라가 있어서 언제 될지는 알 수 없지만, 공식적으로 추가되면 개인이 만든 위의 2개보다 시스템적으로 훨씬 좋은 성능을 낼 수 있을 것입니다. 게다가 이미 구글은 [gapid](https://github.com/google/gapid) 프로젝트에서 [arena](https://github.com/google/gapid/tree/master/core/memory/arena)를 구현했던 적이 있어 더욱 기대됩니다. \n\n게다가 `umem`이나 `mi`도 기존 인텔x64나 amd64에서 아쉬운 결과를 보였지만, arm64에서는 어느 쪽에서든 좋은 결과를 보여서 앞으로 arm64 아키텍처가 더욱 대중화된다면, 충분히 사용을 고려해볼만 하다고 생각합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/013_go-arena/", "created_date": "2022-05-19T18:25:26Z"}
{"title": "제티", "content": "## 제티 (Jetti)\n\n제티는 제가 프로젝트를 구조적으로 관리하고, 생산성을 조금이라도 높이기 위해 만든 코드 생성기입니다.\n\n### 제티의 지향점\n\n제티는 다음과 같은 지향점을 가지고 있습니다.\n\n1. 고 언어 프로젝트 구조에 대해 어느 정도 강제성을 부여합니다.\n2. 코드를 작성할 때, 귀찮은 부분을 최대한 코드 생성을 통해 줄여줍니다.\n3. 까먹고 하지 못 했다는 이유로 해야할 것을 하지 못하는 일을 최대한 줄여줍니다.\n4. cgo 의존성이나 외부 툴 설치에 대한 간단한 지원을 제공합니다.\n\n## 설치\n\n제티는 `go install`을 통해 설치할 수 있습니다.\n\n```bash\ngo install github.com/snowmerak/jetti/v2/cmd/jetti@latest\n```\n\n## 기능\n\n제티는 다음 기능을 제공합니다.\n\n1. 프로젝트 혹은 파일 생성\n   1. 특정 폴더 구조와 파일을 가지는 프로젝트를 생성합니다.\n   2. 고 언어에 바로 쓸 수 있는 프로토버퍼 파일을 생성합니다.\n   3. 바로 실행할 수 있는 `executable` 패키지를 생성합니다.\n2. 패키지 실행\n   1. `cmd` 폴더 내의 실행 가능한 패키지를 실행합니다.\n3. 패키지 임포트 다이어그램 생성\n   1. 어떤 패키지가 어떤 패키지들을 임포트하고 있는지 D2 다이어그램으로 그려줍니다.\n4. 인터페이스 인덱싱과 구현체 생성\n   1. 빌트인 인터페이스와 작업 중인 프로젝트의 인터페이스를 인덱싱 합니다.\n   2. 인덱싱된 인터페이스 중 일부를 구현하는 구현체 코드를 생성합니다.\n   3. 작성된 메서드에 대한 테스트 함수를 생성합니다.\n5. 주석을 통한 특정 행동을 위한 코드 생성\n6. json/yaml 파일에서 고 구조체 생성\n   1. 프로젝트 내에 json이나 yaml 파일이 있을 경우, 마샬링 가능한 고 구조체를 생성합니다.\n7. proto 파일 생성\n   1. 프로젝트 내에 proto 파일이 있을 경우, `protoc`를 통해 고 코드를 생성합니다.\n8. 함수형 프로그래밍\n   1. 함수형 프로그래밍을 위한 코드를 생성합니다.\n9. cgo 의존성을 위한 폴더 생성\n   1. `cgo` 의존성을 위한 폴더를 생성합니다.\n   2. `jetti run` 명령어를 실행할 때, 해당 폴더를 `CGO_CFLAGS`와 `CGO_LDFLAGS`에 추가합니다.\n10. Pure Go 도구 설치\n    1.  `go install`을 통해 설치할 수 있는 도구를 설치합니다.\n    2.  [`jetti-install`](https://github.com/snowmerak/jetti-install) 레포지토리를 통해 설치할 수 있는 도구를 기록합니다.\n    3.  제티 내에서 해당 레포지토리를 통해 쉽게 목록을 검색하고 설치할 수 있게 도와줍니다.\n\n### 프로젝트 혹은 파일 생성\n\n제티에 `new` 커맨드를 입력함으로 프로젝트 혹은 파일을 생성할 수 있습니다.\n\n```bash\njetti new -h\n```\n\n#### 프로젝트 생성\n\n제티를 통해 고 프로젝트를 초기화할 수 있습니다.\n\n```bash\njetti new <module-name>\n```\n\n위 명령어를 통해 `module-name`을 이름으로 하는 고 프로젝트를 생성할 수 있습니다.  \n예를 들어 `jetti new prac`을 입력하면 다음 구조를 가지는 프로젝트가 생성됩니다.\n\n```bash\n.\n├── generate.go\n├── go.mod\n├── internal\n│   └── doc.go\n├── lib\n│   └── doc.go\n├── model\n│   └── doc.go\n└── README.md\n\n4 directories, 6 files\n```\n\n`go.mod` 파일은 다음처럼 생성됩니다.\n\n```bash\nmodule prac\n\ngo 1.21.0\n```\n\n#### 프로토버퍼 파일 생성\n\n`jetti new --proto <file-path>/<file-name>`을 통해 프로토버퍼 파일을 생성할 수 있습니다.\n\n제티의 구조에서는 `model` 폴더를 데이터 전송 객체를 위한 폴더로 사용합니다.\n\n```bash\njetti new --proto model/user/user\n```\n\n위 명령어를 통해 `model/user/user.proto` 파일이 생성됩니다.  \n`user.proto` 파일은 다음과 같은 내용을 가집니다.\n\n```protobuf\nsyntax = \"proto3\";\n\npackage model/user;\n\noption go_package = \"<module-name>/model/user\";\n```\n\n#### 실행 가능한 패키지 생성\n\n`jetti new --cmd <package-name>`을 통해 실행 가능한 패키지를 생성할 수 있습니다.\n\n```bash\njetti new --cmd prac\n```\n\n위 명령어를 통해 `cmd/prac` 폴더와 `cmd/prac/main.go` 파일이 생성됩니다.\n\n```bash\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n   fmt.Println(\"Hello, world!\")\n}\n```\n\n생성하면 `main.go` 파일은 위와같이 작성되어 있으며, 해당 패키지는 `jetti run prac` 명령어를 통해 실행할 수 있습니다.\n\n### 패키지 실행\n\n`jetti run <package-name>`을 통해 `cmd` 폴더 내의 패키지를 실행할 수 있습니다.\n\n```bash\njetti run prac\n\nHello, world!\n```\n\n위 명령어를 통해 `cmd/prac` 패키지가 실행됩니다.\n\n#### 매개변수를 동반한 패키지 실행\n\n`jetti run <package-name> \"<args> ...\"`을 통해 매개변수를 동반한 패키지를 실행할 수 있습니다.\n\n우선 `prac` 패키지 내의 `main.go`를 다음과같이 수정하겠습니다.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n)\n\nfunc mainI() {\n\tfmt.Println(os.Args)\n}\n```\n\n그리고 다음 명령어처럼 매개변수를 넘겨주면 `os.Args`로 값이 넘어간걸 확인할 수 있습니다.\n\n```bash\njetti run prac a b c\n\n[C:\\Users\\snowm\\AppData\\Local\\Temp\\go-build870924501\\b001\\exe\\prac.exe a b c]\n```\n\n### 패키지 정보 보기\n\n제티는 `show` 커맨드를 통해 다양한 패키지 정보를 확인할 수 있도록 제공할 수 있습니다.\n\n#### 패키지 임포트 다이어그램 생성\n\n`jetti show --imports`를 사용하면 패키지 임포트 다이어그램을 생성할 수 있습니다.\n\n```bash\njetti show --imports\n```\n\n위 명령어를 통해 `imports.svg` 파일이 생성됩니다.  \nD2 기반으로 작성된 다이어그램을 `svg` 파일로 생성합니다.\n\n현재로서는 사용을 추천하지 않습니다.\n\n### 인터페이스 인덱싱과 구현체 생성\n\n제티는 빌트인 라이브러리와 현재 작업 중인 프로젝트에 대해 인터페이스를 인덱싱하고, 구현을 위한 구조체와 메서드 코드 스텁을 생성할 수 있습니다.\n\n#### 인터페이스 인덱싱\n\n`jetti index`를 통해 인터페이스를 인덱싱할 수 있습니다.\n\n```bash\njetti index\n```\n\n위 명령어를 통해 `.jetti-cache` 내의 로컬 캐시에 인덱싱 정보를 저장합니다.\n\n#### 인터페이스 구현체 생성\n\n`jetti impl`을 통해 인터페이스 구현체를 생성할 수 있습니다.\n\n```bash\njetti impl\n```\n\n위 명령어를 입력하면 인터페이스들을 선택할 수 있는 메뉴가 나타납니다.\n\n```bash\n? Select interfaces:  [Use arrows to move, space to select, <right> to all, <left> to none, type to filter]\n> [ ]  cmd/api/testdata/src/issue21181/dep Interface\n  [ ]  cmd/api/testdata/src/pkg/p1 Namer\n  [ ]  cmd/api/testdata/src/pkg/p1 I\n  [ ]  cmd/api/testdata/src/pkg/p1 Public\n  [ ]  cmd/api/testdata/src/pkg/p1 Private\n  [ ]  cmd/api/testdata/src/pkg/p1 Error\n  [ ]  cmd/api/testdata/src/pkg/p2 Twoer\n  [ ]  cmd/doc/testdata ExportedInterface\n\n```\n\n원하는 인터페이스를 방향키로 페이지를 넘겨 찾거나, 검색을 통해 찾은 후 `space` 키로 선택합니다.  \n전부 선택했다면, `enter`(혹은 `return`)을 입력하여 다음으로 넘어갑니다.\n\n예시에서는 `fmt.Printer`와 `fmt.GoPrinter`를 선택하여 `lib` 내부에 `printer` 패키지에 `Printer` 구조체를 생성하였습니다.\n\n```bash\n? Select interfaces: fmt Stringer, fmt GoStringer\n? Package path: jetti/lib/printer\n? Struct name: Printer\n```\n\n이때, 패키지 경로는 고 모듈의 이름을 따라야 합니다. 만약 그렇지 않을 경우에는 해당 폴더 위에 그 구조 그대로 생성되거나, 의도치 않은 버그가 발생할 수 있습니다.\n\n생성한다면 다음 패키지와 파일 구조를 가지게 됩니다.\n\n```bash\n.\n└── printer\n    ├── Printer.GoString.jet.go\n    ├── Printer.GoString.jet_test.go\n    ├── Printer.String.jet.go\n    ├── Printer.String.jet_test.go\n    └── Printer.jet.go\n```\n\n```go\n// Printer.jet.go\npackage printer\n\ntype Printer struct {\n}\n```\n\n```go\n// Printer.String.jet.go\npackage printer\n\nfunc (p *Printer) String() string {\n\t// TODO: implement this method\n\tpanic(\"not implemented\")\n}\n```\n\n```go\n// Printer.String.jet_test.go\npackage printer\n\nimport \"testing\"\n\nfunc TestPrinter_String(t *testing.T) {\n}\n\nfunc BenchmarkPrinter_String(b *testing.B) {\n}\n\nfunc ExamplePrinter_String() {\n}\n\nfunc FuzzPrinter_String(f *testing.F) {\n}\n```\n\n### 주석을 통한 코드 생성\n\n제티는 주석의 내용을 기반으로 코드를 생성할 수 있습니다.\n\n모든 생성은 `jetti generate`를 실행하면 `lib`, `internal`, `model` 폴더 내부의 고와 관련 파일들에 대해 분석 후 필요한 코드를 생성합니다.\n\n#### bean container\n\n`jetti:bean <container-name> ...`을 주석에 넣음으로 의존성 주입을 위한 컨테이너를 생성할 수 있습니다.\n\n이 컨테이너는 패키지 단위에 쓸 수 있게 `bean` 패키지를 별도로 생성하고, 내부에 `Container` 인터페이스와 `Default` 구조체를 생성하여 사용합니다.\n\n먼저 다음과 같은 `Compressor` 코드가 있다고 가정합니다.\n\n```go\npackage compressor\n\n// jetti:bean Compressor\ntype Compressor interface {\n\tCompress([]byte) ([]byte, error)\n\tDecompress([]byte) ([]byte, error)\n}\n```\n\n이 코드를 `jetti generate`를 통해 코드를 생성하면 다음과 같은 코드가 생성됩니다.\n\n```go\n// compressor.bean.jet.go\npackage compressor\n\nimport (\n\t\"errors\"\n\t\"prac/gen/bean\"\n)\n\ntype CompressorBeanKey string\n\nvar errCompressorNotFound error = errors.New(\"compressor not found\")\n\nfunc PushCompressor(beanContainer bean.Container, value Compressor) {\n\tbeanContainer.Set(CompressorBeanKey(\"Compressorkey\"), value)\n}\n\nfunc GetCompressor(beanContainer bean.Container) (value Compressor, err error) {\n\tmaybe, ok := beanContainer.Get(CompressorBeanKey(\"Compressorkey\"))\n\tif !ok {\n\t\treturn nil, errCompressorNotFound\n\t}\n\tvalue, ok = maybe.(Compressor)\n\tif !ok {\n\t\treturn nil, errCompressorNotFound\n\t}\n\treturn value, nil\n}\n\nfunc IsErrCompressorNotFound(err error) (ok bool) {\n\treturn errors.Is(err, errCompressorNotFound)\n}\n```\n\n이 코드를 활용하여, 엔트리 포인트에서 `Compressor`를 생성한 후에 `bean.Container`에 등록하고, 필요한 곳에서 `bean.Container`를 통해 `Compressor`를 가져와 사용할 수 있습니다.\n\n위 예시처럼, 인터페이스일 때는 타입 그대로 활용합니다. 그리고 구조체일 경우엔 포인터를 붙입니다.\n\n#### request scope data\n\n`jetti:request <data-name> ...`을 주석에 넣음으로 요청 범위의 데이터를 생성할 수 있습니다.\n\n이 데이터는 `context.valueCtx`를 통해 요청 범위의 데이터를 저장하고, 가져올 수 있습니다.\n\n먼저 다음과 같은 `Claim` 및 `Claims`가 있다고 가정합니다.\n\n```go\npackage claims\n\ntype Claim struct {\n\tIssuer    string `json:\"iss,omitempty\"`\n\tSubject   string `json:\"sub,omitempty\"`\n\tAudience  string `json:\"aud,omitempty\"`\n\tExpiresAt int64  `json:\"exp,omitempty\"`\n\tNotBefore int64  `json:\"nbf,omitempty\"`\n}\n\n// jetti:request Claims\ntype Claims []Claim\n```\n\n이 코드를 `jetti generate`를 통해 코드를 생성하면 다음과 같은 코드가 생성됩니다.\n\n```go\n// claims.context.jet.go\npackage claims\n\nimport (\n\t\"context\"\n\t\"errors\"\n)\n\ntype ClaimsContextKey struct{}\n\nvar errClaimsNotFound error = errors.New(\"claims not found\")\n\nfunc PushClaims(ctx context.Context, v *Claims) context.Context {\n\treturn context.WithValue(ctx, ClaimsContextKey{}, v)\n}\n\nfunc GetClaims(ctx context.Context) (*Claims, bool) {\n\tv, ok := ctx.Value(ClaimsContextKey{}).(*Claims)\n\treturn v, ok\n}\n\nfunc ErrClaimsNotFound() error {\n\treturn errClaimsNotFound\n}\n\nfunc IsClaimsNotFoundErr(err error) bool {\n\treturn errors.Is(err, errClaimsNotFound)\n}\n```\n\n이 생성된 코드를 통해, 요청 범위 내에서 `context.valueCtx`를 사용할 때 키나 타입을 잘못 사용할 가능성을 현저히 줄일 수 있습니다.\n\n#### 오브젝트 풀 생성\n\n`jetti:pool sync:<pool-name> ...`이나 `jetti:pool chan:<pool-name> ...`으로 간단한 풀을 생성할 수 있습니다.\n\n아까 사용한 `Claim` 코드를 아래와같이 고쳐보겠습니다.\n\n```go\npackage claims\n\n// jetti:pool sync:Infinite chan:Limited\ntype Claim struct {\n\tIssuer    string `json:\"iss,omitempty\"`\n\tSubject   string `json:\"sub,omitempty\"`\n\tAudience  string `json:\"aud,omitempty\"`\n\tExpiresAt int64  `json:\"exp,omitempty\"`\n\tNotBefore int64  `json:\"nbf,omitempty\"`\n}\n```\n\n위 코드는 `Infinite`라는 이름으로 `sync.Pool` 래퍼를 생성하고, `Limited`라는 이름으로 `chan`을 사용한 풀을 생성합니다.\n\n```go\n// infinite.pool.jet.go\npackage claims\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"runtime\"\n\t\"sync\"\n)\n\nvar errInfiniteCannotGet error = errors.New(\"cannot get infinite\")\n\ntype InfinitePool struct {\n\tpool *sync.Pool\n}\n\nfunc (i *InfinitePool) Get() (*Claim, error) {\n\tv := i.pool.Get()\n\tif v == nil {\n\t\treturn nil, errInfiniteCannotGet\n\t}\n\treturn v.(*Claim), nil\n}\n\nfunc (i *InfinitePool) GetWithFinalizer() (*Claim, error) {\n\tv := i.pool.Get()\n\tif v == nil {\n\t\treturn nil, errInfiniteCannotGet\n\t}\n\truntime.SetFinalizer(v, func(v interface{}) {\n\t\ti.pool.Put(v)\n\t})\n\treturn v.(*Claim), nil\n}\n\nfunc (i *InfinitePool) GetWithContext(ctx context.Context) (*Claim, error) {\n\tv := i.pool.Get()\n\tif v == nil {\n\t\treturn nil, errInfiniteCannotGet\n\t}\n\tcontext.AfterFunc(ctx, func() {\n\t\ti.pool.Put(v)\n\t})\n\treturn v.(*Claim), nil\n}\n\nfunc (i *InfinitePool) Put(v *Claim) {\n\ti.pool.Put(v)\n}\n\nfunc NewInfinitePool() InfinitePool {\n\treturn InfinitePool{\n\t\tpool: &sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\treturn new(Claim)\n\t\t\t},\n\t\t},\n\t}\n}\n\nfunc IsInfiniteCannotGetErr(err error) bool {\n\treturn errors.Is(err, errInfiniteCannotGet)\n}\n```\n\n```go\n// limited.pool.jet.go\npackage claims\n\nimport (\n\t\"context\"\n\t\"runtime\"\n\t\"time\"\n)\n\ntype LimitedPool struct {\n\tpool    chan *Claim\n\ttimeout time.Duration\n}\n\nfunc (l *LimitedPool) Get() *Claim {\n\tafter := time.After(l.timeout)\n\tselect {\n\tcase v := <-l.pool:\n\t\treturn v\n\tcase <-after:\n\t\treturn new(Claim)\n\t}\n}\n\nfunc (l *LimitedPool) GetWithFinalizer() *Claim {\n\tafter := time.After(l.timeout)\n\tresp := (*Claim)(nil)\n\tselect {\n\tcase v := <-l.pool:\n\t\tresp = v\n\tcase <-after:\n\t\tresp = new(Claim)\n\t}\n\truntime.SetFinalizer(resp, func(v interface{}) {\n\t\tl.pool <- v.(*Claim)\n\t})\n\treturn resp\n}\n\nfunc (l *LimitedPool) GetWithContext(ctx context.Context) *Claim {\n\tafter := time.After(l.timeout)\n\tresp := (*Claim)(nil)\n\tselect {\n\tcase v := <-l.pool:\n\t\tresp = v\n\tcase <-after:\n\t\tresp = new(Claim)\n\t}\n\tcontext.AfterFunc(ctx, func() {\n\t\tl.Put(resp)\n\t})\n\treturn resp\n}\n\nfunc (l *LimitedPool) Put(v *Claim) {\n\tselect {\n\tcase l.pool <- v:\n\tdefault:\n\t}\n}\n\nfunc NewLimitedPool(size int, timeout time.Duration) LimitedPool {\n\tpool := make(chan *Claim, size)\n\treturn LimitedPool{\n\t\tpool:    pool,\n\t\ttimeout: timeout,\n\t}\n}\n```\n\n각 생성된 코드를 각각 다음 장점을 취할 수 있습니다.\n\n1. sync\n   1. 타입 안정성을 유지할 수 있습니다.\n2. chan\n   1. 풀링되는 오브젝트의 수를 제한할 수 있습니다. 단 생성 수는 제한할 수 없습니다.\n\n공통 장점은 `GetWithFinalizer`와 `GetWithContext`를 통해 오브젝트를 사용한 후에 자동으로 풀에 반환할 수 있습니다.\n\n#### optional 래퍼 생성\n\n`jetti:optional`을 구조체나 인터페이스, 타입 별칭 위에 씀으로, 옵셔널 타입 래퍼를 만들 수 있습니다.\n\n위의 `Claim`과 `Claims`를 다음과같이 다시 수정해보겠습니다.\n\n```go\npackage claims\n\n// jetti:pool sync:Infinite chan:Limited\n// jetti:optional\ntype Claim struct {\n\tIssuer    string `json:\"iss,omitempty\"`\n\tSubject   string `json:\"sub,omitempty\"`\n\tAudience  string `json:\"aud,omitempty\"`\n\tExpiresAt int64  `json:\"exp,omitempty\"`\n\tNotBefore int64  `json:\"nbf,omitempty\"`\n}\n\n// jetti:request Claims\n// jetti:optional\ntype Claims []Claim\n```\n\n각각 `Claim`과 `Claims`에 `jetti:optional`을 추가하였습니다.  \n이렇게 작성하면 두 타입의 옵셔널 타입 래퍼를 생성합니다만, `Claims`의 옵셔널 래퍼는 패키지 이름과 동일하기 때문에, 몇몇 함수 이름이 간략화되어 생성됩니다.\n\n```go\n// claim.option.jet.go\npackage claims\n\ntype OptionalClaim struct {\n\tvalue *Claim\n\tvalid bool\n}\n\nfunc (o *OptionalClaim) Unwrap() *Claim {\n\tif !o.valid {\n\t\tpanic(\"unwrap a none value\")\n\t}\n\treturn o.value\n}\n\nfunc (o *OptionalClaim) IsSome() bool {\n\treturn o.valid\n}\n\nfunc (o *OptionalClaim) IsNone() bool {\n\treturn !o.valid\n}\n\nfunc (o *OptionalClaim) UnwrapOr(defaultValue *Claim) *Claim {\n\tif !o.valid {\n\t\treturn defaultValue\n\t}\n\treturn o.value\n}\n\nfunc SomeClaim(value *Claim) OptionalClaim {\n\treturn OptionalClaim{\n\t\tvalue: value,\n\t\tvalid: true,\n\t}\n}\n\nfunc NoneClaim() OptionalClaim {\n\treturn OptionalClaim{\n\t\tvalid: false,\n\t}\n}\n```\n\n`Claim`의 경우엔 생성자가 `SomeClaim`과 `NoneClaim`입니다.  \n어떤 타입의 `Some`과 `None`인지 확인하기 위해, 이렇게 이름을 생성합니다.\n\n```go\npackage claims\n\ntype OptionalClaims struct {\n\tvalue *Claims\n\tvalid bool\n}\n\nfunc (o *OptionalClaims) Unwrap() *Claims {\n\tif !o.valid {\n\t\tpanic(\"unwrap a none value\")\n\t}\n\treturn o.value\n}\n\nfunc (o *OptionalClaims) IsSome() bool {\n\treturn o.valid\n}\n\nfunc (o *OptionalClaims) IsNone() bool {\n\treturn !o.valid\n}\n\nfunc (o *OptionalClaims) UnwrapOr(defaultValue *Claims) *Claims {\n\tif !o.valid {\n\t\treturn defaultValue\n\t}\n\treturn o.value\n}\n\nfunc Some(value *Claims) OptionalClaims {\n\treturn OptionalClaims{\n\t\tvalue: value,\n\t\tvalid: true,\n\t}\n}\n\nfunc None() OptionalClaims {\n\treturn OptionalClaims{\n\t\tvalid: false,\n\t}\n}\n```\n\n`Claims`의 생성자는 `Some`과 `None`입니다.  \n이는 패키지 이름과 동일하기 때문에, 생성할 때 패키지 이름에 해당하는 부분은 생략됩니다.\n\n#### getter 생성\n\n`jetti:getter`를 구조체에 사용하여, 각 필드에 대한 `getter`를 생성할 수 있습니다.  \n당연히 많은 고퍼 분들이 getter를 싫어하시는 걸 알고 있습니다만, 저는 일부분에 대해서 충분히 효율적이고 좋은 방식이라 생각합니다.\n\n먼저 위에 사용했던 `claims` 패키지에 다음 `errors.go` 파일을 생성하겠습니다.\n\n```go\npackage claims\n\n// jetti:getter\ntype InvalidTokenError struct {\n\ttoken string\n\terr   error\n}\n```\n\n이 코드를 생성하게 되면 다음 파일과 코드가 나옵니다.\n\n```go\n// claims.InvalidTokenError.jet.go\npackage claims\n\nfunc (I *InvalidTokenError) GetToken() string {\n\treturn I.token\n}\n\nfunc (I *InvalidTokenError) GetErr() error {\n\treturn I.err\n}\n```\n\n```go\n// gen/errface/InvalidTokenError.errface.jet.go\npackage errface\n\ntype GetTokenOfClaimsInvalidTokenError interface {\n\tGetToken() string\n}\n\ntype GetErrOfClaimsInvalidTokenError interface {\n\tGetErr() error\n}\n\ntype GetClaimsInvalidTokenError interface {\n\tGetToken() string\n\tGetErr() error\n}\n```\n\n`InvalidTokenError`에 대한 `getter`가 생성되었고, `errface` 패키지에 `InvalidTokenError`에 대한 `getter` 인터페이스가 생성되었습니다.\n\n이렇게 하면 `errface`를 통해 상위 스코프에서 하위 스코프에 쓰인 패키지들을 참조하지 않고도 에러 정보를 가져올 수 있습니다.\n\n### json/yaml 파일에서 고 구조체 생성\n\n`jetti:generate`를 하면 `lib`, `model`, `internal` 내의 폴더와 파일 중 `*.json`과 `*.yaml`, `*.yml` 파일을 찾아서 고 구조체를 생성합니다.\n\n이 기능은 `github.com/twpayne/go-jsonstruct/v2` 패키지를 통해 실행됩니다.\n\n`jetti:generate`를 통해 다음과 같은 `*.json` 파일을 생성하면 다음과 같은 코드가 생성됩니다.\n\n```json\n{\n  \"name\": \"snowmerak\",\n  \"age\": 25,\n  \"hobbies\": [\n    \"programming\",\n    \"reading\",\n    \"writing\"\n  ],\n  \"address\": {\n    \"country\": \"South Korea\",\n    \"city\": \"Seoul\",\n    \"street\": \"Seoul\"\n  }\n}\n```\n\n```go\npackage address\n\nimport (\n\t\"io\"\n\t\"os\"\n\n\t\"github.com/goccy/go-json\"\n)\n\nfunc AddressFromJSON(data []byte) (*Address, error) {\n\tv := new(Address)\n\tif err := json.Unmarshal(data, v); err != nil {\n\t\treturn nil, err\n\t}\n\treturn v, nil\n}\n\nfunc AddressFromFile(path string) (*Address, error) {\n\tf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn AddressFromJSON(f)\n}\n\nfunc (address *Address) Marshal2JSON() ([]byte, error) {\n\treturn json.Marshal(address)\n}\n\nfunc (address *Address) Encode2JSON(w io.Writer) error {\n\treturn json.NewEncoder(w).Encode(address)\n}\n\ntype Address struct {\n\tAddress struct {\n\t\tCity    string `json:\"city\"`\n\t\tCountry string `json:\"country\"`\n\t\tStreet  string `json:\"street\"`\n\t} `json:\"address\"`\n\tAge     int      `json:\"age\"`\n\tHobbies []string `json:\"hobbies\"`\n\tName    string   `json:\"name\"`\n}\n```\n\n`yaml`(혹은 `yml`)에 대해서도 동일한 동작을 수행합니다.\n\n자동으로 `goccy`님의 `go-json`과 `go-yaml`을 추가합니다.\n\n### protobuf 코드 생성\n\n`jetti:generate`를 호출할 때 `lib`, `internal`, `model` 내부에서 `*.proto`을 찾으면 `protoc`를 호출해서 `gen` 폴더 밑에 프로토버퍼 고 코드와 gRPC 코드를 생성합니다.\n\n자동으로 프로토버퍼와 gRPC 관련 디펜던시를 추가합니다.\n\n### 함수형 프로그래밍\n\n`lib/doc.go`에 다음과 같은 코드를 작성합니다.\n\n```go\npackage lib\n\n// jetti:fp\nfunc init() {}\n```\n\n이 코드를 `jetti generate`를 통해 코드를 생성하면 다음과 같은 종류의 코드와 타입이 생성됩니다.\n\n1. `func If[T, R any](cond bool, trueFn func() T, falseFn func() R) result.Result[T, R]`: if 구문의 함수 버전입니다. `true`일 때 `T`타입 반환, `false`일 때 `R`타입 반환합니다.\n2. `func When[T, R any](criteria T, cond ...Condition[T, R]) option.Option[R]`: `criteria`가 `cond`에 해당하는지 확인합니다. `cond`는 `Condition[T, R]` 타입이며, `T`타입을 받아 `R`타입을 반환합니다.\n3. `type Option[T any] struct {...}`: `Option`은 `T`타입을 가집니다. `Option`은 `Some[T](T)`과 `None[T]()`으로 생성하며, `Unwrap() T`, `UnwrapOr(T) T`, `IsSome() bool`, `IsNone() bool` 등으로 값을 추출합니다.\n4. `type Result[T, R any] struct {...}`: `Result`는 `T`와 `R`타입을 가집니다. `Result`는 `Ok[T, R](T)`, `Err[T, R](R)`으로 생성하며, `Unwrap() T`, `UnwrapOr(T) T`, `UnwrapErr() R`, `UnwrapErrOr(R) R`, `IsOk() bool`, `IsErr() bool` 등으로 값을 추출합니다.\n\n### cgo 의존성을 위한 폴더 생성\n\n`jetti new`로 프로젝트를 생성할 때, `clib` 폴더를 생성합니다.  \n이 폴더는 `cgo` 의존성을 위한 폴더입니다.\n\n이 폴더의 구조는 다음 규칙을 지켜야합니다.\n\n1. `clib` 폴더 바로 아래에는 `GOOS-GOARCH` 형식의 폴더가 있어야합니다.\n2. `GOOS-GOARCH` 폴더 바로 아래에는 `lib` 폴더와 `include` 폴더가 있어야합니다.\n3. `lib`은 컴파일된 라이브러리 파일이 포함됩니다.\n4. `include`는 헤더 파일이 포함됩니다.\n\n그러면 `jetti run`을 통해 실행할 때, `CGO_CFLAGS`와 `CGO_LDFLAGS`에 각 OS 및 아키텍처에 맞는 폴더를 추가합니다.\n\n### Pure Go 도구 설치\n\n`jetti tools`를 사용하여 `go install`을 통해 설치할 수 있는 툴을 설치할 수 있습니다.\n\n`jetti tools`는 `jetti-install` 레포지토리를 통해 툴 목록을 가져옵니다.  \n이 레포지토리에 기록된 툴을 쉽게 검색 및 설치할 수 있습니다.\n\n1. `jetti tools --renew`: `jetti-install` 레포지토리를 갱신합니다. 자동으로 갱신하지 않으므로, 적절히 실행해주어야 합니다.\n2. `jetti tools --install`: `jetti-install` 레포지토리에 기록된 툴을 설치합니다. 한번에 하나씩 여러번 여러 툴을 설치할 수 있습니다.\n3. `jetti tools --multi --install`: `jetti-install` 레포지토리에 기록된 툴을 한번에 여러개 설치합니다.\n\n## 버전\n\n이 글은 `v2.12.0` 기준으로 작성되었습니다.", "author": "", "original_url": "https://snowmerak.pages.dev/posts/jetti/", "created_date": "2023-08-20T13:48:43Z"}
{"title": "Result를 활용한 에러처리", "content": "이 글은 제가 작성한 레포인 [generics-for-go](https://github.com/snowmerak/generics-for-go)를 기반으로 쓰여졌습니다.\n\n## 개요\n\n예전 고 1.18 dev 버전이 나왔을 때 한 레포를 만들어서 제네릭 관련 함수를 찍어낸 적이 있습니다. 그 이후 한동안 잊고 지내다가 1.18 beta가 나오게 되고 이제 큰 피처의 변화는 없을 거라 판단해서 제네릭으로 몇가지 장난을 하던 도중, 러스트같은 언어에서 자주 보이는 Option과 Result를 만들어 보고 싶었습니다.\n\n예전에도 당연히 `interface{}`만으로 구현에 도전해봤지만 사실 타입 단언(Type Assertion)을 해야한다는 것에서 그다지 사용성이 좋지 못 했고 이로 인해 코드 수만 늘어났습니다. 하지만 제네릭이라는 수단이 추가되니 생각할 폭이 넓어지고 재밌는 문법이 떠올랐습니다.\n\n```go\nfunc (r *Result[T]) Unwrap() T {\n\tvalue, _ := r.value.(T)\n\treturn value\n}\n```\n\n이 코드는 제가 구현한 `Result` 구조체의 메서드입니다. 제네릭 인자가 타입으로 취급되는 점을 이용하여 제네릭 타입으로 타입 단언을 하고 단언한 값을 반환하는 것입니다. 이렇게 되면 먼저 적었던 라이브러리의 사용자가 타입 단언을 해야해서 추가되는 코드를 획기적으로 줄일 수 있습니다. 해당 코드가 동작하는 걸 확인하고 일종의 희열을 느꼈고 관련 패키지 코드를 작성해 나갔습니다.\n\n## result\n\n### 구조체\n\n```go\ntype Result[T any] struct {\n\tvalue any\n}\n\nfunc Ok[T any](data T) *Result[T] {\n\treturn &Result[T]{value: data}\n}\n\nfunc Err[T any](err error) *Result[T] {\n\treturn &Result[T]{value: err}\n}\n```\n\n구조체 내부에는 `any` 타입을 가지는 value 멤버만이 존재합니다. 그리고 `Ok`와 `Err` 생성자가 존재합니다. `Ok`는 특정 타입 변수를 받아서 해당 타입을 저장하는 `Result[T]` 포인터를 생성하여 반환합니다. `Err`는 타입과 에러를 받고 해당 타입의 `Result[T]` 포인터를 반환합니다. 사용할 때는 다음과 같이 사용하면 됩니다.\n\n```go\nokInt := result.Ok(10)\nerrInt := result.Err[int](errors.New(\"anything\"))\n```\n\n`okInt`는 내부적으로 10을 가지고 `errInt`는 내부적으로 에러 인스턴스를 가집니다. 이들은 간단한 메서드로 어떤 값을 가지고 다음에 어떤 행동을 해야하는가를 정할 수 있습니다.\n\n### 메서드\n\n#### Ok, Unwrap, Err\n\n```go\nokInt := result.Ok(10)\n\nswitch okInt.Ok() {\n    case true:\n        fmt.Println(okInt.Unwrap())\n    case false:\n        log.Println(okInt.Err().Error())\n}\n\nerrInt := result.Err[int](errors.New(\"anything\"))\n\nswitch errInt.Ok() {\n    case true:\n        fmt.Println(errInt.Unwrap())\n    case false:\n        log.Println(errInt.Err().Error())\n}\n```\n\n간단하게 `Ok()` 메서드를 호출함으로 적절한 값이 포함되어 있는지, 에러가 포함되어 있는지 검사하고 다음 행동을 정하게 됩니다. 이때 `Unwrap()` 메서드는 내부에 저장된 값이 해당 타입이 아닐 경우 그 타입의 제로 값을 반환하지만 `Err()` 메서드는 내부에 저장된 값이 에러가 아닐 경우에는 `nil`을 반환합니다.\n\n#### Map, MapOr\n\n```go\na := result.Ok(100)\n\na.Map(func (t int) (int, error)) {\n    return t * 2, nil\n})\n\nif a.Ok() {\n    fmt.Println(a.Unwrap())\n}\n```\n\n`Map()` 메서드는 주어진 함수에 따라 만들어진 값으로 내부 값을 바꾸게 됩니다. 당연하게도 에러가 반환되면 내부 값도 에러로 바뀌게 되어 `Ok()` 메서드의 반환값이 `false`로 바뀌게 됩니다. 하지만 에러가 발생한다 하더라도 어떤 값을 가지게 할 수 있을 것입니다.\n\n```go\na := result.Ok(100)\n\na.MapOr(func (t int) (int, error)) {\n    return -1, errors.New(\"some error\")\n}, 99)\n\nif a.Ok() {\n    fmt.Println(a.Unwrap())\n}\n```\n\n이 코드에서는 `MapOr()` 메서드를 사용하였습니다. 인자는 `Map()` 메서드와 같은 함수와 기본값을 넘겨줍니다. 이 코드에서 넘겨준 함수는 무조건 에러를 반환하여 `a`가 에러를 가져야하지만 대신 인자로 넘겨 받은 기본값, `99`를 대입하게 됩니다.\n\n#### AndThen\n\n만약 지속적으로 같은 타입의 인자와 반환값을 넘겨주는 함수가 연속적으로 반복될 경우 일일이 넘겨주고 에러 체크하는 것이 귀찮을 수 있습니다. \n\n```go\na := result.Ok(100)\n\na = a.AndThen(func(t int) (int, error) {\n    return t * 2, nil\n}).AndThen(func (t int) (int, error) {\n    return t * 3, nil\n})\n\nif a.Ok() {\n    fmt.Println(a.Unwrap() == 600)\n}\n```\n\n예시는 간단한 함수로 작성하였습니다. `a`가 가진 `100`에서 시작하여 2를 곱한 후 3을 곱하여 최종적으로 600이 맞는 지 확인합니다. 출력은 `true`가 나올것입니다.\n\n```go\na := result.Ok(100)\n\na = a.AndThen(func(t int) (int, error) {\n    return t * 2, nil\n}).AndThen(func (t int) (int, error) {\n    return -1, errors.New(\"some error\")\n}).AndThen(func (t int) (int, error) {\n    return t * 3, nil\n})\n\nswitch a.Ok() {\n    case true:\n        fmt.Println(a.Unwrap() == 600)\n    case false:\n        log.Fatal(a.Err().Error())\n        // additional error handling\n}\n```\n\n이 예시는 중간에 에러가 발생할 때를 위한 코드입니다. `AndThen()` 메서드로 연결된 메서드 체인의 경우 중간에 에러가 발생하면 다음 단계에서 함수를 실행하지 않고 에러 인스턴스를 가진 `Result[T]` 객체를 지속적으로 반환합니다. 그리고 아래 스위치 구문에서 발생 가능한 에러들을 한번에 처리할 수 있도록 합니다. 저는 충분히 이 메서드 체인으로도 나쁘지 않다고 생각하지만 그래도 만족할 수 없었습니다.\n\n### 함수\n\n#### ToFunctor\n\n```go\nfunc ToFunctor[T any, R any](fn func(T) (R, error)) func(*Result[T]) *Result[R] {\n\treturn func(param *Result[T]) *Result[R] {\n\t\tif param.Ok() {\n\t\t\tn, err := fn(param.Unwrap())\n\t\t\tif err != nil {\n\t\t\t\treturn Err[R](err)\n\t\t\t}\n\t\t\treturn Ok(n)\n\t\t}\n\t\treturn Err[R](param.Err())\n\t}\n}\n```\n\n해당 함수는 `func(T) (R, error)` 함수를 `func(*Result[T]) *Result[R]` 함수로 래핑해주는 역할을 합니다. 이렇게 함으로 `result` 패키지를 사용하는 사람으로 하여금 직접 객체를 다루는 수고를 줄일 수 있습니다.\n\n---\n\n고의 제네릭에는 몇가지 한계점이 존재하는데, 그 중 하나가 메서드에 구조체에서 선언하지 않은 제네릭 인자를 추가할 수 없다는 것입니다. 이는 고 런타임이 가지는 인터페이스 시스템과 연관이 있기에 해달라고 할 수도 없는 부분이라 다른 방안이 필요했습니다. 그 방법으로 제가 제안하는 건 시작 타입과 끝 타입만 제공하는 방법입니다.\n\n만약 런타임 안정성을 위해 컴파일할 때 타입이 다 파악되면 좋겠다고 생각하시면 지금으로서는 여러 함수를 이어서 새 함수를 만들어주는 함수를 만들어서 계속 피라미드처럼 이어 올라가는 수밖에 없습니다. 이렇게 되면 코드를 작성하는 게 큰 노동이 될 것입니다.\n\n## chain\n\n`chain`은 이를 위해 만든 패키지입니다.\n\n### 구조체\n\n```go\ntype Chain[T, R any] struct {\n\tlist []any\n}\n```\n\n런타임과 `reflect`의 힘을 빌려야하기에 함수 리스트의 타입은 `[]any`(기존의 `[]interface{}`와 동일합니다)으로 작성합니다.\n\n### 함수\n\n```go\nfunc From[T, R any](list ...any) *result.Result[*Chain[T, R]] {\n\tif len(list) == 0 {\n\t\treturn result.Err[*Chain[T, R]](errors.New(\"list is empty\"))\n\t}\n\tif len(list) == 1 {\n\t\tfun := reflect.TypeOf(list[0])\n\t\tif fun.Kind() != reflect.Func {\n\t\t\treturn result.Err[*Chain[T, R]](errors.New(\"list is not a function\"))\n\t\t}\n\t\tif fun.In(0) != reflect.TypeOf(new(T)).Elem() {\n\t\t\treturn result.Err[*Chain[T, R]](errors.New(\"the function's parameter type is invalid\"))\n\t\t}\n\t\tif fun.Out(0) != reflect.TypeOf(new(R)).Elem() {\n\t\t\treturn result.Err[*Chain[T, R]](errors.New(\"the function's return type is invalid\"))\n\t\t}\n\t\treturn result.Success(&Chain[T, R]{list})\n\t}\n\tin := reflect.TypeOf(list[0]).In(0)\n\tif in != reflect.TypeOf(new(T)).Elem() {\n\t\treturn result.Err[*Chain[T, R]](errors.New(\"function's parameter type is invalid\"))\n\t}\n\tout := reflect.TypeOf(list[0]).Out(0)\n\tfor i := 1; i < len(list); i++ {\n\t\tin = reflect.TypeOf(list[i]).In(0)\n\t\tif out != in {\n\t\t\treturn result.Err[*Chain[T, R]](errors.New(\"function's parameter type is invalid\"))\n\t\t}\n\t\tout = reflect.TypeOf(list[i]).Out(0)\n\t}\n\tif out != reflect.TypeOf(new(R)).Elem() {\n\t\treturn result.Err[*Chain[T, R]](errors.New(\"function's return type is invalid\"))\n\t}\n\treturn result.Ok(&Chain[T, R]{list})\n}\n```\n\n`reflect` 패키지를 사용한 것에 대해 마음에 안드시는 분들이 분명 존재할 것입니다. 런타임 성능을 어느정도 희생하는 결과를 낳을 것이지만 안정적인 런타임을 위해 필요했습니다. 이 코드는 입력받은 함수 리스트의 출력과 입력이 앞뒤로 맞는 지 확인하고 한번이라도 틀리면 에러를 포함한 `Result`가 반환되고 아니라면 `Chain[T, R]` 인스턴스를 포함한 `Result`가 반환됩니다.\n\n### 메서드\n\n```go\nfunc (c *Chain[T, R]) Run(param T) R {\n\tparamValue := reflect.ValueOf(param)\n\tfor _, fun := range c.list {\n\t\tfunValue := reflect.ValueOf(fun)\n\t\tresult := funValue.Call([]reflect.Value{paramValue})\n\t\tparamValue = result[0]\n\t}\n\treturn paramValue.Interface().(R)\n}\n```\n\n마지막으로 `Run()` 메서드를 실행함으로 `Chain` 구조체가 가지고 있던 함수 리스트를 순차적으로 실행합니다. 반환값은 `Interface()` 메서드로 `any`로 변환된 후 다시 `R` 타입으로 변환되어 반환됩니다.\n\n## 예시\n\n```go\npackage main\n\nimport (\n\t\"strings\"\n\n\t\"github.com/snowmerak/generics-for-go/chain\"\n\t\"github.com/snowmerak/generics-for-go/result\"\n\t\"github.com/snowmerak/generics-for-go/slice\"\n)\n\nfunc main() {\n\tch := chain.From[*result.Result[string], *result.Result[int]](\n\t\tresult.ToFunctor(func(t string) (string, error) {\n\t\t\treturn strings.Repeat(t, 10), nil\n\t\t}),\n\t\tresult.ToFunctor(func(t string) (string, error) {\n\t\t\treturn string(slice.Filter([]rune(t), func(t rune) bool {\n\t\t\t\treturn t != 'a'\n\t\t\t})), nil\n\t\t}),\n\t\tresult.ToFunctor(func(t string) (string, error) {\n\t\t\treturn slice.JoinToString([]rune(t), \"a\"), nil\n\t\t}),\n\t\tresult.ToFunctor(func(t string) (int, error) {\n\t\t\treturn len(t), nil\n\t\t}),\n\t)\n\n\tswitch ch.Ok() {\n\tcase true:\n\t\trs := ch.Unwrap().Run(result.Ok(\"snowmerak is a fool\"))\n\t\tswitch rs.Ok() {\n\t\tcase true:\n\t\t\tprintln(rs.Unwrap())\n\t\tcase false:\n\t\t\tprintln(rs.Err())\n\t\t}\n\tcase false:\n\t\tprintln(ch.Err())\n\t}\n}\n```\n\n복잡한 코드를 만들려고 하지는 않았습니다. 이 코드는 `chain`과 `result` 그리고 설명하지는 않았지만 보시면 바로 아실 `slice` 패키지를 사용하였습니다. 먼저 `chain.From`을 이용하여 문자열을 10번 반복, 문자열에서 'a'를 필터링, 각 문자 사이에 'a'를 삽입하고 그 길이를 구하는 함수를 연결하였습니다. 만들어진 체인이 제대로 만들어졌는지 확인한 후 \"snowmerak is a fool\"이라는 문장을 넣어 실행시킨 후 다시 에러 여부를 확인하고 각 값을 출력합니다.\n\n```bash\n649\n```", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/002_generic-error/", "created_date": "2021-12-30T19:21:53Z"}
{"title": "간단한 로드밸런서와 HTTP 프록시 서버 구현", "content": "## 레이트 리미터\n\n### Limiter interface\n\n```go\npackage limiter\n\ntype Limiter interface {\n\tTryTake([]byte) (bool, int)\n}\n```\n\n레이트 리미터 객체가 구현해야할 인터페이스로 `Limiter` 인터페이스가 있습니다. 바이트 슬라이스를 받아서 해당 슬라이스를 기반으로 레이트 리밋을 계산하여 이번 요청이 사용 가능하면 `true`와 적절한 status code를 반환합니다.\n\n### slide count struct\n\n```go\ntype SlideCount struct {\n\tlock       *lock.Lock\n\tunit       int64\n\tmaxConnPer float64\n\tprevTime   int64\n\tprevCount  int64\n\tcurCount   int64\n\tnextTime   int64\n}\n\nfunc New(maxConnPer float64, unit time.Duration) limiter.Limiter {\n\tnow := int64(time.Now().UnixNano())\n\treturn &SlideCount{\n\t\tlock:       new(lock.Lock),\n\t\tunit:       int64(unit),\n\t\tmaxConnPer: maxConnPer,\n\t\tprevTime:   now - int64(unit),\n\t\tprevCount:  0,\n\t\tcurCount:   0,\n\t\tnextTime:   now + int64(unit),\n\t}\n}\n```\n\n슬라이드 카운트 구조체는 sliding window count 방식의 레이트 리밋을 구현한 것입니다. 일반적인 케이스와 달리 제 주관적인 해석이 들어가 있으므로 코드가 좀 다릅니다.\n\n일반적인 슬라이딩 윈도우 카운트와 동일한 아이디어를 차용했지만 저는 아예 명시적으로 단위 시간과 윈도우 크기를 정해놓고 좀 더 명확하게 구간을 움직이는 방식으로 코드를 작성했습니다.\n\n그렇기에 이전 시간, 이전 단위 시간 당 연결 수, 현재 연결 수, 다음 시간을 의미하는 구조체 멤버가 존재합니다.\n\n최대 연결 수(maxConnPer)는 단위 시간 당 최대 허용 연결 수입니다.\n\n#### TryTake\n\n```go\nfunc (s *SlideCount) TryTake(_ []byte) (bool, int) {\n\ts.lock.Lock()\n\tdefer s.lock.Unlock()\n\n\tnow := int64(time.Now().UnixNano())\n\n\tif now < s.prevTime {\n\t\treturn false, 0\n\t}\n\n\tfor now > s.nextTime {\n\t\ts.prevTime = s.nextTime - s.unit\n\t\ts.prevCount = s.curCount\n\t\ts.curCount = 0\n\t\ts.nextTime = s.nextTime + s.unit\n\t}\n\n\treq := float64(s.prevCount)*float64(-now+s.prevTime+2*s.unit)/float64(s.unit) + float64(s.curCount+1)\n\tif req > s.maxConnPer {\n\t\treturn false, 0\n\t}\n\n\ts.curCount++\n\n\treturn true, 0\n}\n```\n\n슬라이딩 윈도우 카운트 방식에선 바이트 슬라이스를 받을 필요가 없기 때문에 언더바를 이용하여 매개변수를 처리하였습니다.\n\n구조체 멤버를 많이 생성해 놓았기 때문에 코드는 비교적 간결한 편이라고 생각합니다. 먼저 현재 시간을 구한 후 매우 과거인 경우(`now < s.prevTime`)인 경우 잘못된 요청으로 처리하고 `false`를 반환합니다.\n\n그리고 단위 시간보다 훨씬 많이 기준 시간을 벗어났을 경우 지속적으로 갱신시켜주어 슬라이딩 윈도우의 위치를 맞춰줍니다.\n\n마지막으로 이전 시간에서 일정 비율과 현재 연결 누적 수를 계산하여 최대 연결 수를 초과하였는지 확인하고 결과를 반환합니다.\n\n### AccessControlCount\n\n```go\ntype AccessControlCount struct {\n\tlock       *lock.Lock\n\tlist       map[string]limiter.Limiter\n\tmaxConnPer float64\n\tunit       time.Duration\n}\n\nfunc New(maxConnPer float64, unit time.Duration) limiter.Limiter {\n\treturn &AccessControlCount{\n\t\tlock:       new(lock.Lock),\n\t\tlist:       nil,\n\t\tmaxConnPer: maxConnPer,\n\t\tunit:       unit,\n\t}\n}\n```\n\n이 구조체는 슬라이딩 윈도우를 확장하여 만든 각 클라이언트 별 레이트 리미터입니다.\n\n#### TryTake\n\n```go\nfunc (acc *AccessControlCount) TryTake(key []byte) (bool, int) {\n\tacc.lock.Lock()\n\tdefer acc.lock.Unlock()\n\n\tif acc.list == nil {\n\t\tacc.list = make(map[string]limiter.Limiter)\n\t}\n\n\tslide, ok := acc.list[string(key)]\n\tif !ok {\n\t\tslide = slide_count.New(acc.maxConnPer, acc.unit)\n\t\tacc.list[string(key)] = slide\n\t}\n\n\treturn slide.TryTake(nil)\n}\n```\n\n매개변수의 바이트 슬라이스를 가지고 맵에서 특정 레이트 리미터를 꺼내와서 연산한 후 결과를 반환합니다. \n\n## 로드 밸런서\n\n### balancer interface\n\n```go\npackage balancer\n\ntype Balancer interface {\n\tAdd(string) error\n\tSub(string) error\n\tGet(string) (string, error)\n\tRestore(string) error\n}\n```\n\n`balancer` 인터페이스는 대상 값을 추가, 삭제하고 적절한 로직에 따라 값을 받고 돌려주는 메서드를 구현하도록 합니다.\n\n### least balancer\n\n```go\npackage least\n\nimport (\n\t\"math\"\n\n\t\"github.com/diy-cloud/virtual-gate/balancer\"\n\t\"github.com/diy-cloud/virtual-gate/lock\"\n)\n\ntype Least struct {\n\tcandidates map[string]int64\n\tl          *lock.Lock\n}\n\nfunc New() balancer.Balancer {\n\treturn &Least{\n\t\tcandidates: make(map[string]int64),\n\t\tl:          new(lock.Lock),\n\t}\n}\n```\n\n`least` 패키지는 최소 연결 방식 로드밸런서를 모방합니다. 후보군을 맵에 저장하여 가지고 있습니다.\n\n```go\nfunc (l *Least) Get(_ string) (string, error) {\n\tl.l.Lock()\n\tdefer l.l.Unlock()\n\n\tmin := int64(math.MaxInt64)\n\ttarget := \"\"\n\tfor k, v := range l.candidates {\n\t\tif v < min {\n\t\t\ttarget = k\n\t\t\tmin = v\n\t\t}\n\t}\n\n\tif target == \"\" {\n\t\treturn \"\", balancer.ErrorAnythingNotExist()\n\t}\n\n\tl.candidates[target]++\n\treturn target, nil\n}\n```\n\n최소 연결 방식에 따라 맵을 순회하며 가장 적은 연결 수를 가진 후보를 선택하여 반환합니다.\n\n```go\nfunc (l *Least) Restore(target string) error {\n\tl.l.Lock()\n\tdefer l.l.Unlock()\n\n\tif _, ok := l.candidates[target]; !ok {\n\t\treturn balancer.ErrorValueIsNotExist()\n\t}\n\n\tl.candidates[target]--\n\treturn nil\n}\n```\n\n최소 연결 방식은 연결 수를 저장하고 있어야 하기에 모든 작업이 끝날 경우 `Restore` 메서드를 호출하여 연결 수를 줄여주어야합니다.\n\n### hash balancer\n\n```go\npackage hashed\n\nimport (\n\t\"hash\"\n\n\t\"github.com/diy-cloud/virtual-gate/balancer\"\n\t\"github.com/diy-cloud/virtual-gate/lock\"\n)\n\ntype Hashed struct {\n\tcandidates []string\n\thasher     hash.Hash64\n\tindex      int\n\tl          *lock.Lock\n}\n\nfunc New(hasher hash.Hash64) balancer.Balancer {\n\treturn &Hashed{\n\t\tcandidates: make([]string, 0, 8),\n\t\thasher:     hasher,\n\t\tl:          new(lock.Lock),\n\t}\n}\n```\n\n`hashed` 패키지는 사용자의 정보를 해싱하여 나온 값을 기반으로 후보군에서 후보를 선택하는 방식입니다. 그래서 `hasher` 멤버를 가지며 후보군을 문자열 슬라이스로 가집니다.\n\n```go\nfunc (h *Hashed) Get(id string) (string, error) {\n\th.l.Lock()\n\tdefer h.l.Unlock()\n\n\th.hasher.Reset()\n\th.hasher.Write([]byte(id))\n\thashedIndex := int(h.hasher.Sum64() % uint64(len(h.candidates)))\n\tcount := 0\n\tfor {\n\t\tcount++\n\t\tif count >= len(h.candidates) {\n\t\t\treturn \"\", balancer.ErrorNoAvaliableTarget()\n\t\t}\n\t\tif hashedIndex >= len(h.candidates) {\n\t\t\thashedIndex = 0\n\t\t}\n\t\tcandidate := h.candidates[hashedIndex]\n\t\tif candidate != \"\" {\n\t\t\treturn candidate, nil\n\t\t}\n\t\thashedIndex = hashedIndex + 1\n\t}\n}\n```\n\n후보군에서 후보를 선택하는 방식은 앞서 작성했던 대로 사용자 정보를 해싱하여 인덱스를 구한 후 유효한 후보 정보가 나올 때까지 반복합니다. 만약 너무 오래동안 유효한 후보가 나타나지 않으면 에러를 반환합니다.\n\n```go\nfunc (h *Hashed) Restore(_ string) error {\n\treturn nil\n}\n```\n\n`Restore` 메서드는 해시 로드 밸런서에서는 아무 역할도 하지 않습니다.\n\n## 차단기\n\n### breaker interface\n\n```go\npackage breaker\n\ntype Breaker interface {\n\tBreakDown(target string) error\n\tRestore(target string) error\n\tIsBrokeDown(target string) bool\n}\n```\n\n`Breaker` 인터페이스는 대상이 고장났는지, 고쳐졌는지, 고장난 상태인지를 정할 수 있는 메서드를 제공합니다.\n\n### count breaker\n\n```go\npackage count_breaker\n\nimport (\n\t\"math/rand\"\n\n\t\"github.com/diy-cloud/virtual-gate/breaker\"\n\t\"github.com/diy-cloud/virtual-gate/lock\"\n)\n\ntype CountBreaker struct {\n\tcache       map[string]int\n\tmaxCount    int\n\tminimumRate float64\n\tl           *lock.Lock\n}\n\nfunc New(maxCount int, minimumRate float64) breaker.Breaker {\n\treturn &CountBreaker{\n\t\tcache:       make(map[string]int),\n\t\tmaxCount:    maxCount,\n\t\tminimumRate: minimumRate,\n\t\tl:           new(lock.Lock),\n\t}\n}\n```\n\n`CountBreaker`는 카운트 기반 차단기입니다. 카운트의 최대 한도를 정할 수 있고 최소한의 연결 시도를 보장할 값을 설정할 수 있습니다. \n\n```go\nfunc (c *CountBreaker) BreakDown(target string) error {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\tif c.cache[target] < c.maxCount {\n\t\tc.cache[target] += 1\n\t}\n\treturn nil\n}\n\nfunc (c *CountBreaker) Restore(target string) error {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\tif c.cache[target] > 0 {\n\t\tc.cache[target] -= 1\n\t}\n\treturn nil\n}\n```\n\n`BreakDown`과 `Restore` 메서드를 통해 카운트를 늘리고 줄일 수 있습니다.\n\n```go\nfunc (c *CountBreaker) IsBrokeDown(target string) bool {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\tfMaxCount := float64(c.maxCount)\n\tfTarget := float64(c.cache[target])\n\tif fTarget >= fMaxCount {\n\t\tfTarget = fMaxCount\n\t}\n\tmaxRate := ((fMaxCount-fTarget)/fMaxCount)*(100-c.minimumRate) + c.minimumRate\n\trnd := rand.Float64() * 100\n\treturn rnd >= maxRate\n}\n```\n\n카운트가 최대 수를 넘어갈리 없지만 넘어 갔을 때 최대 값으로 한정 짓습니다. `maxRate` 값을 구하는 식을 통해 카운트에 비례하여 고장 났음을 반환합니다. 최소 연결 시도를 보장하기 위한 보정 값이 추가되어 있어 미리 설정한 최소값은 깔고 계산합니다.\n\n## 프록시\n\n### proxy interface\n\n```go\npackage proxy\n\nimport (\n\t\"github.com/diy-cloud/virtual-gate/balancer\"\n\t\"github.com/diy-cloud/virtual-gate/breaker\"\n\t\"github.com/diy-cloud/virtual-gate/limiter\"\n)\n\ntype Proxy interface {\n\tServe(address string, limiter limiter.Limiter, acl limiter.Limiter, breaker breaker.Breaker, balancer balancer.Balancer) error\n}\n```\n\n`Proxy` 인터페이스는 주소와 레이트 리미터, 또 레이트 리미터, 차단기, 로드 밸런서를 받아서 서버를 실행하는 `Serve` 메서드가 정의되어 있습니다.\n\n### http_proxy\n\n```go\npackage http_proxy\n\ntype HttpProxy struct {\n\tproxyCache map[string]*httputil.ReverseProxy\n\tl          *lock.Lock\n}\n\nfunc NewHttp() *HttpProxy {\n\treturn &HttpProxy{\n\t\tproxyCache: make(map[string]*httputil.ReverseProxy),\n\t\tl:          new(lock.Lock),\n\t}\n}\n```\n\n`HttpProxy` 객체는 `httputil` 패키지의 리버스 프록시 객체를 사용합니다. 미리 연결했던 업스트림서버에 대한 리버스 프록시 인스턴스를 저장하고 있다가 재요청이 있을 경우 가져와서 사용합니다.\n\n```go\nfunc (hp *HttpProxy) ServeHTTP(name string, w http.ResponseWriter, r *http.Request) {\n\tvar upstreamServer *httputil.ReverseProxy\n\thp.l.Lock()\n\tif l, ok := hp.proxyCache[name]; ok {\n\t\thp.proxyCache[name] = l\n\t}\n\tif upstreamServer == nil {\n\t\turl, err := url.Parse(name)\n\t\tif err != nil {\n\t\t\tlog.Println(err)\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tupstreamServer = httputil.NewSingleHostReverseProxy(url)\n\t}\n\thp.l.Unlock()\n\n\tupstreamServer.ServeHTTP(w, r)\n\n\thp.l.Lock()\n\tif _, ok := hp.proxyCache[name]; !ok {\n\t\thp.proxyCache[name] = upstreamServer\n\t}\n\thp.l.Unlock()\n}\n```\n\n`ServeHTTP` 메서드는 위에 작성된 것처럼 미리 연결된 리버스 프록시 인스턴스가 있는 지 체크하고 있다면 그걸 사용하고 없다면 새로운 인스턴스는 만들어서 요청하는 동작을 합니다.\n\n```go\nfunc (hp *HttpProxy) Serve(address string, limiter limiter.Limiter, acc limiter.Limiter, breaker breaker.Breaker, balancer balancer.Balancer) error {\n\thandler := func(w http.ResponseWriter, r *http.Request) {\n\t\tremote := []byte(r.RemoteAddr)\n\n\t\twr := NewResponse()\n\n\t\tif b, code := limiter.TryTake(remote); !b {\n\t\t\tlog.Println(\"HttpProxy.Serve: limiter.TryTake: false from\", r.RemoteAddr)\n\t\t\tw.WriteHeader(code)\n\t\t\treturn\n\t\t}\n\n\t\tif b, code := acc.TryTake(remote); !b {\n\t\t\tlog.Println(\"HttpProxy.Serve: acl.TryTake: false from\", r.RemoteAddr)\n\t\t\tw.WriteHeader(code)\n\t\t\treturn\n\t\t}\n\n\t\tfor count := 0; count < 10; count++ {\n\t\t\tupstreamAddress, err := balancer.Get(r.RemoteAddr)\n\t\t\tif err != nil {\n\t\t\t\tlog.Println(\"HttpProxy.Serve: balancer.Get:\", err, \"from\", r.RemoteAddr)\n\t\t\t\tw.Write([]byte(err.Error()))\n\t\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer balancer.Restore(upstreamAddress)\n\n\t\t\tif ok := breaker.IsBrokeDown(upstreamAddress); ok {\n\t\t\t\tlog.Println(\"HttpProxy.Serve: breaker.IsBrokeDown: true from\", r.RemoteAddr, \"to\", upstreamAddress)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\thp.ServeHTTP(upstreamAddress, wr, r)\n\n\t\t\tif _, ok := statusCodeSet[wr.StatusCode]; ok {\n\t\t\t\tlog.Println(\"HttpProxy.Serve: breakDown: true from\", r.RemoteAddr, \"to\", upstreamAddress)\n\t\t\t\tbreaker.BreakDown(upstreamAddress)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tbreaker.Restore(upstreamAddress)\n\n\t\t\tif _, ok := statusCodeSet[wr.StatusCode]; !ok {\n\t\t\t\tw.Write(wr.Body)\n\t\t\t\tw.WriteHeader(wr.StatusCode)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tserver := http.Server{\n\t\tAddr:    address,\n\t\tHandler: http.HandlerFunc(handler),\n\t}\n\treturn server.ListenAndServe()\n}\n```\n\n`Serve` 메서드는 간단한 핸들러를 기반으로 동작합니다. 해당 핸들러는 레이트 리미터를 통해 요청을 처리할 수 있는 여유가 있는지 확인합니다. `acc` 객체를 통해 개별 클라이언트 당 요청을 처리할 수 있는 여유가 있는지 확인합니다.\n\n로드 밸런서를 호출하여 적절한 업스트림서버를 받고 브레이커 객체를 통해 해당 업스트림서버가 정상적인지 확인합니다. 만약 아니라면 반복문을 다시 반복하여 밸런서에서 새로운 업스트림서버를 받아옵니다. \n\n성공적으로 동작할 경우 브레이커 객체의 `Restore` 메서드를 호출하고 요청을 끝냅니다.\n\n## main.go\n\n```go\npackage main\n\nimport (\n\t\"time\"\n\n\t\"github.com/diy-cloud/virtual-gate/balancer/least\"\n\t\"github.com/diy-cloud/virtual-gate/breaker/count_breaker\"\n\t\"github.com/diy-cloud/virtual-gate/limiter/slide_count\"\n\t\"github.com/diy-cloud/virtual-gate/limiter/slide_count/acc\"\n\t\"github.com/diy-cloud/virtual-gate/proxy/http_proxy\"\n)\n\nfunc main() {\n\tlimiter := slide_count.New(30000, time.Microsecond)\n\tacc := acc.New(60, time.Microsecond)\n\tbreaker := count_breaker.New(200, 10)\n\tbalancer := least.New()\n\tbalancer.Add(\"http://127.0.0.1:8080\")\n\tbalancer.Add(\"http://127.0.0.1:8081\")\n\tbalancer.Add(\"http://127.0.0.1:8082\")\n\tbalancer.Add(\"http://127.0.0.1:8083\")\n\tbalancer.Add(\"http://127.0.0.1:8084\")\n\tbalancer.Add(\"http://127.0.0.1:8085\")\n\tbalancer.Add(\"http://127.0.0.1:8086\")\n\tbalancer.Add(\"http://127.0.0.1:8087\")\n\tbalancer.Add(\"http://127.0.0.1:8088\")\n\tbalancer.Add(\"http://127.0.0.1:8089\")\n\tproxy := http_proxy.NewHttp()\n\n\tif err := proxy.Serve(\"0.0.0.0:9999\", limiter, acc, breaker, balancer); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n마이크로세컨드 당 30000회의 요청을 수락하고 사용자 별로 마이크로세컨드 당 60회의 요청을 수락하는 레이트 리미터를 생성하였습니다. 브레이커는 최대 200회의 누적 실패 수를 기록하고 10%의 최소 연결 시도를 보장하는 인스턴스로 생성합니다. 밸런서는 최소 연결 방식이며 로컬 호스트 8080~8089까지 연결했습니다.\n\n프록시는 HTTP 리버스 프록시로 로컬 호스트의 9999 포트에 연결하였습니다. 정상적으로 8080~8089 포트에 http 서버를 띄워놓았다면 9999 포트로 접속했을 때 돌아가며 요청을 처리하는 걸 확인할 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/009_proxy/", "created_date": "2022-03-11T06:45:51Z"}
{"title": "role playing go", "content": "## 개요\n\n고 언어를 사용하여 프로젝트를 구성할 때, 제가 주로 작성하는 스타일을 정리하는 글입니다.\n\n대체로 `import cycle`을 해결하거나 어느정도의 OOP를 구현하는 데에도 용이하다고 생각합니다.\n\n## 역할\n\n각 패키지는 고유의 역할을 가지게 됩니다. 역할은 2가지 경우로 나뉩니다. \n\n1. 역할에 대해 단일 구현체만 존재할 경우\n2. 역할에 대해 여러 구현체가 존재할 경우\n\n### 단일 구현체가 존재할 경우\n\n여러 서비스 간 주고 받는 로그 객체를 만든다고 가정합시다. 저는 `log`라는 패키지를 만들 것입니다.\n\n```go\npackage log\n\ntype Log struct {\n\tUnixTime int64  `parquet:\"name=unix_time, type=INT64\" json:\"unix_time\"`\n\tAppID    int32  `parquet:\"name=app_id, type=INT32\" json:\"app_id\"`\n\tLevel    int32  `parquet:\"name=level, type=INT32\" json:\"level\"`\n\tMessage  string `parquet:\"name=message, type=BYTE_ARRAY, convertedtype=UTF8, encoding=PLAIN_DICTIONARY\" json:\"message\"`\n}\n```\n\n`log`라는 패키지 아래에 `Log` 구조체를 직접 생성하여 `log` 패키지를 클래스처럼 작성합니다. 여기에 `func New() Log`로 생성자도 만들어주면 다른 패키지에서 `log.New()`로 클래스를 쓰듯이 `Log` 구조체의 인스턴스를 생성할 수 있습니다.\n\n### 여러 구현체가 존재할 경우\n\n`Tester`라는 역할이 있다고 가정합시다. 저는 `tester`라는 패키지를 만들게 될 것입니다.\n\n```go\npackage tester\n\ntype Tester interface {\n    Test(...interface{})\n}\n```\n\n먼저 추상적인 테스터의 기능을 작성할 인터페이스를 선언합니다. 편의를 위해 매개변수는 빈 인터페이스 가변 인자로 작성합니다. 테스터에는 단순 입력한 걸 기반으로 동작하는 `SimpleTester`, 자동으로 입력 가능한 랜덤 값을 추출해서 동작하는 `FuzzTester`가 존재할 수 있습니다.\n\ntester 패키지 하위 디렉토리에 `simple_tester`와 `fuzz_tester`를 작성하고 `tester` 인터페이스를 구현하여 작성합니다.\n\n```go\npackage simple_tester\n\ntype SimpleTester struct {}\n\nfunc New() tester.Tester {\n    return new(SimpleTester)\n}\n\nfunc (s *SimpleTester) Test(values ...interface{}) {}\n```\n\n```go\npackage fuzz_tester\n\ntype FuzzTester struct {}\n\nfunc New() tester.Tester {\n    return new(FuzzTester)\n}\n\nfunc (f *FuzzTester) Test(values ...interface{}) {}\n```\n\n각각 테스터 역할을 수행할 수 있지만 세부 기능은 다른 형태로 작성되어 동작하게 될 것입니다. \n\n## 파생과 포함\n\n각 패키지는 고유의 역할을 가지지만 각 역할 간에는 파생 혹은 포함 관계가 성립할 수 있습니다. 그럴 경우 양자 간의 관계일 경우엔 해당 역할의 루트 디렉토리의 하위 디렉토리에 작성합니다.\n\n### 단일 역할에서 단일 역할이 파생될 경우\n\n제가 작성한 로그스트림([github](https://github.com/diy-cloud/logstream))의 구조입니다.\n\n```bash\n.\n├── LICENSE\n├── README.md\n├── consumer\n│   ├── consumer.go\n│   ├── nats\n│   │   └── nats.go\n│   └── stdout\n│       └── stdout.go\n├── go.mod\n├── go.sum\n├── log\n│   ├── logbuffer\n│   │   ├── logbuffer.go\n│   │   ├── logqueue\n│   │   │   ├── pq.go\n│   │   │   └── readme.md\n│   │   └── logring\n│   │       ├── rb.go\n│   │       └── readme.md\n│   ├── loglevel\n│   │   ├── level.go\n│   │   └── readme.md\n│   ├── readme.md\n│   └── struct.go\n├── logstream.go\n└── trie.go\n```\n\n`log` 밑의 `logbuffer` 패키지와 `loglevel` 패키지의 경우 `log`의 객체를 직접 활용하고 있으므로 바로 하위에 위치하고 있습니다. \n\n### 여러 역할에서 파생될 경우 혹은 여러 역할을 포함할 경우\n\n여러 역할에서 파생될 경우에는 최소 공통 최상위 디렉토리에 작성하게 됩니다. virtual-gate 레포([github](https://github.com/diy-cloud/virtual-gate))가 그런 구조입니다.\n\n```bash\n.\n├── LICENSE\n├── README.md\n├── balancer\n│   ├── balancer.go\n│   ├── hashed\n│   │   └── hashed.go\n│   ├── least\n│   │   └── least.go\n│   └── round\n│       └── round.go\n├── breaker\n│   ├── breaker.go\n│   ├── count_breaker\n│   │   └── breaker.go\n│   └── simple_breaker\n│       └── breaker.go\n├── go.mod\n├── go.sum\n├── limiter\n│   ├── bucket\n│   │   └── bucket.go\n│   ├── limiter.go\n│   ├── slide_count\n│   │   ├── acc\n│   │   │   └── acc.go\n│   │   └── slide.go\n│   └── slide_log\n│       └── log.go\n├── lock\n│   └── lock.go\n└── proxy\n    ├── http_proxy\n    │   ├── extensions.go\n    │   ├── http.go\n    │   └── response.go\n    ├── proxy.go\n    └── tcp_proxy\n        └── tcp.go\n```\n\n`balancer`, `limiter`, `breaker`는 모두 독립된 역할을 하지만 `proxy`는 앞의 3가지 역할을 포함합니다. 그렇기에 `proxy`도 동일하게 최소 공통 최상위 디렉토리에 위치하게 됩니다. \n\n### 여러 역할에서 포함하는 역할\n\n`net/http` 베이스로 작성한 럭스([github](https://github.com/diy-cloud/lux))의 디렉토리 구조입니다.\n\n```bash\n.\n├── LICENSE\n├── LICENSES.md\n├── context\n│   ├── context.go\n│   ├── get.go\n│   ├── header.go\n│   ├── reply.go\n│   ├── status.go\n│   └── websocket.go\n├── go.mod\n├── go.sum\n├── handler\n│   └── handler.go\n├── logext\n│   ├── logext.go\n│   └── stdout\n│       └── stdout.go\n├── lux.go\n├── middleware\n│   ├── acl.go\n│   ├── authorize.go\n│   ├── compress.go\n│   ├── cors.go\n│   └── middleware.go\n├── readme.md\n├── router\n│   ├── router.go\n│   └── routergroup.go\n├── signal\n│   └── signal.go\n└── util\n    ├── ip.go\n    └── mime.go\n```\n\n> `router`와 `middleware`, `context` 패키지의 구조는 이 글에서는 안티패턴입니다.\n\n`util` 패키지는 각 패키지에서 사용할 유용한 기능을 담당하는 역할을 합니다. 다른 역할을 파생시키지는 않지만 포함되어 도와주는 역할을 하게 됩니다. 이 경우에도 공통된 최소 최상위 디렉토리에 위치하게 됩니다.\n\n### 여러 독립적 기능을 가진 역할들이 파생될 때\n\n> 기본적으로 이 글에서는 안티패턴입니다. 분리하는 게 맞지만 부득이하게 수정해야할 코드가 많을 경우 이렇게 작성합니다.\n\n위 럭스의 구조에서 `context`는 2가지의 파생 역할이 포함되어 있습니다.\n\n```go\npackage context\n\ntype LuxContext struct {\n\tRequest     *http.Request\n\tResponse    *Response\n\tRouteParams httprouter.Params\n}\n\ntype WSContext struct {\n\tConn net.Conn\n}\n```\n\n럭스는 두가지 컨텍스트를 가집니다. 하나는 일반적인 HTTP 요청에서 사용하는 컨텍스트이고 다른 하나는 웹소켓 요청에서 사용하는 컨텍스트입니다. 서로 같은 컨텍스트라는 역할을 공유하지만 독립적으로 다른 기능을 제공합니다. 이 경우엔 같은 패키지에 작성하고 파일과 이름을 분리하여 관리합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/010_role-playing-go/", "created_date": "2022-03-11T20:09:47Z"}
{"title": "PGO를 쉽게 하는 방법이 있을까?", "content": "## 개요\n\n> 왜 사람들은 PGO(Profile Guided Optimization)을 적극적으로 사용하지 않을까?\n\n저는 이 의문을 예전부터 품고 있었습니다. 생각해보니 프로파일을 저장하고, 가져오는 어떠한 표준화된 프로토콜이 없다는 게 이유로 보였습니다. 사실 쓸 사람들은 어떻게든 쓰고 있겠지만, 저도 처음 PGO를 적용할 때에 어떻게 저장하고 가져와야 할지 고민을 좀 했었습니다. 그래서 이번 글에선 해당 내용에 대한 공유를 하겠습니다.\n\n## 설계 및 구현\n\n### 전형적인 읽는 사람 따로, 쓰는 사람 따로인 구조\n\nPGO 특성 상, 프로파일을 주기적으로 생성하고 업로드하는 실제 서비스로 올라간 어플리케이션과, 해당 프로파일들을 받아서 하나로 합치고 빌드할 때 적용하는 빌드 어플리케이션이 있습니다. 두 과정이 철저하게 분리되어 있기에 동기적으로 사고할 필요가 없습니다. 그럼 가장 적합한 구조는 중간에 버퍼나 저장소를 두고 계속 데이터를 추가, 필요할 때 다운로드, 주기적으로 데이터를 삭제하는 과정만 있으면 됩니다.\n\n그래서 해당 동작들을 모두 수행할 수 있는 `Storage` 인터페이스를 먼저 선언합니다.\n\n```go\n// Storage is an interface that defines the methods that a storage system for golang profiles for PGO can implement.\ntype Storage interface {\n\t// SaveProfile saves the profile data to the storage system.\n\tSaveProfile(ctx context.Context, createdAt time.Time, profile []byte) error\n\t// GetProfile retrieves the profile data from the storage system.\n\tGetProfile(ctx context.Context, createdAt time.Time) ([]byte, error)\n\t// GetProfiles retrieves the profile data from the storage system.\n\tGetProfiles(ctx context.Context, startedAt, endedAt time.Time) ([][]byte, error)\n\t// DeleteProfile deletes the profile data from the storage system.\n\tDeleteProfile(ctx context.Context, createdAt time.Time) error\n\t// DeleteProfiles deletes the profile data from the storage system.\n\tDeleteProfiles(ctx context.Context, startedAt, endedAt time.Time) error\n}\n```\n\n1. SaveProfile은 프로파일을 저장합니다.\n2. GetProfile과 GetProfiles는 프로파일을 가져옵니다.\n3. DeleteProfile과 DeleteProfiles는 프로파일을 삭제합니다.\n\n이제 필요에 따라, `Storage` 인터페이스를 구현하는 전략을 구현하고 적용하면 됩니다.  \n저는 필요에 의해 `minio`(`S3`)와 `local-directory`를 구현해놓았습니다.\n\n### 프로파일 생성 및 업로드는?\n\n프로파일 생성과 업로드를 담당하기 위해 `Profiler`라는 구조체를 생성합니다.\n\n```go\ntype Profiler struct {\n\tstorage storage.Storage\n\n\tcancelFunc context.CancelFunc\n\n\tinterval time.Duration\n\tduration time.Duration\n}\n```\n\n프로파일러는 \n1. 얼마의 간격(internal)을 가지고 프로파일을 수집할 것인지\n2. 얼마나 많은 시간(duration) 동안 프로파일을 수집할 것인지\n에 대한 정보가 필요합니다.\n\n이를 바탕으로 다음과 같은 `Run` 메서드를 작성합니다.\n\n```go\nfunc (p *Profiler) Run(ctx context.Context) (<-chan error, error) {\n\tctx, cancel := context.WithCancel(ctx)\n\tp.cancelFunc = cancel\n\tticker := time.NewTicker(p.interval)\n\n\tdone := ctx.Done()\n\n\terrCh := make(chan error, 32)\n\n\tgo func() {\n\t\tdefer ticker.Stop()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-done:\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t\tgo func() {\n\t\t\t\t\tnow := time.Now()\n\t\t\t\t\tpf, err := collectCpuProfile(p.duration)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terrCh <- fmt.Errorf(\"failed to collect CPU profile: %w\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\tif err := p.storage.SaveProfile(ctx, now, pf); err != nil {\n\t\t\t\t\t\terrCh <- fmt.Errorf(\"failed to save profile: %w\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn errCh, nil\n}\n```\n\n이 메서드는 실행할 때의 컨텍스트가 유지되는 동안 실행되며, CPU 프로파일을 주기적으로 수집해서 `Storage`에 업로드합니다.\n\n그리고 `GetProfiles` 메서드를 구현합니다.\n\n```go\nfunc (p *Profiler) GetProfile(ctx context.Context, startedAt, endedAt time.Time) ([]byte, error) {\n\trawProfiles, err := p.storage.GetProfiles(ctx, startedAt, endedAt)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get profiles: %w\", err)\n\t}\n\n\tprofiles := make([]*profile.Profile, 0, len(rawProfiles))\n\tfor _, rawProfile := range rawProfiles {\n\t\tpf, err := profile.ParseData(rawProfile)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse profile: %w\", err)\n\t\t}\n\t\tprofiles = append(profiles, pf)\n\t}\n\n\tvalue, err := profile.Merge(profiles)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to merge profiles: %w\", err)\n\t}\n\n\tbuf := new(bytes.Buffer)\n\tif err := value.Write(buf); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write profile: %w\", err)\n\t}\n\n\treturn buf.Bytes(), nil\n}\n```\n\n`GetProfiles` 메서드는 특정 시간의 프로파일을 가져와서 하나의 프로파일 파일(pprof)로 병합하는 메서드입니다.  \n이를 통해 실제 빌드할 때, PGO의 옵션으로 프로파일을 쉽게 적용할 수 있도록 합니다.\n\n## 결과\n\n### 프로파일 생성\n\n프로파일을 생성하는 코드는 다음과 같이 작성할 수 있습니다.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/snowmerak/pgolib/profile\"\n\t\"github.com/snowmerak/pgolib/storage/minio\"\n\n\t\"signal\"\n\t\"os\"\n)\n\nfunc main() {\n\tctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt)\n\tdefer cancel()\n\n\tstrg, err := minio.NewClient(ctx, \"sample\", 10, \"profile\", &minio.Config{\n\t\tEndpoint: \"localhost:9000\",\n        Bucket:          \"profile\",\n\t\tAccessKeyID:     \"minio\",\n\t\tSecretAccessKey: \"minio123\",\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tprof := profile.New(strg, 30*time.Minute, 5*time.Minute) // 30 minutes for delay, 5 minutes for collect interval\n\n\terrCh, err := prof.Run(ctx)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdone := ctx.Done()\nloop:\n\tfor {\n\t\tselect {\n\t\tcase err := <-errCh:\n\t\t\tlog.Printf(\"error: %v\", err)\n\t\tcase <-done:\n\t\t\tbreak loop\n\t\t}\n\t}\n\t\n\tlog.Println(\"done\")\n}\n```\n\n전체적인 코드가 보기 쉬운 편은 아닌 것같지만, `Storage`와 `Profiler`를 통해 비교적 쉽게 PGO를 위한 프로파일을 수집 & 업로드할 수 있습니다.  \n\n### 프로파일 다운로드 및 병합\n\n프로파일을 다운로드하고 병합하는 코드는 이렇게 작성할 수 있습니다.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"os/signal\"\n\t\"time\"\n\n\t\"github.com/snowmerak/pgolib/profile\"\n\t\"github.com/snowmerak/pgolib/storage/minio\"\n)\n\nfunc main() {\n\tconst (\n\t\tappName = \"sample\"\n\t)\n\n\tvar (\n\t\tendedAt   = time.Now()\n\t\tstartedAt = endedAt.Add(-24 * time.Hour)\n\t)\n\n\tctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt)\n\tdefer cancel()\n\n\tstrg, err := minio.New(ctx, appName, 32, \"profile\", &minio.Config{\n\t\tEndpoint:        \"localhost:9000\",\n\t\tBucket:          \"profile\",\n\t\tAccessKeyID:     \"minio\",\n\t\tSecretAccessKey: \"minio123\",\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tprofiler := profile.NewProfiler(strg, 0, 0)\n\tdata, err := profiler.GetProfile(ctx, startedAt, endedAt)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tf, err := os.Create(\"profile.pprof\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer f.Close()\n\tdefer f.Sync()\n\n\tif _, err := f.Write(data); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n다운로드 및 병합하는 코드 또한 간단하게 적용할 수 있습니다.\n\n### 도커 빌드\n\n이렇게 만들어진 `profile.pprof`는 다음과 같은 형태의 `Dockerfile`을 통해 쉽게 적용할 수 있습니다.\n\n```Dockerfile\nFROM golang:1.23 AS builder\nLABEL authors=\"<your-name>\"\n\nARG PGO=off\n\nWORKDIR /app\n\nCOPY go.mod go.sum ./\nRUN go mod download\n\nCOPY . .\n\n# Build the Go app\nRUN CGO_ENABLED=0 go build -pgo=$PGO -o ./build/app ./cmd/app/.\n\nFROM alpine:3.20\n\nWORKDIR /app\n\nCOPY --from=builder /app/build/app .\n\nCMD [\"./app\"]\n```\n\n```shell\ndocker build -t sample:latest -f Sample.Dockerfile --build-arg PGO=profile.pprof .\n```\n\n앞으로 좀 더 다듬어야 할 부분이 있겠지만, 이정도면 프로파일링과 PGO에 대한 난이도를 낮출 수 있을 거라 기대합니다.\n\n## 외부 링크\n\n- [snowmerak/pgolib](https://github.com/snowmerak/pgolib)", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/040_auto_pgo/", "created_date": "2024-10-12T16:16:46Z"}
{"title": "테스트 가능한 코드", "content": "## 테스트 주도 개발\n\n테스트 주도 개발을 하자는 이야기는 아닙니다. 테스트 주도 개발은 좋은 개발 방법론이라고 생각은 합니다. 하지만 제가 이번 글에서 말하고자 하는 것은 테스트 주도 개발이 아니라 테스트 가능한 코드를 작성하는 것입니다. 글이 끝나갈 때 쯤엔 마치 테스트 주도 개발이 옳다는 듯이 얘기하고 있을 가능성도 충분히 높습니다.\n\n## 테스트 가능한 코드\n\n테스트 가능한 코드는 충분히 우리가 컨트롤 할 수 있는 정도의 크기로 기능과 코드를 나누어 지고, 추상화되어 있어야 합니다. 그리고 이 작은 검증된 테스트들은 코드가 모여서 만들어진 더욱 상위의 테스트에 대한 검증을 수월하게 합니다. 그에 대한 몇가지 주제와 예시를 작성해보겠습니다.\n\n### 모듈 분리\n\n어떤 웹 서비스를 개발한다고 가정해봅시다. 여러분들은 가장 간단한 형태의 회원가입을 만들어야합니다. 이 회원가입 과정은 사용자에게서 이메일과 이름, 비밀번호를 받습니다. 그리고 입력받은 이메일에 랜덤하게 생성된 인증 번호를 발송하고, 사용자가 인증 번호를 입력하면 회원가입이 완료됩니다. 이 과정은 크게 2개의 유스케이스로 작성될 수 있습니다.\n\n1. 회원 가입 정보 입력\n1. 사용자 이메일 인증\n\n그러면 혹시, 설마, 그럴리 없겠지만, rest api로 만든다고 했을 때, 회원 가입 정보 입력을 `SignUp`이란 핸들러 하나를 만들고, 모든 로직을 거기에 다 넣으시진 않겠죠? 그렇게 작성하신다면, 단일 테스트 지점은 `SignUp`이란 핸들러가 되어 버립니다. 그럼 이건 테스트 가능한 코드일까요? \n\n회원 가입 과정 안에도 저는 최소한 아래 함수를 분리할 수 있다고 생각합니다.\n\n1. 이메일 검증: 올바른 이메일로 입력이 되었는 가\n2. 중복 이메일 검증: 이미 가입된 이메일인가\n3. 사용자 이름 검증: 서비스 정책에 맞는 이름인가\n4. 중복 사용자 이름 검증: 이미 가입된 사용자 이름인가\n5. 패스워드 검증: 서비스 정책에 맞는 패스워드인가\n\n필요에 따라, IP가 제재를 당한 기록이 있다거나, 이메일이 벤을 당한 적이 있다거나, 하는 부가적인 요소도 있을 수 있지만, 여기서는 이정도만 다루겠습니다.\n\n최소 5개의 위 함수들을 사용해서 `SignUp` 핸들러를 작성하면, 단일 테스트 지점은 `SignUp`이 아니라, 위 5개의 함수가 됩니다. 그리고 이 함수들이 테스트를 통과한다면, 우리는 `SignUp` 핸들러가 내부 로직적으로는 정상적으로 동작하니, 핸들러로써 제대로 동작하는 지만 테스트하면 됩니다. \n\n### 단일 책임 원칙\n\n모듈 분리와 일맥상통 하는 부분입니다. \n\n위와 동일하게 웹 서비스를 만드는 걸 가정해보겠습니다. 게시글, 댓글, 닉네임, 이메일, 등등 모든 부분에서 들어가서는 안되는 문자나 단어들을 필터링 해야하는 상황이 온다고 가정해봅시다. 회원가입할 때 필터링을 하는 코드를 같이 작성한다거나, 게시글을 등록할 때 필터링 하는 코드를 같이 작성할 수 있을 것입니다. 하지만 이렇게 되면 문자열 필터링도 각각 회원가입이나 게시글 등록 테스트 중에 파악해야합니다.\n\n당연하게도 이 상황에서 또한, 문자열 필터링에 대한 부분을 따로 분리합니다. 문자열 처리만을 다루는 패키지를 작성한다거나, 문자열 필터링을 담당하는 객체를 만들어서 단일 책임을 가지게 한다면 해당 책임에 대해서만 테스트를 하면 되니 훨씬 단일 테스트 지점이 작아지고 관리하기 쉬워집니다. 프로젝트 구성 전반이 책임을 잘 분리해서 구성되어 있다면, 새로운 책임과 그에 따른 테스트를 추가하는 것이 어렵지 않을 것입니다.\n\n### 의존성 주입\n\n다시 회원가입을 구현해보겠습니다. 아까는 작성하지 않았지만, 입력받은 정보를 RDB에 작성하기도 해야합니다. 또 이메일 인증 번호는 이메일을 키로, 인증 번호를 값으로 하며, TTL을 가지고 저장해 놓으면 되니, 레디스에 저장하도록 합니다. 그러면 `SignUp` 핸들러는 RDB와 레디스 커넥션을 어디에서 참조해야 할까요?\n\n가장 쉬운 방법은 전역 변수를 사용하는 것일 겁니다. 물론 프레임워크의 힘을 빌려서, asp .net core의 경우만 해도 싱글톤 객체를 서비스에 추가한 후 매번 자동으로 주입할 수 있습니다. 스프링이나 장고도 그러합니다. 하지만 모든 상황에서 그들의 힘을 빌릴 수는 없으니 로우하게 생각해보겠습니다.\n\n그래서, 전역 변수를 사용하면 테스트하기 어려워집니다. 덤으로 프로덕션에서 사이드 이펙트로 의도치 않은 상황에 죽을 가능성까지 안고 가야합니다. 회원가입이 정상적으로 동작하는 지 확인하기 위해 우리는 이 시점에 할 필요도 없는 RDB와 레디스의 정상 동작 여부까지, 직접 인스턴스를 띄워서 확인해야합니다.\n\n그럼 다시 생각해서, 회원가입할 때 우리는 RDB와 레디스에 쿼리를 요청할 수 있는 클라이언트들을 호출할 때나 핸들러를 감싸는 객체를 생성할 때 넣어줍니다. 패러미터로 클라이언트를 주입해주게 되면 각 테스트를 진행할 때, 외부 커넥션이나 객체의 실제 구현에 구애받지 않고 동작을 테스트할 수 있습니다.\n\n가장 일반적으로 해당 패러미터 타입의 가짜(Mock) 구현체를 넘겨주어, 대신 처리하도록 합니다. 예를 들어, 데이터베이스의 목 클라이언트의 경우엔 단순 인메모리에 값을 넣고 빼는 걸 할 수도 있죠.\n\n### 인터페이스 추상화\n\n인터페이스 추상화는 의존성 주입과 비슷한 내용입니다.\n\n회원가입 핸들러에 들어갈 RDB와 레디스 클라이언트를 다시 생각해보겠습니다. 당연히 raw query를 넘겨서 해결할 일은 없을 테고, `func (c *Client) SetUser(username string, email string, password string) error`같은 메서드를 만들어서 처리하게 될 것입니다. 레디스의 경우에도 `func (r *RedisClient) SetValidationCode(email string, code string, ttl time.Duration)`같은 형식으로 메서드를 만들 겁니다.\n\n그럼 의존성 주입으로 처리해서 테스트할 때는 회원가입 핸들러에 가짜 클라이언트를 넣어주는데, 어떤 식으로 넣어주어야 하는지가 걸리게 됩니다. 실 구현체의 타입을 아무런 추상화 없이 패터미터에 넣고, 다른 가짜 구현체를 대신 넣기는 어렵습니다. 패러미터가 함수 시그니처로 되어 있고, 일급 객체나 함수 포인터를 넣어준다면 가능하겠지만요.\n\n하지만 고수준 언어를 사용하는 입장에서 좀 더 우아하게 가짜 구현체를 넣어주고 싶습니다. 그렇게 하기 위해 존재하는 것이, 인터페이스 추상화라고 생각합니다. 특정 동작을 하는(메서드를 가지고 있는) 구현체라면 모두 해당 인터페이스를 구현하고 있기에 문제 없다라고 보는 것입니다. 저는 덕 타이핑 영역으로만 글을 쓰겠습니다.\n\n```go\ntype DatabaseClient interface {\n    SetUser(username string, email string, password string) error\n    GetUserByEmail(email string) (username string, password string, err error)\n    GetUserByName(username string) (email string, password string, err error)\n    RemoveUserByEmail(email string) error\n    RemoveUserByName(username string) error\n}\n```\n\n그 어떤 데이터베이스 커넥션을 가진 클라이언트든 위 `DatabaseClient` 인터페이스만 구현하면 인터페이스에 정의된 메서드를 호출하여 처리할 수 있습니다. 그러면 진짜 구현체라면 내부에 외부 데이터베이스에 대한 커넥션을 가지고 실제로 처리하도록 작업하면 되고, 테스트를 위한 가짜 구현체라면 내부에 `map[string]User`같은 해시맵 형식의 자료구조를 사용하여 처리할 수도 있습니다.\n\n## 이거 완전...\n\nOOP의 5 principles와 유사하지 않나요?\n\n저는 객체지향의 5가지 원칙이 특정 분야를 제외하고, 일반적인 프로젝트를 구성하는 대부분의 경우의 기초라고 생각합니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/016_testable_code/", "created_date": "2023-04-01T20:08:12Z"}
{"title": "러스트로 알고리즘 테스트를 풀어봅시다", "content": "## 왜 러스트?\n\n개인적으로 러스트로 알고리즘 문제를 푸는 건 좋은 선택이라 생각합니다.\n\n1. 아래의 다양한 자료구조를 빌트인으로 제공합니다.\n   1. Vector\n   2. VecDeque\n   3. LinkedList\n   4. HashMap\n   5. HashSet\n   6. BtreeMap\n   7. BtreeSet\n   8. BinaryHeap\n2. 유용한 이터러블과 빌트인 고차함수들이 있습니다.\n   1. 일단 이터러블로 변환하면 뭐든지 가능하다는 장점이 있습니다.\n   2. 이 과정은 당연히 정적 타입으로 이루어지기에 실수가 줄어듭니다.\n3. 튜플과 패턴 매칭 문법이 훌륭하고 표현식입니다.\n   1. `()`로 만드는 튜플은 어떤 타입, 어떤 길이로도 가능하여 유용합니다.\n   2. `match` 문법은 튜플조차 패턴 매칭을 해내기에 어떤 조건도 직관적으로 표현할 수 있습니다.\n   3. `if`, `match`, 심지어 `loop`가 값을 반환할 수 있어 유연하게 작성할 수 있습니다.\n4. 메모리를 효율적으로 재사용하고 반납합니다.\n   1. 메모리를 어느정도 막 써도 용서받을 수 있습니다.\n   2. 알고리즘 테스트 과정에서 메모리로 인한 실패 가능성이 줄어듭니다.\n5. 소유권에 익숙하면 좋지만 그렇지 않아도 좋습니다.\n   1. C++의 `&`(레퍼런스) 정도로 알고 있어도 무리가 없습니다.\n   2. 너무 큰 객체가 아니라면 `clone()` 메서드로 복사값을 넘겨줘도 됩니다.\n   3. 솔직히 어지간하면 소유권으로 에러가 나는 케이스가 적습니다.\n\n---\n\n## 변수 할당\n\n### 불변(immutable)\n\n```rust\nlet a = 123;\n```\n\n러스트는 기본적으로 할당되는 변수는 불변입니다. 해당 변수, `a`는 값을 변경할 수 없습니다. `a = a + 3`나 `a = 100`같은 재할당을 할 수 없습니다. 이 불변성은 변수에 할당된 값에도 동일하게 적용됩니다. 예를 들어 `Vector`나 `HashMap`을 할당하게 되면 값을 추가하거나 삭제할 수 없게 됩니다.\n\n### 가변(mutable)\n\n```rust\nlet mut a = 123;\n```\n\n`mut` 키워드를 추가하게 되면 변수가 가변 상태가 됩니다. 해당 변수, `a`는 위의 경우와 달리 값을 변경할 수 있습니다. `a = a + 3`이나 `a = 100`을 수행할 수 있습니다. 당연하게도 `Vector`나 `HashMap` 등 자료구조에 대해서도 값을 추가하거나 삭제할 수 있게 됩니다. \n\n---\n\n## 패턴 매칭\n\n### Option<T>\n\n`Option<T>`은 타입 `T`를 가질 수도 있는 객체입니다. `Some(value)` 함수를 통해 입력받은 값을 가지는 `Option<T>` 객체를 반환합니다. `Some()` 대신 `None`을 반환하면 값을 가지지 않는 `Option<T>` 객체를 반환합니다.\n\n### Result<T, R>\n\n`Result<T, R>`은 타입 `T`와 `R`을 각각 결과값과 에러값으로 가지는 객체입니다. `Ok(value)` 함수를 통해 입력받은 값을 결과값으로 가지는 `Result<T, R>`을 반환합니다. `Err(value)` 함수를 사용하면 입력받은 값을 에러값으로 가지는 `Result<T, R>`을 반환합니다. \n\n### Tuple\n\n`()`을 사용하여 값을 감싸게 되면 복합 타입으로 이루어진 튜플을 생성하고 사용할 수 있습니다.\n\n```rust\nlet a = (1, 2.2, \"str\")\n```\n\n이렇게 작성하게 되면 `a`는 `(i32, f32, &str)` 타입을 가지게 됩니다. 길이나 타입은 원하는 대로 설정할 수 있습니다. 각 원소에 대해서는 `a.0`, `a.1`처럼 인덱스를 직접 호출하여 접근할 수 있습니다. 가변 변수로 선언했을 경우 각 원소의 값을 수정할 수도 있습니다.\n\n### match\n\n`match` 문법은 다른 언어의 `switch`나 `when`과 비슷한 역할을 합니다. 하지만 러스트의 `match`는 그보다 훨씬 풍부한 기능을 가지고 있으며 유용하게 쓰입니다.\n\n#### Option<T>\n\n```rust\nfn main() {\n    let o = Some(4);\n    match o {\n        Some(i) => println!(\"{}\", i),\n        None => println!(\"none\"),\n    }\n\n    let o: Option<i32> = None;\n    match o {\n        Some(i) => println!(\"{}\", i),\n        None => println!(\"none\"),\n    }\n}\n```\n\n`match`는 `Option<T>`의 `Some`과 `None`을 분해할 수 있습니다. `Some(value)` 분기를 통해 값이 있다면 값을 받아오고 `None` 분기를 통해 값이 없을 때의 처리를 할 수 있습니다.\n\n#### Result<T, R>\n\n```rust\nfn main() {\n    let r: Result<i32, String> = Ok(42);\n    match r {\n        Ok(v) => println!(\"{}\", v + 2),\n        Err(e) => println!(\"{}\", e),\n    }\n\n    let r: Result<i32, String> = Err(\"Error message\".to_string());\n    match r {\n        Ok(v) => println!(\"{}\", v + 2),\n        Err(e) => println!(\"{}\", e),\n    }\n}\n```\n\n`Result<T, R>`의 경우도 `Option<T>`와 마찬가지로 작성할 수 있습니다. `Ok(value)` 분기를 통해서 결과값에 해당하는 값의 처리를 할 수 있고,`Err(value)` 분기를 통해서 에러값에 해당하는 값의 처리를 할 수 있습니다.\n\n#### tuple\n\n```rust\nfn main() {\n    let r: (Option<usize>, Option<String>) = (Some(3), Some(\"boo\".to_string()));\n    match r {\n        (Some(x), Some(y)) => println!(\"{}\", y.repeat(x)),\n        (None, Some(y)) => println!(\"{}\", y.repeat(1)),\n        (Some(x), None) => println!(\"{}\", \"foo\".repeat(x)),\n        (None, None) => println!(\"{}\", \"foo\".repeat(1)),\n    }\n}\n```\n\n복합 `Option<T>` 타입을 가지는 튜플을 `match`에 적용했습니다. 각 4가지 경우인 Some+Some, Some+None, None+Some, None+None를 작성한 케이스입니다. 다른 언어에서는 중첩 `switch`나 여러 조건문의 `if`를 여러개 나열했어야 하지만 러스트의 `match`는 튜플을 이용하여 깔끔하게 처리할 수 있습니다.\n\n```rust\nfn main() {\n    let r: (Option<usize>, Option<String>) = (Some(3), Some(\"boo\".to_string()));\n    match r {\n        (Some(x), Some(y)) => println!(\"{}\", y.repeat(x)),\n        (_, _) => println!(\"{}\", \"foo\".repeat(1)),\n    }\n}\n```\n\n만약 특정 튜플 원소에 대해 모든 경우의 수를 모두 수용하고 싶다면 `_`로 표기할 수 있습니다. \n\n---\n\n## 자료 구조\n\n### 벡터(Vector)\n\n```rust\nlet v = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n```\n\n벡터는 C++의 Vector, java의 ArrayList, Go의 Slice처럼 미리 기반이 되는 배열을 만들어놓고 값이 추가될 때마다 기반 배열의 용량을 넘어서게 되면 더 큰 새로운 배열을 만들어 사용하는 방식입니다. 이 방식은 보편적으로 좋은 성능을 가집니다.\n\n#### 스택\n\n벡터의 가장 단순한 활용 형태는 스택입니다.\n\n```rust\nfn main() {\n    let mut v = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n    v.push(11);\n    while !v.is_empty() {\n        let last = v.pop().unwrap();\n        println!(\"{}\", last);\n    }\n}\n```\n\n단순한 형태의 스택 순회입니다. `push()` 메서드로 벡터의 끝에 값을 추가합니다. `while` 반복문의 조건에 `is_empty()` 메서드를 넣어 벡터가 빌 때까지 반복합니다. `pop()` 메서드를 통해 벡터의 마지막 값의 `Option` 객체를 받아 출력합니다.\n\n이때 사용된 `Option<T>`의 `unwrap()`은 옵션 객체가 `Some`일 경우 값을 반환하고, `None`일 경우 런타임 패닉이 발생합니다. 알고리즘 테스트의 경우 `None`이 예상치 못한 곳에서 발생할 가능성이 적으니 부담없이 사용할 수 있습니다.\n\n#### 정렬 및 이진 탐색\n\n```rust\nfn main() {\n    let mut v = vec![7, 3, 5, 4, 1, 2, 9, 10, 6, 8];\n    v.sort();\n    println!(\"sorted: {:?}\", v);\n}\n```\n\n러스트는 벡터가 가변이고 내부 타입이 비교 가능할 경우 `sort()` 메서드를 사용하여 벡터를 정렬할 수 있습니다. 위 예시를 실행하면 `sorted: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`로 정렬된 벡터를 출력함을 확인할 수 있습니다.\n\n```rust\nfn main() {\n    let v = vec![1, 2, 3, 4, 6, 7, 8, 9, 10, 11];\n    \n    match v.binary_search(&4) {\n        Ok(index) => println!(\"found 4 at index {}\", index),\n        Err(index) => println!(\"4 is not found, but index is {}\", index),\n    }\n\n    match v.binary_search(&5) {\n        Ok(index) => println!(\"found 5 at index {}\", index),\n        Err(index) => println!(\"5 is not found, but index is {}\", index),\n    }\n}\n```\n\n```rust\nfound 4 at index 3\n5 is not found, but index is 4\n```\n\n정렬된 벡터일 경우 `binary_search()` 메서드를 사용하여 값을 이진 탐색할 수 있습니다. 각각 4와 5를 탐색했을 때, 4는 존재하는 값이기에 인덱스 값인 3을 `Ok`를 통해 반환했고, 5는 존재하지 않는 값이기에 삽입(`insert`)하면 되는 인덱스인 4를 `Err`를 통해 반환했습니다.\n\n```rust\nfn main() {\n    let mut v = vec![1, 2, 3, 4, 6, 7, 8, 9, 10, 11];\n    \n    v.insert(4, 5);\n    println!(\"inserted: {:?}\", v);\n}\n```\n\n벡터가 가변일 경우에 `insert(index, value)` 메서드를 통해 해당 인덱스에 값을 추가하여 정렬 상태를 유지할 수 있습니다.\n\n### 덱(VecDeque)\n\n```rust\nuse std::collections::VecDeque;\n\nfn main() {\n    let mut a = VecDeque::new();\n}\n```\n\n덱은 벡터와 비슷하지만 맨 처음과 마지막에 값을 추가 삭제할 때 코스트가 더 싸지만 순회는 더 비싸다는 특징을 가지고 있습니다. `use std::collections::VecDeque`으로 `VecDeque`를 가져와야 사용할 수 있습니다. 다른 구조체와 마찬가지로 `::new()` 메서드로 새로운 객체를 만들 수 있습니다.\n\n#### 큐\n\n```rust\nuse std::collections::VecDeque;\n\nfn main() {\n    let mut a = VecDeque::new();\n    a.extend(vec![1, 2, 3, 4, 5].iter());\n\n    a.push_back(6);\n    let first = match a.pop_front() {\n        Some(x) => x,\n        None => 0,\n    };\n    println!(\"{}\", first);\n}\n```\n\n덱을 가변으로 선언하고 `push_back(value)` 메서드를 사용하면 덱의 마지막에 값을 추가할 수 있습니다. 그리고 `pop_front()` 메서드를 통해 덱의 맨 앞의 값의 `Option<T>`을 받습니다. 이렇게 덱의 맨 뒤에 넣고 맨 앞에서 빼는 방식으로 간단한 큐를 구현할 수 있습니다. 반대로 `push_front(value)`와 `pop_back()` 메서드를 사용하면 맨 앞에 넣고 맨 뒤에서 빼는 형식의 큐를 만들 수 있습니다.\n\n#### 스택\n\n```rust\nuse std::collections::VecDeque;\n\nfn main() {\n    let mut a = VecDeque::new();\n    a.extend(vec![1, 2, 3, 4, 5].iter());\n\n    a.push_back(6);\n    let first = match a.pop_back() {\n        Some(x) => x,\n        None => 0,\n    };\n    println!(\"{}\", first);\n}\n```\n\n스택의 예시에서 조금 틀어서 `push_back(value)`로 맨 끝에 값을 넣고 `pop_back()`으로 맨 끝에서 값을 꺼내면 덱의 마지막에 넣고 빼는 스택이 됩니다. 반대로 `push_front(value)`와 `pop_front()` 메서드를 사용하면 맨 앞에서 넣고 빼는 스택이 됩니다.\n\n### 이진 힙(BinaryHeap)\n\n```rust\nuse std::collections::BinaryHeap;\n\nfn main() {\n    let mut heap: BinaryHeap<i32> = BinaryHeap::new();\n    heap.extend(vec![1, 9, 2, 8, 3, 7, 4, 6, 5].iter());\n    while !heap.is_empty() {\n        print!(\"{:?} \", heap.pop().unwrap());\n    }\n}\n```\n\n이진 힙은 벡터를 베이스로 동작하는 우선순위 큐입니다. 이진 힙도 덱과 마찬가지로 `::new()` 메서드로 새로운 인스턴스를 생성합니다. 순서대로 `1, 9, 2, 8, 3, 7, 4, 6, 5`을 입력합니다. 러스트의 이진 힙은 최대 값을 먼저 반환하는 최대 힙입니다. 위 예시의 출력은 `9 8 7 6 5 4 3 2 1`입니다.\n\n#### 최소 힙\n\n```rust\nuse std::{collections::BinaryHeap, cmp::Reverse};\n\nfn main() {\n    let mut heap = vec![1, 9, 2, 8, 3, 7, 4, 6, 5].into_iter().map(Reverse).collect::<BinaryHeap<_>>();\n    while !heap.is_empty() {\n        print!(\"{:?} \", heap.pop().unwrap().0);\n    }\n}\n```\n\n이진 힙은 타입에 정의된 비교 메서드를 통해 이루어지는데, 이 비교를 역전하여 적용할 수 있는 `Reverse`라는 구조체가 존재합니다. 벡터를 기반으로 이터레이터를 만들고 모든 원소에 `map()` 메서드를 통해 `Reverse` 구조체를 적용하여 `Reverse<T>` 인스턴스의 이터레이터로 변환합니다. 이후 `collect::<BinaryHeap<_>>()` 메서드로 `BinaryHeap<Reverse<T>>` 인스턴스로 수집합니다.\n\n```rust\nuse std::{collections::BinaryHeap, cmp::Reverse};\n\nfn main() {\n    let mut heap = BinaryHeap::new();\n    heap.push(Reverse(5));\n    heap.push(Reverse(4));\n    heap.push(Reverse(3));\n    heap.push(Reverse(2));\n    heap.push(Reverse(1));\n\n    while let Some(Reverse(x)) = heap.pop() {\n        print!(\"{} \", x);\n    }\n}\n```\n\n이번에는 처음의 예처럼 일반적인 방법으로 이진 힙을 만듭니다. 이후 `Reverse` 구조체로 감싼 값을 추가합니다. 그럼 모든 비교가 역전되어 적용되어 이진 힙이 최소 힙으로 만들어집니다. \n\n### HashMap, BTreeMap\n\n`HashMap`은 SipHash 1-3 해시 알고리즘을 기반으로 만들어진 해시 맵 자료구조입니다.  \n`BTreeMap`은 B-Tree를 기반으로 만들어진 트리 맵 자료구조입니다.\n\n#### 추가\n\n```rust\nuse std::collections::{HashMap, BTreeMap};\n\nfn main() {\n    let mut m = HashMap::new();\n    m.insert(0, \"zero\");\n    m.insert(1, \"one\");\n    m.insert(2, \"two\");\n\n    for (k, v) in &m {\n        println!(\"{} -> {}\", k, v);\n    }\n\n    let mut m = BTreeMap::new();\n    m.insert(0, \"zero\");\n    m.insert(1, \"one\");\n    m.insert(2, \"two\");\n\n    for (k, v) in &m {\n        println!(\"{} -> {}\", k, v);\n    }\n}\n```\n\n`insert(key, value)` 메서드는 값이 없을 때는 키와 값 페어를 추가하고, 이미 키가 맵에 있을 경우 키에 해당하는 값을 변경합니다. 또한 `HashMap`과 `BTreeMap`은 위 예시에서 보듯이 키와 값을 추가하는 메서드가 동일합니다. 이는 다른 메서드들, 조회나 삭제도 동일합니다.\n\n```rust\nuse std::collections::HashMap;\n\n\nfn main() {\n    let mut m = HashMap::new();\n\n    match m.get_mut(&'b') {\n        Some(v) => {\n            *v += 1;\n        },\n        None => {\n            m.insert('b', 1);\n        },\n    }\n\n    let v = m.entry('a').or_insert(0);\n    *v += 1;\n\n    println!(\"{:?}\", m);\n}\n```\n\n어떤 숫자나 문자의 개수를 세는 경우, 맵의 기본 값이 0으로 초기화 되어 있고 없는 값은 추가하면서 1을 설정하는 경우가 있습니다. 전통적인 방법으로는 `'b'`를 카운팅할 때처럼 값을 받아오고 있으면 1을 더하고 없으면 1을 설정하는 방식을 쓰겠지만, `entry(key).or_insert(default)` 메서드를 사용하면 쉽게 처리할 수 있습니다. 키가 존재하면 원래 값을 가변으로 가져오고 없다면 `default` 값으로 추가한 후, 맵의 값을 가변으로 가져옵니다. 저희가 할 일은 이렇게 나온 가변 변수에 1을 더하는 것 뿐입니다.\n\n#### 조회\n\n```rust\nuse std::collections::HashMap;\n\nfn main() {\n    let mut m: HashMap<_, _> = HashMap::from([(1, 2), (2, 3), (3, 4)]);\n\n    println!(\"contains key 2: {}\", m.contains_key(&2));\n\n    match m.get(&1) {\n        Some(v) => println!(\"{} of key 1\", v),\n        None => println!(\"not contains\"),\n    }\n\n    println!(\"keys: {:?}\", m.keys());\n\n    println!(\"values: {:?}\", m.values());\n\n    print!(\"for loop: \");\n    for (k, v) in &m {\n        print!(\"{} -> {}, \", k, v);\n    }\n    println!();\n}\n```\n\n두 맵은 동일한 메서드를 사용하므로 `HashMap`만 사용하였습니다. `contains_key(&key)`는 입력받은 키가 존재하는 지를 반환합니다. 당연하게도 있다면 `true`를 반환합니다. `get(&key)` 메서드는 입력받은 키에 해당하는 값의 `Option<T>` 객체를 반환합니다. `Some(value)` 분기로 빌린 값에 접근할 수 있고, `None` 분기를 통해 없을 때의 동작을 정의할 수 있습니다.\n\n#### 삭제\n\n```rust\nuse std::collections::HashMap;\n\nfn main() {\n    let mut m: HashMap<_, _> = HashMap::from([(1, 2), (2, 3), (3, 4)]);\n\n    match m.remove(&2) {\n        Some(x) => println!(\"{} of key 2\", x),\n        None => println!(\"not exists\"),\n    };\n\n    println!(\"{:?}\", m);\n\n    m.clear();\n\n    println!(\"{:?}\", m);\n}\n```\n\n`remove(&key)` 메서드를 사용하여 맵에 저장된 키와 값을 삭제합니다. 결과는 `Option<T>`로 반환하며 성공했을 경우, `Some(value)` 분기에서 값을 받아서 다룰 수 있으며, 실패했을 경우 `None` 분기에서 적절한 처리를 할 수 있습니다. `clear()` 메서드는 맵 내부 데이터를 모두 삭제합니다.\n\n### HashSet, BTreeSet\n\n`HashSet`은 해시 맵과 마찬가지로 해시로 동작하는 해시 셋십니다.  \n`BTreeSet`은 비트리 맵과 마찬가지로 B-Tree를 기반으로 동작하는 트리 셋입니다.\n\n#### 추가\n\n```rust\nuse std::collections::HashSet;\n\nfn main() {\n    let mut s = HashSet::new();\n    s.insert(1);\n    s.insert(3);\n    s.insert(5);\n}\n```\n\n`insert(value)` 메서드를 통해 값을 셋에 추가할 수 있습니다. 이 때, `true`가 반환되면 셋에 존재하지 않았다가 추가된 것이고, `false`가 반환되면 셋에 존재하고 있는 값을 추가한 것입니다.\n\n#### 조회\n\n```rust\nuse std::collections::HashSet;\n\nfn main() {\n    let s = HashSet::from([1, 2, 3, 4, 5]);\n    println!(\"key 5 is exists? {:?}\", s.contains(&5));\n    println!(\"key 6 is exists? {:?}\", s.contains(&6));\n}\n```\n\n`contains(&value)` 메서드를 통해 값이 존재하는 지 확인할 수 있습니다.\n\n#### 삭제\n\n```rust\nuse std::collections::HashSet;\n\nfn main() {\n    let mut s = HashSet::from([1, 3, 5, 7, 9, 11]);\n    \n    println!(\"5 is removed? {}\", s.remove(&5));\n    println!(\"13 is removed? {}\", s.remove(&13));\n}\n```\n\n`remove(&value)` 메서드를 통해 특정 값을 삭제할 수 있습니다. 반환값은 `bool` 타입으로 존재한 것을 삭제했을 경우 `true`를 반환하고, 존재하지 않는 값을 삭제하려고 했을 경우 `false`를 반환합니다.\n\n#### 집합 연산\n\n```rust\nuse std::collections::HashSet;\n\nfn main() {\n    let a = HashSet::from([1, 3, 5, 7, 9, 11]);\n    let b = HashSet::from([1, 2, 3, 4, 5, 6]);\n\n    println!(\"intersection: {:?}\", a.intersection(&b).collect::<HashSet<_>>());\n\n    println!(\"union: {:?}\", a.union(&b).collect::<HashSet<_>>());\n\n    println!(\"subtract: {:?}\", a.difference(&b).collect::<HashSet<_>>());\n\n    println!(\"symmetric_difference: {:?}\", a.symmetric_difference(&b).collect::<HashSet<_>>());\n\n    println!(\"is_disjoint: {}\", a.is_disjoint(&b));\n\n    println!(\"is_subset: {}\", a.is_subset(&b));\n\n    println!(\"is_superset: {}\", a.is_superset(&b));\n\n    println!(\"is_empty: {}\", a.is_empty());\n}\n```\n\n```bash\nintersection: {1, 5, 3}\nunion: {4, 3, 2, 6, 7, 11, 5, 9, 1}\nsubtract: {9, 11, 7}\nsymmetric_difference: {2, 9, 7, 11, 6, 4}\nis_disjoint: false\nis_subset: false\nis_superset: false\nis_empty: false\n```\n\n`intersection(&other)`, `union(&other)`, `difference(&other)`, `symmetric_difference(&other)` 메서드는 각각 교집합, 합집합, 차집합, 대칭 차집합을 의미합니다. 이 네 가지 메서드들은 각각 `collect()` 메서드로 특정 자료구조로 수집할 수 있습니다.\n\n`is_disjoint(&other)`, `is_subset(&other)`, `is_superset(&other)`는 각각 서로소 집합인지, `other`의 부분집합인지, `other`가 부분집합인지를 반환합니다.\n\n### 문자열(String)\n\n러스트에는 수 많은 문자열이 있지만 `String` 구조체만 생각합니다. `String`은 내부적으로 UTF8 인코딩을 따르고 있기에 범용성이 좋습니다.\n\n```rust\nfn main() {\n    let from_str = String::from(\"hello\");\n    let new_constuctor = String::new();\n    let from_literal = \"hello\".to_string();\n}\n```\n\n`String` 구조체는 `::from(&str)` 메서드로 문자열 리터럴을 받아 생성하는 방법, `&str`의 `to_string()` 메서드로 반환받는 방법, `::new()` 생성자로 생성하는 방법, 3가지가 존재합니다. \n\n#### 추가\n\n러스트의 `String`은 바이트 배열로 이루어져 있지만 UTF8 인코딩을 기본으로 하고 있기에 스트링 리터럴 및 바이트 접근과 `char` 타입을 통한 UTF8 접근이 가능합니다. \n\n```rust\nfn main() {\n    let mut s = String::new();\n\n    s.push_str(\"아이스아메리카노\");\n    s.push(char::from('리'));\n\n    println!(\"{}\", s);  \n}\n```\n\n`push_str(&str)` 메서드는 입력받은 문자열 리터럴을 단순하게 뒤에 붙입니다.  \n`push(char)` 메서드는 `char` 타입 변수 하나를 뒤에 추가합니다. 러스트에서는 UTF8 인코딩의 한 글자를 `char`로 표기합니다.\n\n #### 조회\n\n ```rust\nfn main() {\n    let mut s = String::from(\"qpsdosssxclvwemfwjkbssscbzxyugadasdefsssdfv\");\n\n    match s.find(\"sss\") {\n        Some(index) => println!(\"Found 'sss' at index {}\", index),\n        None => println!(\"Didn't find 'sss'\"),\n    }\n\n    for (i, v) in s.matches(\"sss\").enumerate() {\n        println!(\"{}: {}\", i, v);\n    }\n\n    println!(\"'sss' is contains? {}\", s.contains(\"sss\"));\n}\n ```\n\n`String` 구조체의 조회 방법은 크게 3가지가 있습니다. `find(pattern)`, `matches(pattern)`, `contains(pattern)` 메서드는 문자열 리터럴, `char` 타입 문자, `char` 타입 배열, 혹은 `char` 타입 문자를 받아 판별하는 함수를 입력 받아 해당 패턴에 맞는 결과를 반환합니다.\n\n일반적인 문자열 리터럴과 `char` 문자, `char` 배열이 아닌 `char` 타입 문자를 판별하는 함수는 러스트에서 기본적으로 몇가지 제공해주고 있습니다.\n\n```rust\nfn main() {\n    let mut s = String::from(\"A little copying is better than a little dependency.\");\n\n    let pat = char::is_lowercase;\n    let pat = char::is_uppercase;\n    let pat = char::is_alphabetic;\n    let pat = char::is_numeric;\n    let pat = char::is_alphanumeric;\n    let pat = char::is_whitespace;\n    let pat = char::is_control;\n\n    match s.find(pat) {\n        Some(i) => {\n            println!(\"Found at {}\", i);\n        }\n        None => {\n            println!(\"No found!\");\n        }\n    }\n\n    for (i, v) in s.matches(pat).enumerate() {\n        println!(\"{}: {}\", i, v);\n    }\n\n    println!(\"contains? {}\", s.contains(pat));\n}\n```\n\n각각 `is_lowercase`, `is_uppercase`, `is_alphabetic`, `is_numeric`, `is_alphanumeric`, `is_whitespace`, `is_control`은 유니코드 상에서 소문자인지, 유니코드 상에서 대문자인지, 글자인지, 숫자인지, 문자 혹은 숫자인지, 공백인지, 컨트롤 문자인지 반환합니다. 이 함수들을 패턴으로 입력함으로 원하는 패턴을 검색할 수 있습니다.\n\n#### 삭제, 교체\n\n```rust\nfn main() {\n    let mut s = String::from(\"A little copying is better than a little dependency.\");\n\n    let result = s.replace(char::is_whitespace, \"\");\n\n    println!(\"{}\", result);\n}\n```\n\n다른 언어와 마찬가지로 `replace(from, to)` 메서드로 제거합니다. `from` 매개변수는 위 `char`의 함수들도 사용할 수 있습니다. \n\n```rust\nfn main() {\n    let mut s = String::from(\"A little copying is better than a little dependency.\");\n\n    let result = s.remove(0);\n\n    println!(\"{}\", result);\n}\n```\n\n`remove(index)` 메서드는 `String` 변수가 가변일 때 사용할 수 있습니다. 입력된 인덱스에 해당하는 문자를 삭제하고 반환합니다. 만약 인덱스가 문자열 전체 길이를 넘어갔을 경우 런타임 패닉이 발생하기 때문에 주의해서 사용해야합니다.\n\n---\n\n## Iterator\n\n러스트는 거의 모든 자료구조가 이터레이터로 변환되어 동작할 수 있습니다. 다른 자료구조들, 벡터, 덱, 셋은 `iter()` 메서드나 `into_iter()` 메서드로 이터레이터를 만들 수 있습니다. `iter()` 메서드는 내부 값들을 빌려주기 때문에 기존 자료구조를 그대로 재활용할 수 있습니다. 하지만 `into_iter()`의 경우 내부 값들을 이동 시키기 때문에 기존 자료구조를 재활용할 수 없습니다.\n\n```rust\nfn main() {\n    let v = vec![1, 2, 3, 4, 5];\n\n    let a = v.iter().map(|x| x + 1).collect::<Vec<_>>();\n\n    println!(\"prev: {:?}\", v);\n    println!(\"new: {:?}\", a);\n}\n```\n\n이터레이터를 활용하여 벡터의 모든 원소에 1을 더한 새로운 벡터를 생성하는 간단한 코드입니다. 이전 변수도 그대로 활용할 수 있습니다.\n\n```rust\nfn main() {\n    let mut v = vec![1, 2, 3, 4, 5];\n\n    v.iter_mut().for_each(|x| *x += 1);\n\n    println!(\"prev: {:?}\", v);\n}\n```\n\n이번엔 `iter_mut()` 메서드를 이용해 가변 자료구조 변수의 값을 가변 변수로 빌렸습니다. 포인터 참조를 하듯이 `*`을 사용하여 1을 더함으로 모든 벡터의 값이 1이 더해져 있음을 확인할 수 있습니다.\n\n```rust\nfn main() {\n    let v = vec![1, 2, 3, 4, 5];\n\n    let a  = v.into_iter().map(|x| x + 1).collect::<Vec<_>>();\n\n    println!(\"prev: {:?}\", v); // error\n    println!(\"new: {:?}\", a);\n}\n```\n\n이 코드는 실행되지 않습니다. `into_iter()` 메서드는 내부 값을 모두 이터레이터에게 빌려주지 않고 그냥 주기 때문에 기존 자료구조 변수를 재사용할 수 없습니다. `v`를 출력하려고 할 때 에러가 납니다. 대신 이터페리터 연산 시 거추장스러운 `*`을 보지 않아도 됩니다.\n\n```rust\nfn main() {\n    let v = vec![1, 2, 3, 4, 5];\n\n    let v  = v.into_iter().map(|x| x + 1).collect::<Vec<_>>();\n\n    println!(\"prev: {:?}\", v);\n}\n```\n\n마지막으로 `into_iter()` 메서드를 사용할 때는 이렇게 같은 이름의 새로운 변수로 할당하는 방식을 쓰기도 합니다. 이를 러스트에서는 쉐도잉(shadowing)이라고 합니다. 이렇게 사용하게 되면 기존의 `v`는 접근할 수 없게 되고 새로 만든 `v`만 접근할 수 있게 되어 자연스럽게 쓸 수 있습니다. 주의할 점은 쉐도잉된 기존 `v`는 사라지는게 아니라 가려지는 것이기 때문에 의도치 않게 메모리를 낭비할 수 있습니다.\n\n### 생성 패턴\n\n이터레이터는 다양한 자료구조 구조체에서 만들 수 있기 때문에 다양한 생성 패턴이 존재합니다.\n\n#### Vector\n\n1. `drain(range)`: 범위에 해당하는 값들을 벡터에서 추출하여 이터레이터로 반환합니다.\n2. `drain(filter)`: 필터 함수를 만족하는 값들을 벡터에서 추출하여 이터레이터로 반환합니다.\n3. `chunk(size)`: 입력받은 크기마다 값을 나누어 이터레이터로 반환합니다.\n4. `rchunk(size)`: `chunk(size)` 메서드의 동작을 끝에서부터 수행하여 반환합니다.\n5. `group_by(comparer)`: `comparer`에 순차적으로 값을 넣어 연속적으로 `true`가 나온 값들을 하나의 그룹으로 엮은 이터레이터를 반환합니다.\n6. `split(pred)`: `pred` 함수가 `true`를 반환하는 값을 기준으로 벡터를 잘라 이터레이터로 반환합니다.\n\n#### VecDeque\n\n1. `drain(range)`: 범위에 해당하는 값들을 덱에서 추출하여 이터레이터로 반환합니다.\n2. `range(range)`: 범위에 해당하는 값들의 이터레이터를 반환합니다.\n\n#### HashMap, BTreeMap\n\n1. `keys()`: 맵의 모든 키를 빌린 이터레이터를 반환합니다.\n2. `into_keys()`: 맵의 모든 키를 위임 받은 이터레이터를 반환합니다.\n3. `values()`: 맵의 모든 값을 빌린 이터레이터를 반환합니다.\n4. `into_values()`: 맵의 모든 값을 위임 받은 이터레이터를 반환합니다.\n5. `drain()`: 맵의 모든 키와 값 쌍을 위임 받은 이터레이터를 반환합니다.\n6. `drain_filter(filter)`: 필터 함수에 걸러지는 모든 맵의 키와 값 쌍을 위임 받은 이터레이터를 반환합니다.\n\n#### HashSet, BTreeSet\n\n1. `drain()`: 셋의 값을 위임 받은 이터레이터를 반환합니다.\n2. `drain_filter(filter)`: 필터 함수에 걸러지는 모든 셋의 값을 위임 받은 이터레이터를 반환합니다.\n3. `difference(&other)`: 차집합의 값들을 이터레이터로 반환합니다.\n4. `symmetric_difference(&other)`: 대칭 차집합의 값들을 이터레이터로 반환합니다.\n5. `intersection(&other)`: 교집합의 값들을 이터레이터로 반환합니다.\n6. `union(&other)`: 합집합의 값들을 이터레이터로 반환합니다.\n\n#### String\n\n1. `bytes()`: 문자열의 바이트(`u8`) 타입 값들을 이터레이터로 반환합니다.\n2. `chars()`: 문자열의 UTF8 캐릭터(`char`) 값들을 이터레이터로 반환합니다.\n3. `char_indices()`: 문자열의 UTF8 캐릭터 인덱스를 이터레이터로 반환합니다.\n4. `drain(range)`: 범위에 해당하는 값들을 문자열에서 추출하여 이터레이터로 반환합니다.\n5. `split_whitespace()`: 문자열을 공백 기준으로 나누어 이터레이터로 반환합니다.\n6. `split_ascii_whitespace()`: 문자열을 아스키 공백 기준으로 나누어 이터레이터로 반환합니다.\n7. `lines()`: 문자열을 줄바꿈 기준으로 나누어 이터레이터로 반환합니다.\n8. `encode_utf16()`: 문자열을 UTF16으로 인코딩하여 이터레이터로 반환합니다.\n9. `split(pat)`: 문자열을 패턴 기준으로 나누어 이터레이터로 반환합니다.\n10. `split_inclusive(pat)`: 문자열을 패턴 기준으로 나누어 반환합니다. 패턴은 삭제되지 않고 앞 문자열의 마지막에 포함됩니다.\n11. `split_terminator(pat)`: `split(pat)`과 같은 동작을 수행하지만 마지막 값이 공백 문자열일 경우 제거합니다.\n12. `rsplit_terminator(pat)`: `split_terminator(pat)`과 같은 동작을 수행하지만 끝에서부터 시작합니다.\n13. `matches(pat)`: 패턴에 해당하는 문자열을 이터레이터로 반환합니다.\n14. `match_indices(pat)`: 문자열에서 패턴에 해당하는 인덱스를 이터레이터로 반환합니다.\n\n#### 그 외\n\n러스트 언어 공식 문서를 보시면 더 많은 이터레이터 구현체들을 확인할 수 있습니다.\n\n### 메서드\n\n#### 개수 세기\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    println!(\"{:?}\", i.count()); // 10\n}\n```\n\n`count()` 메서드는 이터레이터에 포함된 값의 개수를 반환합니다.\n\n#### 마지막 값에 접근하기\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    println!(\"{:?}\", i.last()); // Some(10)\n}\n```\n\n`last()` 메서드는 이터레이터의 마지막 값을 반환합니다. `Option<T>` 객체로 반환되며 `Some(&value)` 분기가 있을 때, `None` 분기가 이터레이터가 비어 있을 때 입니다.\n\n#### 인덱스 접근\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    println!(\"{:?}\", i.nth(3)); // Some(4)\n    println!(\"{:?}\", i.nth(100)); // None\n}\n```\n\n`nth(n)` 메서드는 이터레이터에서 `n`번째 값을 가져옵니다. 반환값은 `Option<T>`입니다.\n\n#### n만큼 뛰어서 순회하기(step_by)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let mut stepped = i.step_by(3);\n\n    stepped.for_each(|x| print!(\"{} \", x)); // 1 4 7 10\n}\n```\n\n`step_by(step)` 메서드는 다음 값으로 이동할 때 입력받은 스텝 수만큼 인덱스가 증가합니다. 예시처럼 3을 입력하면 0 번째 다음 3칸 움직여서 3 번째 값을 출력하고 이후는 6 번째, 9 번째, ..., 이렇게 나아갑니다.\n\n#### 이어붙이기(chain)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let mut chained = i.chain(vec![11, 12, 13].into_iter());\n\n    chained.for_each(|x| print!(\"{} \", x)); // 1 2 3 4 5 6 7 8 9 10 11 12 13\n}\n```\n\n`chain(other)` 메서드는 한 이터레이터 뒤에 새로운 이터레이터를 이어 붙여줍니다.\n\n#### 합성하기(zip)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let mut zipped = i.zip(vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter());\n\n    zipped.for_each(|x| print!(\"{:?} \", x)); // (1, 1) (2, 2) (3, 3) (4, 4) (5, 5) (6, 6) (7, 7) (8, 8) (9, 9) (10, 10)\n}\n```\n\n`zip(other)` 메서드는 한 이터레이터와 입력받은 이터레이터의 값을 1:1 매칭하여 만든 튜플의 이터레이터를 반환합니다.\n\n#### 모든 값 변경하기(map)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let mut mapped = i.map(|x| x * 2);\n\n    mapped.for_each(|x| print!(\"{} \", x));\n}\n```\n\n`map(func)` 메서드는 입력받은 이터레이터를 순회하며 값을 순차적으로 입력받은 함수에 대입하고 반환된 결과로 이터레이터를 구성하여 반환합니다.\n\n#### 순회하기(for_each)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    i.for_each(|x| print!(\"{} \", x));\n}\n```\n\n`for_each(func)` 메서드는 `map(func)` 메서드와 마찬가지로 순차적으로 입력받은 함수에 값을 대입하여 실행합니다. 함수의 반환값은 쓰이지 않고 `()`만 허용됩니다. 러스트에서 `()`를 반환하는 방법은 `;`을 함수 마지막에 두거나 명시적으로 `return ()`을 호출하는 방법이 있습니다.\n\n#### 거르기(filter)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let mut filtered = i.filter(|x| x % 2 == 0);\n\n    filtered.for_each(|x| print!(\"{} \", x)); // 2 4 6 8 10\n}\n```\n\n`filter(pred)` 메서드는 입력받은 조건 함수를 통과한 값으로 이터레이터를 구성하여 반환합니다.\n\n#### 나열하기(enumerate)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    for (i, v) in i.enumerate() {\n        print!(\"({}, {}) \", i, v);\n    }\n    // (0, 1) (1, 2) (2, 3) (3, 4) (4, 5) (5, 6) (6, 7) (7, 8) (8, 9) (9, 10) \n}\n```\n\n`enumerate()` 메서드는 반복문에서 인덱스와 값으로 구조 분해하여 순회할 수 있게 이터레이터를 나열해줍니다.\n\n#### 찍먹하기(peekable, peek)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter().peekable();\n\n    println!(\"{:?}\", i.peek());\n    println!(\"{:?}\", i.next());\n    // Some(1)\n    // Some(1)\n    println!(\"{:?}\", i.peek());\n    println!(\"{:?}\", i.next());\n    // Some(2)\n    // Some(2)\n}\n```\n\n`peekable()` 메서드는 이터레이터의 값을 찍먹할 수 있는 객체로 만들어 반환합니다. 변환 후에는 `peek()` 메서드로 현재 값을 찍먹할 수 있고 `next()`로 받아서 다음 인덱스로 넘길 수 있습니다. \n\n#### 생략하기(skip, skip_while)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter().peekable();\n\n    let skipped = i.clone().skip(2);\n    skipped.for_each(|x| print!(\"{} \", x));\n    println!(\"\");\n    // 3 4 5 6 7 8 9 10\n\n    let skipped = i.skip_while(|x| *x < 5);\n    skipped.for_each(|x| print!(\"{} \", x));\n    // 5 6 7 8 9 10\n}\n```\n\n`skip(lenth)` 메서드는 입력받은 길이만큼 값을 생략합니다. `skip_while(pred)` 메서드는 입력받은 함수가 `false`를 반환할 때까지 스킵합니다.\n\n#### 뽑아내기(take, take_while)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter().peekable();\n\n    let took = i.clone().take(2);\n    took.for_each(|x| print!(\"{} \", x));\n    println!(\"\");\n\n    let took = i.take_while(|x| *x < 5);\n    took.for_each(|x| print!(\"{} \", x));\n}\n```\n\n`take(size)` 메서드는 입력받은 크기만큼의 값을 가져와서 이터레이터로 만듭니다. `take_while(pred)` 메서드는 입력받은 함수가 `false`를 반환할 때까지 뽑아내서 이터레이터로 만듭니다.\n\n#### 훑어보기(scan)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let scanned = i.scan(0, |state, x| {\n        *state += x;\n        Some(*state)\n    });\n\n    scanned.for_each(|x| print!(\"{} \", x));\n}\n```\n\n`scan(init_state, func)` 메서드는 초기화된 상태와 함수를 가집니다. 이 함수는 `state`와 이터레이터의 값을 받습니다. 이 때 함수가 취하는 `state`는 빌려온 변수로 이후에도 적용된 값이 계속 적용됩니다. 그리고 이터레이터의 값은 `map(...)`이나 `for_each(...)` 메서드처럼 순차적으로 들어옵니다. 그리고 매 단계의 출력값은 `Option<T>` 타입으로 반환해야하며 중간에 종료해야할 경우 `None`을 반환하면 됩니다.\n\n#### 평탄화(flatten)\n\n```rust\nfn main() {\n    let mut i = vec![vec![1, 2, 3], vec![4, 5, 6], vec![7, 8, 9]].into_iter();\n\n    let flatten = i.flatten();\n\n    flatten.for_each(|x| print!(\"{} \", x));\n}\n```\n\n`flatten()` 메서드는 이터레이터 내부에 이터레이터, 혹은 이터레이터로 변환할 수 있는 타입이 있을 경우에 하나의 이터레이터로 만듭니다.\n\n#### 중간 점검하기(inspect)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    i.inspect(|x| print!(\"{} \", x)).map(|x| x + 1).for_each(|x| print!(\"{} \", x));\n    // 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11\n}\n```\n\n`inspect(func)` 메서드는 `for_each(func)` 메서드와 동일한 동작을 하지만 `for_each(func)`와 다르게 동일한 이터레이터를 반환함으로 메서드 체인을 추가로 연결할 수 있도록 합니다.\n\n#### 수집하기(collect)\n\n```rust\nuse std::collections::HashSet;\n\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let set = i.map(|x| x + 1).collect::<HashSet<i32>>();\n\n    println!(\"{:?}\", set);\n    // {2, 7, 4, 10, 9, 5, 3, 6, 8, 11}\n}\n```\n\n`collect()` 메서드는 지정된 타입으로 이터레이터의 값을 수집하여 만들어줍니다.\n\n```rust\nuse std::collections::HashMap;\n\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let map = i.map(|x| (x, x + 1)).collect::<HashMap<i32, i32>>();\n\n    println!(\"{:?}\", map);\n    // {2: 3, 5: 6, 4: 5, 8: 9, 9: 10, 6: 7, 1: 2, 3: 4, 10: 11, 7: 8}\n}\n```\n\n맵의 경우엔 값을 2개 가지는 튜플로 만들어주어야 합니다.\n\n#### 접기(fold, rfold)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let folded = i.clone().fold(0, |acc, x| acc + x);\n\n    println!(\"{:?}\", folded);\n\n    let rfolded = i.rfold(0, |acc, x| acc + x);\n\n    println!(\"{:?}\", rfolded);\n}\n```\n\n`fold(init, func)` 메서드는 `init`을 시작으로 이전 계산 결과와 이터레이터의 시작부터 끝까지의 값을 입력받은 함수의 결과 값을 반환합니다. `rfold(init, func)`는 반대로 끝에서부터 시작합니다.\n\n#### 줄이기(reduce)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let reduced = i.reduce(|acc, x| acc + x);\n\n    println!(\"{:?}\", reduced);\n}\n```\n\n`reduce(func)`은 `fold(init, func)`에서 초기값이 첫번째 인자인 상태와 동일한 동작을 보여줍니다.\n\n#### 테스트 통과(all, any)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let all_even = i.all(|x| x % 2 == 0);\n    let any_even = i.any(|x| x % 2 == 0);\n\n    println!(\"{:?}, {:?}\", all_even, any_even); \n    // false, true\n}\n```\n\n`all(pred)` 메서드는 입력된 판별 함수가 모두 `true`를 반환할 때 `true`를 반환하고, `any(pred)` 메서드는 판별 함수가 하나라도 `true`를 반환하면 `true`를 반환합니다. 둘 다 그 외의 상황에는 `false`를 반환합니다.\n\n#### 찾기(find)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let found = i.find(|&x| x % 2 == 0);\n\n    println!(\"{:?}\", found);\n    // Some(2)\n}\n```\n\n`find(pred)` 메서드는 이터레이터를 순회하며 판별 함수가 처음으로 `true`를 반환하는 값을 반환합니다.\n\n#### 위치 찾기(position, rposition)\n\n```rust\nfn main() {\n    let s = String::from(\"A little copying is better than a little dependency.\");\n\n    let pos = s.chars().position(|x| x == 'b');\n\n    println!(\"{:?}\", pos);\n}\n```\n\n`position(pred)` 함수는 이터레이터를 순회하며 가장 먼저 판별 함수를 만족하는 값이 등장한 인덱스를 반환합니다. 이 메서드는 문자열의 인덱스를 찾는 것에 유용하게 쓰일 수 있습니다.\n\n#### 최대, 최소(max, min)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let max = i.clone().max();\n    let min = i.min();\n\n    println!(\"{:?} {:?}\", max, min);\n    // Some(10) Some(1)\n}\n```\n\n`max()`는 이터레이터의 최대값을, `min()`은 이터레이터의 최소값을 반환합니다.\n\n#### 역전(rev)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let rev = i.rev();\n\n    rev.for_each(|x| print!(\"{} \", x));\n}\n```\n\n`rev()` 메서드는 이터레이터의 순서를 역전시킵니다.\n\n#### 계산하기(sum, product)\n\n```rust\nfn main() {\n    let mut i = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10].into_iter();\n\n    let sum: i32 = i.clone().sum();\n    let prod: i32 = i.product();\n\n    println!(\"sum: {}\", sum); // sum: 55\n    println!(\"prod: {}\", prod); // prod: 3628800\n}\n```\n\n`sum()`과 `product()` 메서드는 각각 값들의 합과 곱을 반환합니다.\n\n#### 그 외\n\n그 외에도 이터레이터에는 더 많은 유용한 메서드들이 존재하고, 추가되고 있습니다.\n\n---\n\n## 몇 문제 풀죠\n\n### LeetCode 393. UTF-8 Validation\n\n문제는 UTF8 인코딩의 적법성을 테스트하는 것입니다.\n\n```bash\n   Char. number range  |        UTF-8 octet sequence\n      (hexadecimal)    |              (binary)\n   --------------------+---------------------------------------------\n   0000 0000-0000 007F | 0xxxxxxx\n   0000 0080-0000 07FF | 110xxxxx 10xxxxxx\n   0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx\n   0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx\n```\n\n리트코드 문제에 나와 있는 대로 UTF8은 바이트 개수에 따라 첫번째 바이트 포맷이 달라지고 따라오는 바이트 들은 비트가 10으로 시작합니다. 그래서 첫번째 바이트의 포맷과 나머지 바이트들이 정상적으로 구성되어 있는 지만 테스트 하면 됩니다.\n\n```rust\nimpl Solution {\n    pub fn valid_utf8(data: Vec<i32>) -> bool {\n        if data.len() == 0 {\n            return true;\n        }\n        let pre = data.iter().scan(0, |state, x| {\n            match *state {\n                0 => {\n                    if x >> 5 == 0b110 {\n                        *state = 1;\n                        Some(*state)\n                    } else if x >> 4 == 0b1110 {\n                        *state = 2;\n                        Some(*state)\n                    } else if x >> 3 == 0b11110 {\n                        *state = 3;\n                        Some(*state)\n                    } else if x >> 7 == 0 {\n                        *state = 0;\n                        Some(*state)\n                    } else {\n                        *state = 0;\n                        None\n                    }\n                }\n                _ => {\n                    if x >> 6 == 0b10 {\n                        *state -= 1;\n                        Some(*state)\n                    } else {\n                        *state = 0;\n                        None\n                    }\n                }\n            }\n        });\n        if pre.clone().count() != data.len() || pre.last().unwrap() != 0 {\n            return false;\n        }\n        true\n    }\n}\n```\n\n먼저 데이터를 이터레이터로 변환한 후 스캔을 시작합니다. `state`는 헤더를 검사하여 몇개의 바이트가 10으로 시작해야하는지 저장합니다. 그리고 `x`는 당연히 이터레이터의 각 값이 들어옵니다. `state`가 0일 때 헤더라고 파악하고 검사합니다. 검사에 따라 남아있는 바이트 수를 갱신하고 상태를 `Some()`으로 감싸서 반환합니다. `state`가 0이 아닐 때는 바이트가 10으로 시작하는 지 체크하고 현재 `state`를 반환합니다.\n\n계속 반복하여 바이트를 검사한 후 스캔한 후의 이터레이터의 길이와 입력받은 데이터의 길이가 동일한지 파악합니다. 만약 중간에 문제가 생겨 `None`이 반환되면 순회를 멈추기 때문에 길이가 부족하게 됩니다. 이를 통해 중간에 문제가 있었는 지 파악합니다. 그리고 문제가 없다면 이터레이터의 마지막 값을 가져와서 `0`인지 확인합니다. 만약 `0`이 아니면 더 확인해야하는 바이트가 존재한다는 뜻이기 때문입니다.\n\n이렇게 풀면 시간복잡도와 공간복잡도는 O(N)입니다. \n\n```text\nSuccess\nRuntime: 2 ms, faster than 42.86% of Rust online submissions for UTF-8 Validation.\nMemory Usage: 2 MB, less than 100.00% of Rust online submissions for UTF-8 Validation.\n```\n\n테스트 결과는 통과입니다. 이터레이터가 아니라 `for`나 `while` 반복문으로 작성하면 더 빠르게 작성할 수 있을 것입니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/012_rust-for-cp/", "created_date": "2022-04-12T00:08:40Z"}
{"title": "고 언어의 의존성 주입을 위한 Provider", "content": "## Provider\n\n`Provider`는 제가 만들고 있는 고 언어에서 의존성 주입을 위한 컨테이너입니다.\n\n### 정의\n\n```go\ntype Provider struct {\n\tconstructors map[reflect.Type]map[reflect.Value]struct{}\n\tcontainer    map[reflect.Type]any\n\tlock         sync.RWMutex\n}\n```\n\n`Provider`는 `constructors`와 `container`를 가지고 있습니다.\n1. `constructors`는 생성자를 저장하는 맵입니다.\n2. `container`는 실제로 생성된 인스턴스를 저장하는 맵입니다.\n\n### 생성자 등록\n\n```go\nfunc (p *Provider) Register(constructFunction ...any) error {\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\tfor _, con := range constructFunction {\n\t\tif err := p.register(con); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (p *Provider) register(constructFunction any) error {\n\targs, _, err := analyzeConstructor(constructFunction)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, arg := range args {\n\t\tif _, ok := p.constructors[arg]; !ok {\n\t\t\tp.constructors[arg] = make(map[reflect.Value]struct{})\n\t\t}\n\t\tp.constructors[arg][reflect.ValueOf(constructFunction)] = struct{}{}\n\t}\n\n\treturn nil\n}\n\nfunc analyzeConstructor(constructFunction any) ([]reflect.Type, []reflect.Type, error) {\n\tif reflect.TypeOf(constructFunction).Kind() != reflect.Func {\n\t\treturn nil, nil, ErrNotAFunction{}\n\t}\n\n\tconstructor := reflect.ValueOf(constructFunction)\n\tvar args []reflect.Type\n\n\tfor i := 0; i < constructor.Type().NumIn(); i++ {\n\t\targs = append(args, constructor.Type().In(i))\n\t}\n\n\tvar returns []reflect.Type\n\n\tfor i := 0; i < constructor.Type().NumOut(); i++ {\n\t\treturns = append(returns, constructor.Type().Out(i))\n\t}\n\n\treturn args, returns, nil\n}\n```\n\n`Register` 메서드는 생성자를 등록하는 메서드입니다.  \n`...any`로 가변인자를 받아서 여러개의 생성자를 등록할 수 있습니다.  \n내부에 정의된 `register` 메서드를 호출해서 각 함수를 `constructors`에 저장합니다.\n\n`register`는 내부에 다시 `analyzeConstructor`를 호출해서 생성자의 인자와 반환값을 분석합니다.\n\n보면 알겠지만, 함수의 시그니처는 `func Constructure(p1 T1, p2 T2, ...) (r1 R1, r2, R2 ... , error)`로 되어있어야 합니다.\n\n매개변수가 하나도 정의되어 있지 않다면, 동작하지 않습니다.\n\n### 생성자 호출\n\n```go\nfunc getContextType() reflect.Type {\n\treturn reflect.TypeOf((*context.Context)(nil)).Elem()\n}\n\nfunc (p *Provider) Construct(ctx context.Context) error {\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\tp.container[getContextType()] = ctx\n\tcount := 0\n\tfor len(p.constructors) > 0 {\n\t\tfor arg, constructors := range p.constructors {\n\t\tConsLoop:\n\t\t\tfor con := range constructors {\n\t\t\t\targs := make([]reflect.Value, con.Type().NumIn())\n\t\t\t\tfor i := 0; i < con.Type().NumIn(); i++ {\n\t\t\t\t\tat := con.Type().In(i)\n\t\t\t\t\tv, ok := p.container[at]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcontinue ConsLoop\n\t\t\t\t\t}\n\t\t\t\t\targs[i] = reflect.ValueOf(v)\n\t\t\t\t}\n\n\t\t\t\treturns := con.Call(args)\n\n\t\t\t\tfor _, ret := range returns {\n\t\t\t\t\tif ret.Type().Kind().String() == \"error\" {\n\t\t\t\t\t\tif !ret.IsNil() {\n\t\t\t\t\t\t\treturn ret.Interface().(error)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tp.container[ret.Type()] = ret.Interface()\n\t\t\t\t}\n\n\t\t\t\tcount++\n\n\t\t\t\tdelete(p.constructors[arg], con)\n\t\t\t}\n\n\t\t\tif len(p.constructors[arg]) == 0 {\n\t\t\t\tdelete(p.constructors, arg)\n\t\t\t}\n\t\t}\n\n\t\tif count == 0 {\n\t\t\treturn ErrMaybeCyclicDependency{}\n\t\t}\n\t}\n\n\treturn nil\n}\n```\n\n`Construct` 메서드는 생성자를 호출하는 메서드입니다.\n\n`constructors`에 저장된 생성자를 하나씩 호출하면서, 인자로 필요한 값이 `container`에 있는지 확인하고, 있다면 호출합니다.\n\n반환값이 있을 경우, 타입을 검사하여 `container`에 저장합니다.\n\n이 과정을 반복하면서, `constructors`에 저장된 생성자가 모두 호출되면 종료합니다.  \n만약 이전 호출과 비교하여 호출된 생성자가 없다면, 순환 의존성이 있을 수 있으므로 에러를 반환합니다.\n\n### 사용 예시\n\n```go\nfunc JustRun(provider *Provider, function any) error {\n\tprovider.lock.RLock()\n\tdefer provider.lock.RUnlock()\n\n\targs, rets, err := analyzeConstructor(function)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(rets) != 1 || rets[0].String() != \"error\" {\n\t\treturn ErrInvalidFunctionReturn{}\n\t}\n\n\treflectArgs := make([]reflect.Value, len(args))\n\tfor i, arg := range args {\n\t\tv, ok := provider.container[arg]\n\t\tif !ok {\n\t\t\treturn ErrNotProvided{arg}\n\t\t}\n\t\treflectArgs[i] = reflect.ValueOf(v)\n\t}\n\n\treflectReturns := reflect.ValueOf(function).Call(reflectArgs)\n\n\tif !reflectReturns[0].IsNil() {\n\t\treturn reflectReturns[0].Interface().(error)\n\t}\n\n\treturn nil\n}\n```\n\n`JustRun`은 프로바이더에 존재하는 인스턴스를 사용하여 함수를 호출하는 메서드입니다.\n\n`function`은 생성자와 비슷하게 `func Function(p1 T1, p2 T2, ...) (error)`로 되어있어야 합니다.  \n`JustRun`은 `function`을 호출하고, 반환값이 에러라면 에러를 반환합니다.\n\n### 마치며\n\n이 라이브러리를 통해 쉽게 인스턴스를 생성하고 활용할 수 있게 되었으면 합니다.\n\n한가지 아쉬운 점은 아직 로직이 효율적이지 않아, 앱 스타트가 느릴 수 있다는 점입니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/026_provider_of_go/", "created_date": "2024-02-09T01:10:20Z"}
{"title": "에러 처리 in go", "content": "## 쉬운 에러 처리\n\n```go\npackage main\n\nimport (\n\t\"errors\"\n)\n\nfunc NewError() error {\n\treturn errors.New(\"this is new error!\")\n}\n\nfunc ThrowError() error {\n\t_, err := os.Open(\"not-exist\")\n\treturn err\n}\n\nfunc main() {\n\terr := NewError()\n\tif err != nil {\n\t\tlog.Println(err)\n\t}\n\n\terr = ThrowError()\n\tif err != nil {\n\t\tlog.Println(err)\n\t}\n}\n```\n\n가장 쉬운 에러 처리 방법은 `errors.New` 함수로 에러 메시지를 담은 `error` 인터페이스 인스턴스를 직접 만드는 것과 반환된 에러를 그대로 상위 스택에 넘겨주는 방법이 있습니다. 하지만 이 2가지 방식은 각기 다른 문제점이 있습니다.\n\n첫번째 에러 메시지만 담아 보낼 때는 어떤 에러인지 파악하기 위해 메시지를 검사해야한다는 점이 있습니다. 단순 로깅을 위해 메시지만 저장하는 경우엔 큰 문제가 되지 않지만 런타임에 어떤 에러인지에 따라 다음 행동을 정해야할 때는 코딩 난이도가 높아지게 됩니다. 그래서 이를 해결하기 위한 여러가지 솔루션을 고랭 팀과 고퍼들이 연구 & 개발했습니다. 그 중 몇가지만 더 낫다고 생각하는 순서대로 작성해보겠습니다.\n\n## error 인스턴스 비교\n\n```go\nvar thisError = errors.New(\"this is a new error!\")\n\nfunc NewError() error {\n\treturn thisError\n}\n\nfunc main() {\n\terr := NewError()\n\tswitch err {\n\tcase thisError:\n\t\tlog.Println(err)\n\tdefault:\n\t\tlog.Println(\"no error\")\n\t}\n}\n```\n\n첫번째로 알아볼 것은 미리 만들어 놓은 `error` 인스턴스를 에러가 발생할 때 반환하는 것입니다. `switch` 문으로 `thisError`와 비교하면 옳다고 판단하여 에러 로그를 남기는 모습을 확인할 수 있습니다. \n\n```bash\ngo % go run .\n2021/12/26 13:52:02 this is a new error!\n```\n\n하지만 이 방법에는 2가지 문제점이 있습니다. 하나는 고 언어에는 불변값이 존재하지 않기에 외부로 내보내진 `error` 인스턴스에 악의적인 행동 혹은 실수에 의해 수정되어 다음 동작에 대한 결과를 보장할 수 없다는 것입니다.\n\n그리고 다른 하나는, 예제 코드에서는 두 에러를 `switch`를 통해 비교했지만 실제로는 `if err == thisError { ... }`와 같다는 걸 알고, 두 에러를 `==`로 비교하는 건 두 문자열이 동등한지 파악하는 거와 같기에 그다지 유연한 코드가 아니라는 걸 알 수 있습니다.\n\n### 문자열 상수 비교\n\n```go\nvar thisError = \"this is a new error!\"\n\nfunc NewError() error {\n\treturn errors.New(thisError)\n}\n\nfunc main() {\n\terr := NewError()\n\tif err.Error() == thisError {\n\t\tlog.Println(\"this is a new error!\")\n\t}\n}\n```\n\n문자열 상수를 비교하는 건 error 인스턴스를 비교하는 것과 완전히 동일합니다. 대신 문자열 상수를 비교하기 때문에 에러 메시지가 수정되는 걸 방지할 수 있다는 장점이 있습니다.\n\n## 타입 비교\n\n```go\ntype FileError struct {\n\tIsExist         bool\n\tInvalidName     bool\n\tPermissionError bool\n\tMsg             string\n}\n\nfunc (e *FileError) Error() string {\n\treturn e.Msg\n}\n\nfunc NewError() error {\n\treturn &FileError{\n\t\tIsExist:         false,\n\t\tInvalidName:     false,\n\t\tPermissionError: true,\n\t\tMsg:             \"permission error\",\n\t}\n}\n\nfunc main() {\n\terr := NewError()\n\tswitch err := err.(type) {\n\tcase *FileError:\n\t\tlog.Println(err.Msg)\n\t\tlog.Println(fmt.Sprintf(\"is exist: %v\", err.IsExist))\n\t\tlog.Println(fmt.Sprintf(\"invalid name: %v\", err.InvalidName))\n\t\tlog.Println(fmt.Sprintf(\"permission error: %v\", err.PermissionError))\n\t}\n}\n```\n\n이번에는 구체적인 타입을 만들어서 `error` 인터페이스로 반환하였습니다. 이렇게 작성하면 더 많은 정보를 에러를 통해 넘겨줄 수 있으며 `switch assertion` 문법을 활용하여 쉽게 접근할 수 있습니다. 하지만 잘 아시다시피 구체적인 타입을 이용하여 코드를 작성할 경우 느슨한 결합을 유도할 수 없게 된다는 단점이 있습니다. 그리고 저희는 고 코드를 작성할 때 인터페이스를 사용함으로 느슨한 결합을 만들었습니다.\n\n## 인터페이스 비교\n\n```go\ntype IsExistErrorInterface interface {\n\tIsExist() bool\n}\n\ntype InvalidNameErrorInterface interface {\n\tIsInvalidName() bool\n}\n\ntype PermissionErrorInterface interface {\n\tIsPermissionError() bool\n}\n\ntype FileError struct {\n\tisExist         bool\n\tinvalidName     bool\n\tpermissionError bool\n\tMsg             string\n}\n\ntype PermissionError FileError\n\nfunc (p *PermissionError) IsPermissionError() bool {\n\treturn true\n}\n\nfunc (p *PermissionError) Error() string {\n\treturn p.Msg\n}\n\nfunc NewError() error {\n\treturn &PermissionError{\n\t\tisExist:         true,\n\t\tinvalidName:     true,\n\t\tpermissionError: true,\n\t\tMsg:             \"permission error\",\n\t}\n}\n\nfunc main() {\n\terr := NewError()\n    log.Println(err.Error())\n\tswitch e := err.(type) {\n\tcase PermissionErrorInterface:\n\t\tlog.Println(fmt.Sprintf(\"is permission error: %v\", e.IsPermissionError()))\n\tcase IsExistErrorInterface:\n\t\tlog.Println(fmt.Sprintf(\"is exist error: %v\", e.IsExist()))\n\tcase InvalidNameErrorInterface:\n\t\tlog.Println(fmt.Sprintf(\"is invalid name error: %v\", e.IsInvalidName()))\n\t}\n}\n```\n\n코드 상태가 그다지 좋지 않은 점은 죄송하게 생각합니다. 그다지 좋은 예시가 떠오르지 않았습니다.\n\n이번엔 타입을 비교하는 것에서 한단계 더 나아가서 인터페이스를 비교합니다. 이 코드는 여러가지 에러의 정보를 얻을 수 있는 인터페이스를 만들어서 에러의 구체적인 타입을 몰라도 값을 얻을 수 있게 덕타이핑한 것입니다. \n\n위에서 이어져오는 예시 코드의 상태가 너무 안좋으니 간단한 `GetRequest` 코드를 새로 작성해보겠습니다.\n\n```go\nfunc GetRequest(url string) ([]byte, error) {\n\tresp, err := http.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn body, nil\n}\n```\n\n`GetRequest` 함수는 입력받은 url에 GET 요청을 하여 얻은 body를 반환하는 함수입니다. 하지만 여기에 한가지 문제가 있습니다. 요청이 성공했든 실패했든 `Status Code`를 사용자에게 전달할 방법이 없습니다. 이걸 해결하기 위해 에러를 살짝 수정하여 다시 작성해보겠습니다.\n\n```go\ntype errorWithStatus struct {\n\terr    error\n\tstatus int\n}\n\nfunc (e *errorWithStatus) Error() string {\n\treturn e.err.Error()\n}\n\nfunc (e *errorWithStatus) Status() int {\n\treturn e.status\n}\n\ntype HasStatus interface {\n\tStatus() int\n}\n\nfunc GetRequest(url string) ([]byte, error) {\n\tresp, err := http.Get(url)\n\tif err != nil {\n\t\tif resp == nil {\n\t\t\treturn nil, &errorWithStatus{\n\t\t\t\terr:    err,\n\t\t\t\tstatus: http.StatusNotFound,\n\t\t\t}\n\t\t}\n\t\treturn nil, &errorWithStatus{\n\t\t\terr:    err,\n\t\t\tstatus: resp.StatusCode,\n\t\t}\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn body, nil\n}\n```\n\n새로운 에러 타입인 `errorWithStatus`를 만들었고 멤버로 스테이터스 코드를 가지고 `Status()` 메서드를 통해 해당 값을 받을 수 있도록 수정하였습니다. 이제 이 함수를 실행하고 만약 요청에서 에러가 발생하면 스테이터스 코드를 확인하여 디버깅할 수 있습니다.\n\n```go\nfunc main() {\n\t_, err := GetRequest(\"https://not.exists.domain\")\n\tswitch e := err.(type) {\n\tcase nil:\n\t\tlog.Println(\"no error\")\n\tcase *errorWithStatus:\n\t\tlog.Println(\"Status:\", e.Status())\n\t\tlog.Fatal(err.Error())\n\tdefault:\n\t\tlog.Fatal(err)\n\t}\n}\n```\n\n```bash\ngo % go run .\n2021/12/26 16:10:48 Status: 404\n2021/12/26 16:10:48 Get \"https://not.exists.domain\": dial tcp: lookup not.exists.domain: no such host\nexit status 1\n```\n\n이렇게 작성하면 단순 GET 요청 뿐만 아니라 다른 REST API 요청에도 하나의 인터페이스로 에러를 처리하여 느슨한 결합을 만들 수 있을 것입니다.\n\n## 계층적 에러 처리\n\n마지막은 에러를 어떻게 만들고 처리하냐가 아니라 첫번째 코드의 마지막 `ThrowError` 함수에서의 에러 처리에 대한 부분입니다. 표준 `errors` 패키지에는 에러를 계층적으로 처리하기에 유용한 함수들이 작성되어 있습니다.\n\n```go\nfunc ThrowError() error {\n\t_, err := os.Open(\"not-exist\")\n\treturn err\n}\n```\n\n### 에러 포장\n\n이 코드는 `os.Open` 함수가 주는 에러를 그대로 반환합니다. 그럴 경우 해당 에러가 어디서부터 왔는지, 어떤 계층을 가지는 지 파악할 수 없습니다. 그러면 `ThrowError` 함수를 거쳤음을 표현하기 위해 코드를 수정해보겠습니다.\n\n```go\nfunc ThrowError() error {\n\t_, err := os.Open(\"not-exist\")\n\treturn fmt.Errorf(\"ThrowError: %w\", err)\n}\n\nfunc main() {\n\terr := ThrowError()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n```\n\n`ThrowError` 함수에서 에러를 반환하기 전에 `fmt.Errorf` 함수를 사용하여 에러를 래핑하고 있습니다. 래핑한 에러를 출력하게 되면 아래와 같이 문구가 추가되어 있음을 확인할 수 있습니다.\n\n```bash\ngo % go run .\n2021/12/26 16:20:10 ThrowError: open not-exist: no such file or directory\nexit status 1\n```\n\n이렇게 래핑된 에러는 `errors.Unwrap` 함수를 이용하여 내부 에러를 가져올 수 있습니다.\n\n```go\nfunc main() {\n\terr := ThrowError()\n\tfmt.Println(errors.Unwrap(err))\n\tfmt.Println(errors.Unwrap(errors.Unwrap(err)))\n\tfmt.Println(errors.Unwrap(errors.Unwrap(errors.Unwrap(err))))\n}\n```\n\n`main` 함수를 수정하여 여러 단계의 `errors.Unwrap`을 수행하게 되면 순서대로 하나씩 벗겨지는 걸 확인할 수 있습니다.\n\n```bash\ngo % go run .\nopen not-exist: no such file or directory\nno such file or directory\n<nil>\n```\n\n순서대로 `ThrowError`, `open not-exist`, `no such file or directory`이 벗겨지고 마지막으로 `nil`이 나옴을 확인할 수 있습니다. 이러한 구조가 있기에 `for err != nil { ... }`같은 형태로 쉽게 반복할 수 있습니다.\n\n### 에러 추적\n\n```go\nfunc main() {\n\tfirstErr := errors.New(\"this is an error\")\n\tsecondErr := fmt.Errorf(\"second error: %w\", firstErr)\n\tthirdErr := fmt.Errorf(\"third error: %w\", secondErr)\n\n\tfmt.Println(errors.Is(thirdErr, firstErr))\n}\n```\n\n`errors` 패키지에는 `errors.Is` 함수도 제공하고 있습니다. 이 함수는 지속적인 래핑의 결과물로 나온 에러가 원본 에러와 같은 에러인가를 쉽게 확인할 수 있게 작성된 함수입니다. 위 코드를 실행하게 되면 `thirdErr`은 `firstErr`와 같다는 것을 확인할 수 있습니다. 그리고 객체지향의 상속 계층과 마찬가지로 두 패러미터의 위치가 바뀌면(`errors.Is(firstErr, thirdErr)`) 성립되지 않고 `false`를 출력합니다.\n\n### 에러 확인\n\n```go\ntype HasStatus interface {\n\tStatus() int\n}\n\ntype ErrorWithStatus struct {\n\terr    error\n\tstatus int\n}\n\nfunc (e *ErrorWithStatus) Error() string {\n\treturn fmt.Sprintf(\"%d: %s\", e.status, e.err)\n}\n\nfunc (e *ErrorWithStatus) Status() int {\n\treturn e.status\n}\n\nfunc main() {\n\terr := &ErrorWithStatus{\n\t\terr:    errors.New(\"error\"),\n\t\tstatus: 500,\n\t}\n\n\tfmt.Println(errors.As(err, new(HasStatus)))\n}\n```\n\n방금 `GetRequest` 함수를 만들었을 때 사용한 에러 타입을 간단하게 다시 만들었습니다. 해당 에러는 `HasStatus` 인터페이스를 구현하고 있음을 생각하고 `main` 함수를 보면 `errors.As` 함수를 이용하여 에러 인스턴스와 인터페이스 포인터를 사용하여 해당 인터페이스를 구현하고 있는 지 확인하는 걸 볼 수 있습니다.\n\n```go\nfunc main() {\n\terr := &ErrorWithStatus{\n\t\terr:    errors.New(\"error\"),\n\t\tstatus: 500,\n\t}\n\n\twrappedErr := fmt.Errorf(\"%w\", err)\n\n\tfmt.Println(errors.As(wrappedErr, new(HasStatus)))\n}\n```\n\n`errors.As` 함수는 계층적 동작도 가능하기에 위 코드처럼 에러 인스턴스를 한번 감싸도 `true`를 출력하는 것을 확인할 수 있습니다.", "author": "snowmerak", "original_url": "https://snowmerak.pages.dev/posts/001_error-handling/", "created_date": "2021-12-26T13:12:58Z"}
